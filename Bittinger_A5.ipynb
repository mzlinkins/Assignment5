{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "Bittinger-A5.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "PvahNgA2rVxD"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgWGBVyJsVeQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxqpEXERrVvK",
        "colab_type": "text"
      },
      "source": [
        "# CMSC471 Artificial Intelligence\n",
        "\n",
        "# Assignment-5: Classification and Regression with Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsrJexD8rVvL",
        "colab_type": "text"
      },
      "source": [
        "Name: Justin Bittinger<br>ID: WH10613\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbqoJLGzrVvM",
        "colab_type": "text"
      },
      "source": [
        "## Overview and Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxPXXWVprVvN",
        "colab_type": "text"
      },
      "source": [
        "You have learned the fundamentals of Neural Networks and Deep Learning. You have also learned how to train them based on the techniques discussed in lectures and the contents from Chapter 10 of the textbook (and you'll learn more on training deep neural networks in Chapter 11).\n",
        "\n",
        "In Part I of this assignment, you are going to build a NN for classification. In Part II, you will build a NN for regression.\n",
        "\n",
        "Pedagogically, this assignment will help you:\n",
        "- better understand Neural Networks.\n",
        "\n",
        "- practice with Tensorflow and Keras API.\n",
        "\n",
        "- practice the skills you learned in sklearn and combine them with tf/keras to use in your project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fAxvXaJrVvN",
        "colab_type": "text"
      },
      "source": [
        "Notice that you should have Tensorflow version 2.0 installed and ready.\n",
        "\n",
        "Alternatively, you may complete the assignment in [Colab](https://colab.research.google.com) but you need to run the following magic command in Colab to switch to version 2.0:\n",
        "\n",
        "`%tensorflow_version 2.x`\n",
        "\n",
        "<b>Important Notice:</b> Some outputs/plots are shared with you in this notebook for your reference. Some outputs are intentionally not shared. Notice that it is the strict course policy NOT to include any (or even parts) of the code solutions and/or answers to the questions in your posts in Piazza. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZwA0IWxrVvO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import necessary Python/tf/keras modules\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import sklearn.metrics as metrics\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtmF7448rVvR",
        "colab_type": "code",
        "outputId": "99f323f9-3493-4c17-d6e8-7b9d216650cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"tf Version: \", tf.__version__)\n",
        "print(\"Eager Execution mode: \", tf.executing_eagerly())"
      ],
      "execution_count": 266,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf Version:  2.0.0\n",
            "Eager Execution mode:  True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOod5Vi8rVvU",
        "colab_type": "text"
      },
      "source": [
        "## Part I - Classification with NNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sqc_S91rVvU",
        "colab_type": "text"
      },
      "source": [
        "First, [download the data](https://github.com/fereydoonvafaei/UMBC-CMSC-471-Fall-2019/blob/master/Assignment-5/diabetes.csv) `diabetes.csv` The target is to predict the onset of diabetes based on patient's features. You can read more about the data [here](https://www.kaggle.com/uciml/pima-indians-diabetes-database)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2ClxOilrVvV",
        "colab_type": "code",
        "outputId": "54200063-0e3a-4b05-9028-08ae66c7ac7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "# Load diabetes data with Pandas, it should be in the same working directory.\n",
        "df = pd.read_csv(\"diabetes.csv\")\n",
        "\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "execution_count": 267,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(768, 9)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Pregnancies  Glucose  BloodPressure  ...  DiabetesPedigreeFunction  Age  Outcome\n",
              "0            6      148             72  ...                     0.627   50        1\n",
              "1            1       85             66  ...                     0.351   31        0\n",
              "2            8      183             64  ...                     0.672   32        1\n",
              "3            1       89             66  ...                     0.167   21        0\n",
              "4            0      137             40  ...                     2.288   33        1\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 267
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbB-CykHrVvX",
        "colab_type": "text"
      },
      "source": [
        "## <font color=\"red\"> Required Coding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csvu94W6rVvY",
        "colab_type": "code",
        "outputId": "f5dbff3f-bbe9-4e76-8d47-68280ba234b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Create X, y - Notice that X should contain all the features (columns) except 'Outcome'\n",
        "# y should include only 'Outcome' because it's the label!\n",
        "### START CODING HERE ###\n",
        "X = df.drop(columns = \"Outcome\")\n",
        "y = df['Outcome']\n",
        "### END CODING HERE ###\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": 268,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(768, 8)\n",
            "(768,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6jNZ6uMrVva",
        "colab_type": "code",
        "outputId": "48f94d9b-2cb3-4535-c035-f80ffb4031a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Split the data to train and test with test_size=0.33 and random_state=66\n",
        "### START CODING HERE ###\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .33, random_state = 66)\n",
        "### END CODING HERE ###\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 269,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(514, 8)\n",
            "(514,)\n",
            "(254, 8)\n",
            "(254,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VOVZcLYrVvc",
        "colab_type": "text"
      },
      "source": [
        "> There are different ways to load the data into tf tensors depeneding on your data type (image, text, etc). The following cell is one way of loading pandas dataframes to tensorflow tensors so that you can use tf/keras methods on them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mpu0WJJRrVvd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load train and test data to tf\n",
        "train_tensor = tf.data.Dataset.from_tensor_slices((X_train.values, y_train.values))\n",
        "test_tensor = tf.data.Dataset.from_tensor_slices((X_test.values, y_test.values))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQ7E2D9drVve",
        "colab_type": "code",
        "outputId": "6561fdb4-bba1-4e1e-da24-26dceb641e69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(train_tensor)"
      ],
      "execution_count": 271,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensorflow.python.data.ops.dataset_ops.TensorSliceDataset"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 271
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ra9wqbEbrVvh",
        "colab_type": "code",
        "outputId": "0fa818d4-9834-4fe4-a7e6-0284c1108421",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "for feat, targ in train_tensor.take(5):\n",
        "  print ('Features: {}, Target: {}'.format(feat, targ))"
      ],
      "execution_count": 272,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features: [  3.   123.   100.    35.   240.    57.3    0.88  22.  ], Target: 0\n",
            "Features: [  0.    104.     64.     23.    116.     27.8     0.454  23.   ], Target: 0\n",
            "Features: [10.    75.    82.     0.     0.    33.3    0.263 38.   ], Target: 0\n",
            "Features: [  4.    183.      0.      0.      0.     28.4     0.212  36.   ], Target: 1\n",
            "Features: [  2.     98.     60.     17.    120.     34.7     0.198  22.   ], Target: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EK9Yx4SJrVvi",
        "colab_type": "text"
      },
      "source": [
        "> As discussed in the lectures, data is fed into the network in batches (mini-batches)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mi8fn7S7rVvj",
        "colab_type": "code",
        "outputId": "a8c78a02-afe1-4b88-8beb-c3658388b417",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Batch train and test data\n",
        "train_batch = train_tensor.shuffle(len(X_train)).batch(1)\n",
        "test_batch = test_tensor.shuffle(len(X_test)).batch(1)\n",
        "\n",
        "print(type(train_batch))\n",
        "print(type(test_batch))"
      ],
      "execution_count": 273,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'tensorflow.python.data.ops.dataset_ops.BatchDataset'>\n",
            "<class 'tensorflow.python.data.ops.dataset_ops.BatchDataset'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OrMsM3_rVvl",
        "colab_type": "text"
      },
      "source": [
        ">Now, build the model based on the given architecture specifications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyOUAsVLrVvm",
        "colab_type": "text"
      },
      "source": [
        "## <font color=\"red\"> Required Coding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmBouvbErVvm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### START CODING HERE ###\n",
        "# Build a Sequential neural network - a classifier model\n",
        "nn_clf = tf.keras.Sequential([\n",
        "    # Create a dense layer with 12 units, input_dim=8, and 'relu' activation function ~ 1 line\n",
        "    tf.keras.layers.Dense(12, input_dim = 8, activation = 'relu'),\n",
        "    # Create a dense layer with 8 units, and 'relu' activation function ~ 1 line\n",
        "    tf.keras.layers.Dense(8, activation = 'relu'),\n",
        "    # Create a dense layer with ? unit(s), and '?' activation function ~ 1 line\n",
        "    # YOU should decide on the number of untis and the activation function for this last layer (output layer)\n",
        "    # Hint: What type of ML task is this problem?\n",
        "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
        "    ])  \n",
        "### END CODING HERE ###"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOxrkhVwrVvo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### START CODING HERE ###\n",
        "# Compile the model by 'adam' optimizer, 'binary_crossentropy' loss and 'accuracy' as metrics ~ 1 line\n",
        "nn_clf.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "### END CODING HERE ###"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEuTn4YUrVvq",
        "colab_type": "code",
        "outputId": "11f9e8f6-de14-4c5e-90e2-d2b4fe304e23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "### START CODING HERE ###\n",
        "# Fit nn_clf model on train_batch with 150 epochs\n",
        "nn_clf_history = nn_clf.fit(X, y, epochs = 150)\n",
        "### END CODING HERE ###"
      ],
      "execution_count": 276,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
            "Train on 768 samples\n",
            "Epoch 1/150\n",
            "768/768 [==============================] - 0s 196us/sample - loss: 3.5274 - accuracy: 0.6003\n",
            "Epoch 2/150\n",
            "768/768 [==============================] - 0s 54us/sample - loss: 2.5490 - accuracy: 0.6198\n",
            "Epoch 3/150\n",
            "768/768 [==============================] - 0s 64us/sample - loss: 2.0719 - accuracy: 0.6146\n",
            "Epoch 4/150\n",
            "768/768 [==============================] - 0s 52us/sample - loss: 1.8093 - accuracy: 0.6029\n",
            "Epoch 5/150\n",
            "768/768 [==============================] - 0s 63us/sample - loss: 1.6390 - accuracy: 0.6016\n",
            "Epoch 6/150\n",
            "768/768 [==============================] - 0s 66us/sample - loss: 1.5165 - accuracy: 0.6029\n",
            "Epoch 7/150\n",
            "768/768 [==============================] - 0s 59us/sample - loss: 1.3848 - accuracy: 0.6107\n",
            "Epoch 8/150\n",
            "768/768 [==============================] - 0s 50us/sample - loss: 1.2879 - accuracy: 0.6107\n",
            "Epoch 9/150\n",
            "768/768 [==============================] - 0s 65us/sample - loss: 1.2042 - accuracy: 0.6276\n",
            "Epoch 10/150\n",
            "768/768 [==============================] - 0s 70us/sample - loss: 1.1600 - accuracy: 0.6250\n",
            "Epoch 11/150\n",
            "768/768 [==============================] - 0s 61us/sample - loss: 1.0510 - accuracy: 0.6224\n",
            "Epoch 12/150\n",
            "768/768 [==============================] - 0s 50us/sample - loss: 1.0037 - accuracy: 0.6315\n",
            "Epoch 13/150\n",
            "768/768 [==============================] - 0s 56us/sample - loss: 0.9684 - accuracy: 0.6497\n",
            "Epoch 14/150\n",
            "768/768 [==============================] - 0s 64us/sample - loss: 0.8878 - accuracy: 0.6432\n",
            "Epoch 15/150\n",
            "768/768 [==============================] - 0s 60us/sample - loss: 0.8566 - accuracy: 0.6523\n",
            "Epoch 16/150\n",
            "768/768 [==============================] - 0s 55us/sample - loss: 0.8784 - accuracy: 0.6471\n",
            "Epoch 17/150\n",
            "768/768 [==============================] - 0s 66us/sample - loss: 0.7922 - accuracy: 0.6602\n",
            "Epoch 18/150\n",
            "768/768 [==============================] - 0s 63us/sample - loss: 0.7731 - accuracy: 0.6706\n",
            "Epoch 19/150\n",
            "768/768 [==============================] - 0s 65us/sample - loss: 0.7552 - accuracy: 0.6589\n",
            "Epoch 20/150\n",
            "768/768 [==============================] - 0s 69us/sample - loss: 0.7320 - accuracy: 0.6680\n",
            "Epoch 21/150\n",
            "768/768 [==============================] - 0s 57us/sample - loss: 0.7162 - accuracy: 0.6784\n",
            "Epoch 22/150\n",
            "768/768 [==============================] - 0s 69us/sample - loss: 0.7105 - accuracy: 0.6836\n",
            "Epoch 23/150\n",
            "768/768 [==============================] - 0s 59us/sample - loss: 0.6922 - accuracy: 0.6654\n",
            "Epoch 24/150\n",
            "768/768 [==============================] - 0s 61us/sample - loss: 0.6857 - accuracy: 0.6732\n",
            "Epoch 25/150\n",
            "768/768 [==============================] - 0s 55us/sample - loss: 0.6792 - accuracy: 0.6680\n",
            "Epoch 26/150\n",
            "768/768 [==============================] - 0s 53us/sample - loss: 0.6686 - accuracy: 0.6914\n",
            "Epoch 27/150\n",
            "768/768 [==============================] - 0s 52us/sample - loss: 0.6641 - accuracy: 0.6758\n",
            "Epoch 28/150\n",
            "768/768 [==============================] - 0s 54us/sample - loss: 0.6717 - accuracy: 0.6719\n",
            "Epoch 29/150\n",
            "768/768 [==============================] - 0s 49us/sample - loss: 0.6590 - accuracy: 0.6862\n",
            "Epoch 30/150\n",
            "768/768 [==============================] - 0s 60us/sample - loss: 0.6430 - accuracy: 0.6940\n",
            "Epoch 31/150\n",
            "768/768 [==============================] - 0s 49us/sample - loss: 0.6288 - accuracy: 0.6862\n",
            "Epoch 32/150\n",
            "768/768 [==============================] - 0s 62us/sample - loss: 0.6340 - accuracy: 0.6745\n",
            "Epoch 33/150\n",
            "768/768 [==============================] - 0s 60us/sample - loss: 0.6187 - accuracy: 0.6823\n",
            "Epoch 34/150\n",
            "768/768 [==============================] - 0s 65us/sample - loss: 0.6268 - accuracy: 0.6901\n",
            "Epoch 35/150\n",
            "768/768 [==============================] - 0s 60us/sample - loss: 0.6222 - accuracy: 0.6862\n",
            "Epoch 36/150\n",
            "768/768 [==============================] - 0s 54us/sample - loss: 0.6158 - accuracy: 0.6888\n",
            "Epoch 37/150\n",
            "768/768 [==============================] - 0s 59us/sample - loss: 0.6250 - accuracy: 0.6953\n",
            "Epoch 38/150\n",
            "768/768 [==============================] - 0s 53us/sample - loss: 0.6026 - accuracy: 0.6940\n",
            "Epoch 39/150\n",
            "768/768 [==============================] - 0s 74us/sample - loss: 0.5997 - accuracy: 0.6849\n",
            "Epoch 40/150\n",
            "768/768 [==============================] - 0s 57us/sample - loss: 0.5926 - accuracy: 0.6966\n",
            "Epoch 41/150\n",
            "768/768 [==============================] - 0s 64us/sample - loss: 0.6045 - accuracy: 0.6810\n",
            "Epoch 42/150\n",
            "768/768 [==============================] - 0s 55us/sample - loss: 0.6011 - accuracy: 0.7031\n",
            "Epoch 43/150\n",
            "768/768 [==============================] - 0s 51us/sample - loss: 0.5873 - accuracy: 0.7005\n",
            "Epoch 44/150\n",
            "768/768 [==============================] - 0s 61us/sample - loss: 0.5917 - accuracy: 0.6927\n",
            "Epoch 45/150\n",
            "768/768 [==============================] - 0s 60us/sample - loss: 0.5873 - accuracy: 0.6888\n",
            "Epoch 46/150\n",
            "768/768 [==============================] - 0s 59us/sample - loss: 0.5812 - accuracy: 0.6927\n",
            "Epoch 47/150\n",
            "768/768 [==============================] - 0s 65us/sample - loss: 0.5760 - accuracy: 0.7109\n",
            "Epoch 48/150\n",
            "768/768 [==============================] - 0s 103us/sample - loss: 0.5755 - accuracy: 0.7122\n",
            "Epoch 49/150\n",
            "768/768 [==============================] - 0s 55us/sample - loss: 0.5782 - accuracy: 0.7083\n",
            "Epoch 50/150\n",
            "768/768 [==============================] - 0s 55us/sample - loss: 0.5742 - accuracy: 0.7096\n",
            "Epoch 51/150\n",
            "768/768 [==============================] - 0s 49us/sample - loss: 0.5757 - accuracy: 0.6979\n",
            "Epoch 52/150\n",
            "768/768 [==============================] - 0s 72us/sample - loss: 0.5647 - accuracy: 0.7161\n",
            "Epoch 53/150\n",
            "768/768 [==============================] - 0s 87us/sample - loss: 0.5713 - accuracy: 0.7148\n",
            "Epoch 54/150\n",
            "768/768 [==============================] - 0s 60us/sample - loss: 0.5750 - accuracy: 0.7122\n",
            "Epoch 55/150\n",
            "768/768 [==============================] - 0s 53us/sample - loss: 0.5674 - accuracy: 0.7122\n",
            "Epoch 56/150\n",
            "768/768 [==============================] - 0s 70us/sample - loss: 0.5761 - accuracy: 0.7070\n",
            "Epoch 57/150\n",
            "768/768 [==============================] - 0s 50us/sample - loss: 0.5704 - accuracy: 0.7148\n",
            "Epoch 58/150\n",
            "768/768 [==============================] - 0s 74us/sample - loss: 0.5584 - accuracy: 0.7122\n",
            "Epoch 59/150\n",
            "768/768 [==============================] - 0s 66us/sample - loss: 0.5742 - accuracy: 0.7109\n",
            "Epoch 60/150\n",
            "768/768 [==============================] - 0s 59us/sample - loss: 0.5856 - accuracy: 0.6966\n",
            "Epoch 61/150\n",
            "768/768 [==============================] - 0s 55us/sample - loss: 0.5892 - accuracy: 0.6914\n",
            "Epoch 62/150\n",
            "768/768 [==============================] - 0s 54us/sample - loss: 0.5753 - accuracy: 0.7174\n",
            "Epoch 63/150\n",
            "768/768 [==============================] - 0s 50us/sample - loss: 0.5624 - accuracy: 0.7174\n",
            "Epoch 64/150\n",
            "768/768 [==============================] - 0s 41us/sample - loss: 0.5651 - accuracy: 0.7135\n",
            "Epoch 65/150\n",
            "768/768 [==============================] - 0s 46us/sample - loss: 0.5595 - accuracy: 0.7161\n",
            "Epoch 66/150\n",
            "768/768 [==============================] - 0s 47us/sample - loss: 0.5558 - accuracy: 0.7253\n",
            "Epoch 67/150\n",
            "768/768 [==============================] - 0s 51us/sample - loss: 0.5562 - accuracy: 0.7096\n",
            "Epoch 68/150\n",
            "768/768 [==============================] - 0s 54us/sample - loss: 0.5706 - accuracy: 0.7122\n",
            "Epoch 69/150\n",
            "768/768 [==============================] - 0s 52us/sample - loss: 0.5558 - accuracy: 0.7214\n",
            "Epoch 70/150\n",
            "768/768 [==============================] - 0s 51us/sample - loss: 0.5574 - accuracy: 0.7188\n",
            "Epoch 71/150\n",
            "768/768 [==============================] - 0s 49us/sample - loss: 0.5483 - accuracy: 0.7253\n",
            "Epoch 72/150\n",
            "768/768 [==============================] - 0s 57us/sample - loss: 0.5503 - accuracy: 0.7240\n",
            "Epoch 73/150\n",
            "768/768 [==============================] - 0s 47us/sample - loss: 0.5577 - accuracy: 0.7161\n",
            "Epoch 74/150\n",
            "768/768 [==============================] - 0s 42us/sample - loss: 0.5574 - accuracy: 0.7161\n",
            "Epoch 75/150\n",
            "768/768 [==============================] - 0s 46us/sample - loss: 0.5559 - accuracy: 0.7383\n",
            "Epoch 76/150\n",
            "768/768 [==============================] - 0s 45us/sample - loss: 0.5462 - accuracy: 0.7227\n",
            "Epoch 77/150\n",
            "768/768 [==============================] - 0s 51us/sample - loss: 0.5463 - accuracy: 0.7174\n",
            "Epoch 78/150\n",
            "768/768 [==============================] - 0s 47us/sample - loss: 0.5501 - accuracy: 0.7305\n",
            "Epoch 79/150\n",
            "768/768 [==============================] - 0s 43us/sample - loss: 0.5574 - accuracy: 0.7266\n",
            "Epoch 80/150\n",
            "768/768 [==============================] - 0s 50us/sample - loss: 0.5552 - accuracy: 0.7122\n",
            "Epoch 81/150\n",
            "768/768 [==============================] - 0s 54us/sample - loss: 0.5479 - accuracy: 0.7266\n",
            "Epoch 82/150\n",
            "768/768 [==============================] - 0s 49us/sample - loss: 0.5440 - accuracy: 0.7331\n",
            "Epoch 83/150\n",
            "768/768 [==============================] - 0s 40us/sample - loss: 0.5487 - accuracy: 0.7188\n",
            "Epoch 84/150\n",
            "768/768 [==============================] - 0s 48us/sample - loss: 0.5723 - accuracy: 0.7109\n",
            "Epoch 85/150\n",
            "768/768 [==============================] - 0s 47us/sample - loss: 0.5688 - accuracy: 0.7174\n",
            "Epoch 86/150\n",
            "768/768 [==============================] - 0s 41us/sample - loss: 0.5639 - accuracy: 0.7096\n",
            "Epoch 87/150\n",
            "768/768 [==============================] - 0s 44us/sample - loss: 0.5425 - accuracy: 0.7214\n",
            "Epoch 88/150\n",
            "768/768 [==============================] - 0s 66us/sample - loss: 0.5448 - accuracy: 0.7253\n",
            "Epoch 89/150\n",
            "768/768 [==============================] - 0s 57us/sample - loss: 0.5380 - accuracy: 0.7279\n",
            "Epoch 90/150\n",
            "768/768 [==============================] - 0s 59us/sample - loss: 0.5569 - accuracy: 0.7279\n",
            "Epoch 91/150\n",
            "768/768 [==============================] - 0s 56us/sample - loss: 0.5433 - accuracy: 0.7161\n",
            "Epoch 92/150\n",
            "768/768 [==============================] - 0s 54us/sample - loss: 0.5573 - accuracy: 0.7161\n",
            "Epoch 93/150\n",
            "768/768 [==============================] - 0s 72us/sample - loss: 0.5489 - accuracy: 0.7305\n",
            "Epoch 94/150\n",
            "768/768 [==============================] - 0s 49us/sample - loss: 0.5413 - accuracy: 0.7214\n",
            "Epoch 95/150\n",
            "768/768 [==============================] - 0s 60us/sample - loss: 0.5391 - accuracy: 0.7318\n",
            "Epoch 96/150\n",
            "768/768 [==============================] - 0s 57us/sample - loss: 0.5363 - accuracy: 0.7253\n",
            "Epoch 97/150\n",
            "768/768 [==============================] - 0s 67us/sample - loss: 0.5419 - accuracy: 0.7214\n",
            "Epoch 98/150\n",
            "768/768 [==============================] - 0s 56us/sample - loss: 0.5384 - accuracy: 0.7357\n",
            "Epoch 99/150\n",
            "768/768 [==============================] - 0s 67us/sample - loss: 0.5350 - accuracy: 0.7266\n",
            "Epoch 100/150\n",
            "768/768 [==============================] - 0s 60us/sample - loss: 0.5460 - accuracy: 0.7161\n",
            "Epoch 101/150\n",
            "768/768 [==============================] - 0s 64us/sample - loss: 0.5469 - accuracy: 0.7227\n",
            "Epoch 102/150\n",
            "768/768 [==============================] - 0s 64us/sample - loss: 0.5385 - accuracy: 0.7422\n",
            "Epoch 103/150\n",
            "768/768 [==============================] - 0s 64us/sample - loss: 0.5402 - accuracy: 0.7253\n",
            "Epoch 104/150\n",
            "768/768 [==============================] - 0s 74us/sample - loss: 0.5428 - accuracy: 0.7240\n",
            "Epoch 105/150\n",
            "768/768 [==============================] - 0s 65us/sample - loss: 0.5291 - accuracy: 0.7357\n",
            "Epoch 106/150\n",
            "768/768 [==============================] - 0s 59us/sample - loss: 0.5436 - accuracy: 0.7214\n",
            "Epoch 107/150\n",
            "768/768 [==============================] - 0s 57us/sample - loss: 0.5276 - accuracy: 0.7370\n",
            "Epoch 108/150\n",
            "768/768 [==============================] - 0s 53us/sample - loss: 0.5326 - accuracy: 0.7201\n",
            "Epoch 109/150\n",
            "768/768 [==============================] - 0s 70us/sample - loss: 0.5299 - accuracy: 0.7383\n",
            "Epoch 110/150\n",
            "768/768 [==============================] - 0s 59us/sample - loss: 0.5310 - accuracy: 0.7344\n",
            "Epoch 111/150\n",
            "768/768 [==============================] - 0s 62us/sample - loss: 0.5304 - accuracy: 0.7331\n",
            "Epoch 112/150\n",
            "768/768 [==============================] - 0s 57us/sample - loss: 0.5355 - accuracy: 0.7279\n",
            "Epoch 113/150\n",
            "768/768 [==============================] - 0s 63us/sample - loss: 0.5360 - accuracy: 0.7422\n",
            "Epoch 114/150\n",
            "768/768 [==============================] - 0s 60us/sample - loss: 0.5354 - accuracy: 0.7188\n",
            "Epoch 115/150\n",
            "768/768 [==============================] - 0s 51us/sample - loss: 0.5406 - accuracy: 0.7279\n",
            "Epoch 116/150\n",
            "768/768 [==============================] - 0s 71us/sample - loss: 0.5358 - accuracy: 0.7331\n",
            "Epoch 117/150\n",
            "768/768 [==============================] - 0s 63us/sample - loss: 0.5299 - accuracy: 0.7318\n",
            "Epoch 118/150\n",
            "768/768 [==============================] - 0s 70us/sample - loss: 0.5240 - accuracy: 0.7422\n",
            "Epoch 119/150\n",
            "768/768 [==============================] - 0s 66us/sample - loss: 0.5408 - accuracy: 0.7188\n",
            "Epoch 120/150\n",
            "768/768 [==============================] - 0s 58us/sample - loss: 0.5376 - accuracy: 0.7357\n",
            "Epoch 121/150\n",
            "768/768 [==============================] - 0s 68us/sample - loss: 0.5373 - accuracy: 0.7461\n",
            "Epoch 122/150\n",
            "768/768 [==============================] - 0s 62us/sample - loss: 0.5380 - accuracy: 0.7357\n",
            "Epoch 123/150\n",
            "768/768 [==============================] - 0s 62us/sample - loss: 0.5246 - accuracy: 0.7461\n",
            "Epoch 124/150\n",
            "768/768 [==============================] - 0s 62us/sample - loss: 0.5228 - accuracy: 0.7383\n",
            "Epoch 125/150\n",
            "768/768 [==============================] - 0s 64us/sample - loss: 0.5239 - accuracy: 0.7396\n",
            "Epoch 126/150\n",
            "768/768 [==============================] - 0s 59us/sample - loss: 0.5242 - accuracy: 0.7448\n",
            "Epoch 127/150\n",
            "768/768 [==============================] - 0s 65us/sample - loss: 0.5267 - accuracy: 0.7435\n",
            "Epoch 128/150\n",
            "768/768 [==============================] - 0s 52us/sample - loss: 0.5203 - accuracy: 0.7513\n",
            "Epoch 129/150\n",
            "768/768 [==============================] - 0s 62us/sample - loss: 0.5255 - accuracy: 0.7370\n",
            "Epoch 130/150\n",
            "768/768 [==============================] - 0s 68us/sample - loss: 0.5224 - accuracy: 0.7344\n",
            "Epoch 131/150\n",
            "768/768 [==============================] - 0s 68us/sample - loss: 0.5194 - accuracy: 0.7448\n",
            "Epoch 132/150\n",
            "768/768 [==============================] - 0s 54us/sample - loss: 0.5218 - accuracy: 0.7383\n",
            "Epoch 133/150\n",
            "768/768 [==============================] - 0s 64us/sample - loss: 0.5315 - accuracy: 0.7396\n",
            "Epoch 134/150\n",
            "768/768 [==============================] - 0s 54us/sample - loss: 0.5466 - accuracy: 0.7396\n",
            "Epoch 135/150\n",
            "768/768 [==============================] - 0s 52us/sample - loss: 0.5521 - accuracy: 0.7214\n",
            "Epoch 136/150\n",
            "768/768 [==============================] - 0s 58us/sample - loss: 0.5314 - accuracy: 0.7487\n",
            "Epoch 137/150\n",
            "768/768 [==============================] - 0s 61us/sample - loss: 0.5267 - accuracy: 0.7383\n",
            "Epoch 138/150\n",
            "768/768 [==============================] - 0s 59us/sample - loss: 0.5182 - accuracy: 0.7474\n",
            "Epoch 139/150\n",
            "768/768 [==============================] - 0s 59us/sample - loss: 0.5188 - accuracy: 0.7461\n",
            "Epoch 140/150\n",
            "768/768 [==============================] - 0s 71us/sample - loss: 0.5227 - accuracy: 0.7409\n",
            "Epoch 141/150\n",
            "768/768 [==============================] - 0s 50us/sample - loss: 0.5198 - accuracy: 0.7422\n",
            "Epoch 142/150\n",
            "768/768 [==============================] - 0s 60us/sample - loss: 0.5173 - accuracy: 0.7487\n",
            "Epoch 143/150\n",
            "768/768 [==============================] - 0s 64us/sample - loss: 0.5191 - accuracy: 0.7435\n",
            "Epoch 144/150\n",
            "768/768 [==============================] - 0s 61us/sample - loss: 0.5152 - accuracy: 0.7409\n",
            "Epoch 145/150\n",
            "768/768 [==============================] - 0s 48us/sample - loss: 0.5283 - accuracy: 0.7409\n",
            "Epoch 146/150\n",
            "768/768 [==============================] - 0s 64us/sample - loss: 0.5218 - accuracy: 0.7500\n",
            "Epoch 147/150\n",
            "768/768 [==============================] - 0s 58us/sample - loss: 0.5201 - accuracy: 0.7422\n",
            "Epoch 148/150\n",
            "768/768 [==============================] - 0s 54us/sample - loss: 0.5305 - accuracy: 0.7344\n",
            "Epoch 149/150\n",
            "768/768 [==============================] - 0s 59us/sample - loss: 0.5186 - accuracy: 0.7318\n",
            "Epoch 150/150\n",
            "768/768 [==============================] - 0s 65us/sample - loss: 0.5338 - accuracy: 0.7266\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_L_PFCvgrVvs",
        "colab_type": "text"
      },
      "source": [
        "> `history` now contains values for `loss` and `accuracy` during training, let's plot them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rM545cehrVvt",
        "colab_type": "code",
        "outputId": "4fab2e70-c65a-4b90-d0c5-36dca8b0af1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        }
      },
      "source": [
        "pd.DataFrame(nn_clf_history.history).plot(figsize=(10, 5))\n",
        "plt.grid(True)\n",
        "\n",
        "# set the y-axis range to [0-1]\n",
        "plt.gca().set_ylim(0, 1) "
      ],
      "execution_count": 277,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 277
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAEzCAYAAAAVXYYvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8H/BfIssL+bN6QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FAnbqxmrVvv",
        "colab_type": "text"
      },
      "source": [
        "## <font color=\"red\"> Required Coding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGhtmZGlrVvw",
        "colab_type": "code",
        "outputId": "fad435c3-117a-4121-85e9-5cd9f1f78370",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "### START CODING HERE ###\n",
        "# evaluate nn_clf model on test_batch using .evaluate method\n",
        "_, accuracy = nn_clf.evaluate(X, y)\n",
        "print('Accuracy is: %.6f' % (accuracy * 100))\n",
        "### END CODING HERE ###"
      ],
      "execution_count": 278,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
            "768/768 [==============================] - 0s 52us/sample - loss: 0.5108 - accuracy: 0.7396\n",
            "Accuracy is: 73.958331\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6yjwgzfrVvz",
        "colab_type": "code",
        "outputId": "ee953d54-7f79-464f-d51a-999c8d9b3d43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "nn_clf.summary()"
      ],
      "execution_count": 279,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_21\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_63 (Dense)             (None, 12)                108       \n",
            "_________________________________________________________________\n",
            "dense_64 (Dense)             (None, 8)                 104       \n",
            "_________________________________________________________________\n",
            "dense_65 (Dense)             (None, 1)                 9         \n",
            "=================================================================\n",
            "Total params: 221\n",
            "Trainable params: 221\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPCDWiT_rVv1",
        "colab_type": "text"
      },
      "source": [
        "> Now, let's plot ROC curve for this neural network model. Recall from Assignment-4 that you need to get class probabilities, fpr and tpr. To get class probabilities, keras has `predict()` method. Notice that it's applied on `X_test` not `test_batch`. Alternatively, you can use `predict_proba()` method, similar to sklearn, which would generate identical results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJN6FDwsrVv2",
        "colab_type": "code",
        "outputId": "42e90faa-9d9a-4599-b6ef-66e72d8ef908",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Get class probabilities for nn - ignore the warning\n",
        "nn_preds = nn_clf.predict(X_test).ravel()"
      ],
      "execution_count": 280,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsX3NOzXrVv8",
        "colab_type": "code",
        "outputId": "8072a35f-dec7-4787-f03f-d80c14fe573e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# See class probabilities predicted by nn classifier\n",
        "nn_preds[:5]"
      ],
      "execution_count": 281,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.12393677, 0.17900467, 0.18561217, 0.23749372, 0.02067345],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 281
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7yAXRNsrVwA",
        "colab_type": "text"
      },
      "source": [
        "## <font color=\"red\"> Required Coding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCyjHmImrVwA",
        "colab_type": "code",
        "outputId": "cc194544-89bb-42dc-9617-7a343c466a57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "### START CODING HERE ### \n",
        "# Plot ROC curve for nn_clf - Write as many lines of code as needed\n",
        "# Hint: check back your Assignment-4 code, you need to calculate tpr, fpr, thresholds\n",
        "# Plot should have all the elements that Assignment-4 ROC curves had, title, xlabel and ylabel, xlim & ylim\n",
        "# Plot should also have AUC_NN (roc_auc) shown on lower right\n",
        "\n",
        "#nn_preds already declared earlier, no need for it here\n",
        "nn_fpr, nn_tpr, nn_threshold = metrics.roc_curve(y_test, nn_preds)\n",
        "nn_roc_auc = metrics.auc(nn_fpr, nn_tpr)\n",
        "\n",
        "# Graph components to follow\n",
        "plt.title('ROC Graph - Neural Network Classifier')\n",
        "plt.plot(nn_fpr, nn_tpr, 'g', label='AUC_NN - %.2f' % nn_roc_auc)\n",
        "\n",
        "# Plot the points, limits (both x & y's = 1), and the labels for x & y\n",
        "plt.plot([0,1],[0,1],'b--')   # sets the graph view\n",
        "plt.xlim([0,1])  # x ranges from 0->1\n",
        "plt.ylim([0,1])  # y ranges from 0->1\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "\n",
        "### END CODING HERE ###"
      ],
      "execution_count": 282,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'True Positive Rate')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 282
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nISIiKSlRiIhISv8PeR35mi57znEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lL_FN8d5rVwC",
        "colab_type": "text"
      },
      "source": [
        "## Part I - Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KypMAArrVwD",
        "colab_type": "text"
      },
      "source": [
        "<b>ANSWER THE FOLLOWING QUESTIONS HERE:</b><br>\n",
        "\n",
        "Q1 - If this problem was a multi-class classification, what activation function would you use for the output layer neurons? How many neurons would be required for the output layer for multi-class? What other hyperparameters of nn can you change in different tasks? Name at least 3 other hyperparameters. GIVE COMPLETE ANSWER!\n",
        "<br>`Answer: You would need to use the softmax activation function and would need 3 neurons for the outer layer. The other hyperparameters you can change, in different tasks, are 1) learning rate, which can change how the accuracy is represented, 2) the number of neurons in hidden layers, which adds depth, and 3) batch size, which can affect the AUC.`\n",
        "\n",
        "\n",
        "Q2 - Change the batch number from 1 to 10. Would it improve or hurt the results? Make an argument with reasoning on your observation (you may consult with page 326 of the textbook).\n",
        "<br>`Answer: Changing the batch size from 1 to 10 improves the results. While the loss vs accuracy graph isn't as smooth, meaning it is more jagged, the overall accuracy seems to improve and the AUC also greatly improves compared to before. I believe this occurs because only having 1 batch to work with is very limiting. While having too many batches can be overbearing and take up more time and space, having a larger number than 1 is a good idea. 10 batches gives the machine a chance to look at this 10 times and then gather the results. Please note that I tested this multiple times with both train and test at 10 batches, since that is what the instructions said to do. Doing this appeared to generate better results, even if minor, overall.`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHVb93mtrVwD",
        "colab_type": "text"
      },
      "source": [
        "## Part II - Regression with NNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hAN_LJ1rVwE",
        "colab_type": "text"
      },
      "source": [
        "In this part, you will create a neural network to do a regression task.\n",
        "\n",
        "[Download the data from here](https://github.com/fereydoonvafaei/UMBC-CMSC-471-Fall-2019/blob/master/Assignment-5/auto.csv). This is cars dataset. You can read more about the data [here](https://archive.ics.uci.edu/ml/datasets/auto+mpg). The goal is predicting MPG based on other features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60KKRfDNrVwE",
        "colab_type": "code",
        "outputId": "43a550ed-f5e1-4c50-c72e-a5bf10668829",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "auto_data = pd.read_csv('auto.csv')\n",
        "print(auto_data.shape)\n",
        "auto_data.head()"
      ],
      "execution_count": 283,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(398, 8)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MPG</th>\n",
              "      <th>Cylinders</th>\n",
              "      <th>Displacement</th>\n",
              "      <th>Horsepower</th>\n",
              "      <th>Weight</th>\n",
              "      <th>Acceleration</th>\n",
              "      <th>Model Year</th>\n",
              "      <th>Origin</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>18.0</td>\n",
              "      <td>8</td>\n",
              "      <td>307.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>3504.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>15.0</td>\n",
              "      <td>8</td>\n",
              "      <td>350.0</td>\n",
              "      <td>165.0</td>\n",
              "      <td>3693.0</td>\n",
              "      <td>11.5</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>18.0</td>\n",
              "      <td>8</td>\n",
              "      <td>318.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>3436.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>16.0</td>\n",
              "      <td>8</td>\n",
              "      <td>304.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>3433.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>17.0</td>\n",
              "      <td>8</td>\n",
              "      <td>302.0</td>\n",
              "      <td>140.0</td>\n",
              "      <td>3449.0</td>\n",
              "      <td>10.5</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    MPG  Cylinders  Displacement  ...  Acceleration  Model Year  Origin\n",
              "0  18.0          8         307.0  ...          12.0          70       1\n",
              "1  15.0          8         350.0  ...          11.5          70       1\n",
              "2  18.0          8         318.0  ...          11.0          70       1\n",
              "3  16.0          8         304.0  ...          12.0          70       1\n",
              "4  17.0          8         302.0  ...          10.5          70       1\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 283
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fd3LjTMcrVwG",
        "colab_type": "code",
        "outputId": "439132cb-2905-4aa7-d9ff-cdbf06b70a5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "auto_data.isna().sum()"
      ],
      "execution_count": 284,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MPG             0\n",
              "Cylinders       0\n",
              "Displacement    0\n",
              "Horsepower      6\n",
              "Weight          0\n",
              "Acceleration    0\n",
              "Model Year      0\n",
              "Origin          0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 284
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62Wjj3mOrVwI",
        "colab_type": "text"
      },
      "source": [
        "## <font color=\"red\"> Required Coding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHXvYRRvrVwJ",
        "colab_type": "code",
        "outputId": "a183a80e-cddb-4d2c-8af2-93b20a071b99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "### START CODING HERE ### \n",
        "# Drop all na's using dataframe .dropna(inplace=True) method ~ 1 line\n",
        "auto_data.dropna(inplace = True)\n",
        "### END CODING HERE ###\n",
        "auto_data.isna().sum()"
      ],
      "execution_count": 285,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MPG             0\n",
              "Cylinders       0\n",
              "Displacement    0\n",
              "Horsepower      0\n",
              "Weight          0\n",
              "Acceleration    0\n",
              "Model Year      0\n",
              "Origin          0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 285
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "830BPDAnrVwL",
        "colab_type": "text"
      },
      "source": [
        ">For some datasets like this, a technique in data preprocessing is used to encode categorical features to dummy variables. Here, we convert <b>Origin</b> (which looks numeric but is actually categorical) using pandas [get_dummies](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html) method. This technique is one example of One Hot Encoding of categorical features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoOjVn54rVwL",
        "colab_type": "code",
        "outputId": "8bfb8168-018c-4853-c6fa-7ab86cea359c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "auto_data['Origin'] = auto_data['Origin'].map(lambda x: {1: 'USA', 2: 'Europe', 3: 'Japan'}.get(x))\n",
        "auto_data = pd.get_dummies(auto_data, prefix='', prefix_sep='')\n",
        "print(auto_data.shape)\n",
        "auto_data.head()"
      ],
      "execution_count": 286,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(392, 10)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MPG</th>\n",
              "      <th>Cylinders</th>\n",
              "      <th>Displacement</th>\n",
              "      <th>Horsepower</th>\n",
              "      <th>Weight</th>\n",
              "      <th>Acceleration</th>\n",
              "      <th>Model Year</th>\n",
              "      <th>Europe</th>\n",
              "      <th>Japan</th>\n",
              "      <th>USA</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>18.0</td>\n",
              "      <td>8</td>\n",
              "      <td>307.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>3504.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>70</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>15.0</td>\n",
              "      <td>8</td>\n",
              "      <td>350.0</td>\n",
              "      <td>165.0</td>\n",
              "      <td>3693.0</td>\n",
              "      <td>11.5</td>\n",
              "      <td>70</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>18.0</td>\n",
              "      <td>8</td>\n",
              "      <td>318.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>3436.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>70</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>16.0</td>\n",
              "      <td>8</td>\n",
              "      <td>304.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>3433.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>70</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>17.0</td>\n",
              "      <td>8</td>\n",
              "      <td>302.0</td>\n",
              "      <td>140.0</td>\n",
              "      <td>3449.0</td>\n",
              "      <td>10.5</td>\n",
              "      <td>70</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    MPG  Cylinders  Displacement  Horsepower  ...  Model Year  Europe  Japan  USA\n",
              "0  18.0          8         307.0       130.0  ...          70       0      0    1\n",
              "1  15.0          8         350.0       165.0  ...          70       0      0    1\n",
              "2  18.0          8         318.0       150.0  ...          70       0      0    1\n",
              "3  16.0          8         304.0       150.0  ...          70       0      0    1\n",
              "4  17.0          8         302.0       140.0  ...          70       0      0    1\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 286
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a083g3tbrVwN",
        "colab_type": "text"
      },
      "source": [
        "## <font color=\"red\"> Required Coding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_wNnEQ5rVwN",
        "colab_type": "code",
        "outputId": "e06a833a-a1b7-4778-b86e-0c70013272e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "### START CODING HERE ### \n",
        "# Create X and y, X should contain all features except MPG column and y should only contain MPG column\n",
        "# Hint: You can use dataframe .pop() method, but you may need to create a deep copy of the dataframe first\n",
        "# There are usually multiple ways of doing these operations in pnadas dataframes\n",
        "X = auto_data.drop(columns = \"MPG\")\n",
        "y = auto_data['MPG']\n",
        "### END CODING HERE ###\n",
        "print(X.shape)\n",
        "print(y.shape)\n",
        "X.head()"
      ],
      "execution_count": 287,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(392, 9)\n",
            "(392,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Cylinders</th>\n",
              "      <th>Displacement</th>\n",
              "      <th>Horsepower</th>\n",
              "      <th>Weight</th>\n",
              "      <th>Acceleration</th>\n",
              "      <th>Model Year</th>\n",
              "      <th>Europe</th>\n",
              "      <th>Japan</th>\n",
              "      <th>USA</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8</td>\n",
              "      <td>307.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>3504.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>70</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8</td>\n",
              "      <td>350.0</td>\n",
              "      <td>165.0</td>\n",
              "      <td>3693.0</td>\n",
              "      <td>11.5</td>\n",
              "      <td>70</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>318.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>3436.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>70</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8</td>\n",
              "      <td>304.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>3433.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>70</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8</td>\n",
              "      <td>302.0</td>\n",
              "      <td>140.0</td>\n",
              "      <td>3449.0</td>\n",
              "      <td>10.5</td>\n",
              "      <td>70</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Cylinders  Displacement  Horsepower  Weight  ...  Model Year  Europe  Japan  USA\n",
              "0          8         307.0       130.0  3504.0  ...          70       0      0    1\n",
              "1          8         350.0       165.0  3693.0  ...          70       0      0    1\n",
              "2          8         318.0       150.0  3436.0  ...          70       0      0    1\n",
              "3          8         304.0       150.0  3433.0  ...          70       0      0    1\n",
              "4          8         302.0       140.0  3449.0  ...          70       0      0    1\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 287
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0rITc1urVwQ",
        "colab_type": "text"
      },
      "source": [
        "> <b>Note:</b> The original auto_data dataframe should remain the same and should still include MPG."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJonHDv_rVwQ",
        "colab_type": "code",
        "outputId": "ddda8cfc-886d-482d-a26a-377e7ee4eeb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "auto_data.head()"
      ],
      "execution_count": 288,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MPG</th>\n",
              "      <th>Cylinders</th>\n",
              "      <th>Displacement</th>\n",
              "      <th>Horsepower</th>\n",
              "      <th>Weight</th>\n",
              "      <th>Acceleration</th>\n",
              "      <th>Model Year</th>\n",
              "      <th>Europe</th>\n",
              "      <th>Japan</th>\n",
              "      <th>USA</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>18.0</td>\n",
              "      <td>8</td>\n",
              "      <td>307.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>3504.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>70</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>15.0</td>\n",
              "      <td>8</td>\n",
              "      <td>350.0</td>\n",
              "      <td>165.0</td>\n",
              "      <td>3693.0</td>\n",
              "      <td>11.5</td>\n",
              "      <td>70</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>18.0</td>\n",
              "      <td>8</td>\n",
              "      <td>318.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>3436.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>70</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>16.0</td>\n",
              "      <td>8</td>\n",
              "      <td>304.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>3433.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>70</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>17.0</td>\n",
              "      <td>8</td>\n",
              "      <td>302.0</td>\n",
              "      <td>140.0</td>\n",
              "      <td>3449.0</td>\n",
              "      <td>10.5</td>\n",
              "      <td>70</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    MPG  Cylinders  Displacement  Horsepower  ...  Model Year  Europe  Japan  USA\n",
              "0  18.0          8         307.0       130.0  ...          70       0      0    1\n",
              "1  15.0          8         350.0       165.0  ...          70       0      0    1\n",
              "2  18.0          8         318.0       150.0  ...          70       0      0    1\n",
              "3  16.0          8         304.0       150.0  ...          70       0      0    1\n",
              "4  17.0          8         302.0       140.0  ...          70       0      0    1\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 288
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2nMZG26rVwU",
        "colab_type": "text"
      },
      "source": [
        ">Normalization is a good pratice when you work with values with different ranges."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoPAsLvMrVwU",
        "colab_type": "code",
        "outputId": "5cefada1-fde1-4969-f40b-9a4e2f362464",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# Normalize X \n",
        "X = (X - X.mean())/X.std()\n",
        "print(X.shape)\n",
        "X.head()"
      ],
      "execution_count": 289,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(392, 9)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Cylinders</th>\n",
              "      <th>Displacement</th>\n",
              "      <th>Horsepower</th>\n",
              "      <th>Weight</th>\n",
              "      <th>Acceleration</th>\n",
              "      <th>Model Year</th>\n",
              "      <th>Europe</th>\n",
              "      <th>Japan</th>\n",
              "      <th>USA</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.075915</td>\n",
              "      <td>0.663285</td>\n",
              "      <td>0.619748</td>\n",
              "      <td>-1.283618</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>-0.457538</td>\n",
              "      <td>-0.501749</td>\n",
              "      <td>0.773608</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.486832</td>\n",
              "      <td>1.572585</td>\n",
              "      <td>0.842258</td>\n",
              "      <td>-1.464852</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>-0.457538</td>\n",
              "      <td>-0.501749</td>\n",
              "      <td>0.773608</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.181033</td>\n",
              "      <td>1.182885</td>\n",
              "      <td>0.539692</td>\n",
              "      <td>-1.646086</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>-0.457538</td>\n",
              "      <td>-0.501749</td>\n",
              "      <td>0.773608</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.047246</td>\n",
              "      <td>1.182885</td>\n",
              "      <td>0.536160</td>\n",
              "      <td>-1.283618</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>-0.457538</td>\n",
              "      <td>-0.501749</td>\n",
              "      <td>0.773608</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.028134</td>\n",
              "      <td>0.923085</td>\n",
              "      <td>0.554997</td>\n",
              "      <td>-1.827320</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>-0.457538</td>\n",
              "      <td>-0.501749</td>\n",
              "      <td>0.773608</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Cylinders  Displacement  Horsepower  ...    Europe     Japan       USA\n",
              "0   1.482053      1.075915    0.663285  ... -0.457538 -0.501749  0.773608\n",
              "1   1.482053      1.486832    1.572585  ... -0.457538 -0.501749  0.773608\n",
              "2   1.482053      1.181033    1.182885  ... -0.457538 -0.501749  0.773608\n",
              "3   1.482053      1.047246    1.182885  ... -0.457538 -0.501749  0.773608\n",
              "4   1.482053      1.028134    0.923085  ... -0.457538 -0.501749  0.773608\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 289
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmoOli7wrVwX",
        "colab_type": "text"
      },
      "source": [
        "## <font color=\"red\"> Required Coding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrmQ4QThrVwX",
        "colab_type": "code",
        "outputId": "b82ecd2c-163a-482e-c980-23b646948bf6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "### START CODING HERE ###\n",
        "# Split the data to train and test using train_test_split method with test_size=0.2 and random_state=42\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .20, random_state = 42)\n",
        "### END CODING HERE ###\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 290,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(313, 9)\n",
            "(313,)\n",
            "(79, 9)\n",
            "(79,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmnhYPwErVwZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### START CODING HERE ### \n",
        "# Build a neural network for regression\n",
        "nn_reg1 = tf.keras.Sequential([\n",
        "    # Create a dense layer with 64 neurons, 'relu' activation function and input_shape=[len(X_train.keys())]\n",
        "    tf.keras.layers.Dense(64, activation = 'relu', input_shape = [len(X_train.keys())]),\n",
        "    # Create a dense layer with 64 neurons and 'relu' activation function\n",
        "    tf.keras.layers.Dense(64, activation = 'relu'),\n",
        "    # Create a dense layer with ? neuron(s) and ? activation\n",
        "    # YOU should decide how many neuron(s) is/are needed and what activation function (if any) to use for output\n",
        "    # Hint: What type of ML task is this problem?\n",
        "    tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "### END CODING HERE ###"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZeIZ02yrVwb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is another way of defining the optimizer, you can pass learning_rate and other parameters to it.\n",
        "optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
        "\n",
        "### START CODING HERE ### \n",
        "# Compile nn_reg1 with 'mse' loss, optimizer=optimizer, metrics=['mae', 'mse']\n",
        "nn_reg1.compile(loss = 'mse', optimizer = optimizer, metrics = ['mae','mse'])\n",
        "### END CODING HERE ###"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MDQpoVBrVwd",
        "colab_type": "code",
        "outputId": "bf7272bf-1161-4151-ed7f-4fca8fb9c176",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "nn_reg1.summary()"
      ],
      "execution_count": 293,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_22\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_66 (Dense)             (None, 64)                640       \n",
            "_________________________________________________________________\n",
            "dense_67 (Dense)             (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dense_68 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 4,865\n",
            "Trainable params: 4,865\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XywGnbn7rVwg",
        "colab_type": "text"
      },
      "source": [
        "## <font color=\"red\"> Required Coding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJ1ESug5rVwh",
        "colab_type": "code",
        "outputId": "ad22a6c0-40a6-4eef-d4fd-4694289706bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "EPOCHS = 1000\n",
        "\n",
        "### START CODING HERE ### \n",
        "# fit nn_reg1 on X_train, y_train, epochs=EPOCHS, validation_split=0.2, verbose=1\n",
        "nn_reg1_history = nn_reg1.fit(X_train, y_train, epochs = EPOCHS, validation_split = .2, verbose = 1)\n",
        "### END CODING HERE ###"
      ],
      "execution_count": 294,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
            "Train on 250 samples, validate on 63 samples\n",
            "Epoch 1/1000\n",
            "250/250 [==============================] - 0s 598us/sample - loss: 569.7200 - mae: 22.5362 - mse: 569.7200 - val_loss: 621.0953 - val_mae: 23.6862 - val_mse: 621.0952\n",
            "Epoch 2/1000\n",
            "250/250 [==============================] - 0s 75us/sample - loss: 523.8890 - mae: 21.4707 - mse: 523.8889 - val_loss: 574.2553 - val_mae: 22.6582 - val_mse: 574.2552\n",
            "Epoch 3/1000\n",
            "250/250 [==============================] - 0s 72us/sample - loss: 478.8602 - mae: 20.3627 - mse: 478.8602 - val_loss: 522.8912 - val_mae: 21.4745 - val_mse: 522.8912\n",
            "Epoch 4/1000\n",
            "250/250 [==============================] - 0s 80us/sample - loss: 429.3261 - mae: 19.0689 - mse: 429.3260 - val_loss: 464.7005 - val_mae: 20.0582 - val_mse: 464.7005\n",
            "Epoch 5/1000\n",
            "250/250 [==============================] - 0s 75us/sample - loss: 373.6935 - mae: 17.5543 - mse: 373.6935 - val_loss: 400.7120 - val_mae: 18.4084 - val_mse: 400.7120\n",
            "Epoch 6/1000\n",
            "250/250 [==============================] - 0s 57us/sample - loss: 315.8676 - mae: 15.8603 - mse: 315.8676 - val_loss: 335.0681 - val_mae: 16.5821 - val_mse: 335.0681\n",
            "Epoch 7/1000\n",
            "250/250 [==============================] - 0s 63us/sample - loss: 257.4474 - mae: 14.1359 - mse: 257.4474 - val_loss: 269.4721 - val_mae: 14.6857 - val_mse: 269.4722\n",
            "Epoch 8/1000\n",
            "250/250 [==============================] - 0s 64us/sample - loss: 200.8402 - mae: 12.3402 - mse: 200.8402 - val_loss: 206.5850 - val_mae: 12.6719 - val_mse: 206.5850\n",
            "Epoch 9/1000\n",
            "250/250 [==============================] - 0s 67us/sample - loss: 149.8778 - mae: 10.5342 - mse: 149.8778 - val_loss: 153.2803 - val_mae: 10.7874 - val_mse: 153.2803\n",
            "Epoch 10/1000\n",
            "250/250 [==============================] - 0s 86us/sample - loss: 107.1119 - mae: 8.8303 - mse: 107.1119 - val_loss: 108.5014 - val_mae: 8.8714 - val_mse: 108.5014\n",
            "Epoch 11/1000\n",
            "250/250 [==============================] - 0s 63us/sample - loss: 73.2063 - mae: 7.1846 - mse: 73.2063 - val_loss: 74.5558 - val_mae: 7.0617 - val_mse: 74.5558\n",
            "Epoch 12/1000\n",
            "250/250 [==============================] - 0s 66us/sample - loss: 49.4005 - mae: 5.7315 - mse: 49.4005 - val_loss: 52.7542 - val_mae: 5.6018 - val_mse: 52.7542\n",
            "Epoch 13/1000\n",
            "250/250 [==============================] - 0s 66us/sample - loss: 35.1800 - mae: 4.7403 - mse: 35.1800 - val_loss: 39.3190 - val_mae: 4.8145 - val_mse: 39.3190\n",
            "Epoch 14/1000\n",
            "250/250 [==============================] - 0s 90us/sample - loss: 26.7948 - mae: 4.1206 - mse: 26.7948 - val_loss: 30.5211 - val_mae: 4.1889 - val_mse: 30.5211\n",
            "Epoch 15/1000\n",
            "250/250 [==============================] - 0s 93us/sample - loss: 21.3224 - mae: 3.6480 - mse: 21.3224 - val_loss: 23.8167 - val_mae: 3.6501 - val_mse: 23.8167\n",
            "Epoch 16/1000\n",
            "250/250 [==============================] - 0s 80us/sample - loss: 17.3589 - mae: 3.2686 - mse: 17.3589 - val_loss: 19.8282 - val_mae: 3.3345 - val_mse: 19.8282\n",
            "Epoch 17/1000\n",
            "250/250 [==============================] - 0s 93us/sample - loss: 14.8137 - mae: 2.9783 - mse: 14.8137 - val_loss: 16.0822 - val_mae: 2.8820 - val_mse: 16.0822\n",
            "Epoch 18/1000\n",
            "250/250 [==============================] - 0s 112us/sample - loss: 13.1130 - mae: 2.8106 - mse: 13.1130 - val_loss: 14.2345 - val_mae: 2.7034 - val_mse: 14.2345\n",
            "Epoch 19/1000\n",
            "250/250 [==============================] - 0s 96us/sample - loss: 12.1416 - mae: 2.6511 - mse: 12.1416 - val_loss: 12.9240 - val_mae: 2.5938 - val_mse: 12.9240\n",
            "Epoch 20/1000\n",
            "250/250 [==============================] - 0s 90us/sample - loss: 11.2553 - mae: 2.5395 - mse: 11.2553 - val_loss: 11.8705 - val_mae: 2.4679 - val_mse: 11.8705\n",
            "Epoch 21/1000\n",
            "250/250 [==============================] - 0s 115us/sample - loss: 10.6523 - mae: 2.4514 - mse: 10.6523 - val_loss: 10.6510 - val_mae: 2.3059 - val_mse: 10.6510\n",
            "Epoch 22/1000\n",
            "250/250 [==============================] - 0s 112us/sample - loss: 9.8329 - mae: 2.3524 - mse: 9.8329 - val_loss: 9.9166 - val_mae: 2.2267 - val_mse: 9.9166\n",
            "Epoch 23/1000\n",
            "250/250 [==============================] - 0s 102us/sample - loss: 9.3845 - mae: 2.2947 - mse: 9.3845 - val_loss: 9.2744 - val_mae: 2.1601 - val_mse: 9.2744\n",
            "Epoch 24/1000\n",
            "250/250 [==============================] - 0s 96us/sample - loss: 9.1111 - mae: 2.2423 - mse: 9.1111 - val_loss: 8.7522 - val_mae: 2.1039 - val_mse: 8.7522\n",
            "Epoch 25/1000\n",
            "250/250 [==============================] - 0s 94us/sample - loss: 8.8966 - mae: 2.1875 - mse: 8.8966 - val_loss: 8.3672 - val_mae: 2.0464 - val_mse: 8.3672\n",
            "Epoch 26/1000\n",
            "250/250 [==============================] - 0s 95us/sample - loss: 8.6129 - mae: 2.1528 - mse: 8.6129 - val_loss: 7.8507 - val_mae: 1.9633 - val_mse: 7.8507\n",
            "Epoch 27/1000\n",
            "250/250 [==============================] - 0s 98us/sample - loss: 8.6827 - mae: 2.1615 - mse: 8.6827 - val_loss: 7.6263 - val_mae: 1.9362 - val_mse: 7.6263\n",
            "Epoch 28/1000\n",
            "250/250 [==============================] - 0s 102us/sample - loss: 8.0363 - mae: 2.0583 - mse: 8.0363 - val_loss: 8.4840 - val_mae: 2.0902 - val_mse: 8.4840\n",
            "Epoch 29/1000\n",
            "250/250 [==============================] - 0s 90us/sample - loss: 8.4319 - mae: 2.0892 - mse: 8.4319 - val_loss: 7.2757 - val_mae: 1.8962 - val_mse: 7.2757\n",
            "Epoch 30/1000\n",
            "250/250 [==============================] - 0s 82us/sample - loss: 8.0204 - mae: 2.0644 - mse: 8.0204 - val_loss: 7.7174 - val_mae: 1.9597 - val_mse: 7.7174\n",
            "Epoch 31/1000\n",
            "250/250 [==============================] - 0s 94us/sample - loss: 7.9344 - mae: 2.0551 - mse: 7.9344 - val_loss: 7.9137 - val_mae: 1.9920 - val_mse: 7.9137\n",
            "Epoch 32/1000\n",
            "250/250 [==============================] - 0s 84us/sample - loss: 7.8727 - mae: 2.0254 - mse: 7.8727 - val_loss: 7.4298 - val_mae: 1.8999 - val_mse: 7.4298\n",
            "Epoch 33/1000\n",
            "250/250 [==============================] - 0s 83us/sample - loss: 7.8039 - mae: 2.0026 - mse: 7.8039 - val_loss: 8.1096 - val_mae: 2.0349 - val_mse: 8.1096\n",
            "Epoch 34/1000\n",
            "250/250 [==============================] - 0s 97us/sample - loss: 7.7241 - mae: 2.0217 - mse: 7.7241 - val_loss: 7.1777 - val_mae: 1.8848 - val_mse: 7.1777\n",
            "Epoch 35/1000\n",
            "250/250 [==============================] - 0s 107us/sample - loss: 7.6359 - mae: 1.9743 - mse: 7.6359 - val_loss: 7.6821 - val_mae: 1.9482 - val_mse: 7.6821\n",
            "Epoch 36/1000\n",
            "250/250 [==============================] - 0s 94us/sample - loss: 7.4412 - mae: 1.9775 - mse: 7.4412 - val_loss: 7.6810 - val_mae: 1.9846 - val_mse: 7.6810\n",
            "Epoch 37/1000\n",
            "250/250 [==============================] - 0s 98us/sample - loss: 7.5848 - mae: 1.9893 - mse: 7.5848 - val_loss: 6.9212 - val_mae: 1.8469 - val_mse: 6.9212\n",
            "Epoch 38/1000\n",
            "250/250 [==============================] - 0s 92us/sample - loss: 7.7998 - mae: 2.0073 - mse: 7.7998 - val_loss: 6.9909 - val_mae: 1.8598 - val_mse: 6.9909\n",
            "Epoch 39/1000\n",
            "250/250 [==============================] - 0s 88us/sample - loss: 7.4434 - mae: 1.9523 - mse: 7.4434 - val_loss: 7.0962 - val_mae: 1.8786 - val_mse: 7.0962\n",
            "Epoch 40/1000\n",
            "250/250 [==============================] - 0s 89us/sample - loss: 7.3110 - mae: 1.9383 - mse: 7.3110 - val_loss: 6.8451 - val_mae: 1.8428 - val_mse: 6.8451\n",
            "Epoch 41/1000\n",
            "250/250 [==============================] - 0s 107us/sample - loss: 7.4146 - mae: 1.9888 - mse: 7.4146 - val_loss: 6.9632 - val_mae: 1.8539 - val_mse: 6.9632\n",
            "Epoch 42/1000\n",
            "250/250 [==============================] - 0s 130us/sample - loss: 7.3322 - mae: 1.9174 - mse: 7.3322 - val_loss: 6.7200 - val_mae: 1.8101 - val_mse: 6.7200\n",
            "Epoch 43/1000\n",
            "250/250 [==============================] - 0s 118us/sample - loss: 7.1726 - mae: 1.9297 - mse: 7.1726 - val_loss: 7.0992 - val_mae: 1.9052 - val_mse: 7.0992\n",
            "Epoch 44/1000\n",
            "250/250 [==============================] - 0s 94us/sample - loss: 7.1560 - mae: 1.9279 - mse: 7.1560 - val_loss: 8.2224 - val_mae: 2.0814 - val_mse: 8.2224\n",
            "Epoch 45/1000\n",
            "250/250 [==============================] - 0s 89us/sample - loss: 7.2639 - mae: 1.8960 - mse: 7.2639 - val_loss: 6.8338 - val_mae: 1.8452 - val_mse: 6.8338\n",
            "Epoch 46/1000\n",
            "250/250 [==============================] - 0s 117us/sample - loss: 7.0882 - mae: 1.9083 - mse: 7.0882 - val_loss: 7.8748 - val_mae: 2.0232 - val_mse: 7.8748\n",
            "Epoch 47/1000\n",
            "250/250 [==============================] - 0s 89us/sample - loss: 7.0530 - mae: 1.9212 - mse: 7.0530 - val_loss: 6.7547 - val_mae: 1.8616 - val_mse: 6.7547\n",
            "Epoch 48/1000\n",
            "250/250 [==============================] - 0s 87us/sample - loss: 6.9337 - mae: 1.8968 - mse: 6.9337 - val_loss: 7.0435 - val_mae: 1.8785 - val_mse: 7.0435\n",
            "Epoch 49/1000\n",
            "250/250 [==============================] - 0s 84us/sample - loss: 7.1345 - mae: 1.9043 - mse: 7.1345 - val_loss: 6.5813 - val_mae: 1.7874 - val_mse: 6.5813\n",
            "Epoch 50/1000\n",
            "250/250 [==============================] - 0s 129us/sample - loss: 7.3959 - mae: 1.9388 - mse: 7.3959 - val_loss: 7.1243 - val_mae: 1.8971 - val_mse: 7.1243\n",
            "Epoch 51/1000\n",
            "250/250 [==============================] - 0s 106us/sample - loss: 6.9580 - mae: 1.8798 - mse: 6.9580 - val_loss: 6.6861 - val_mae: 1.8446 - val_mse: 6.6861\n",
            "Epoch 52/1000\n",
            "250/250 [==============================] - 0s 129us/sample - loss: 6.8286 - mae: 1.8824 - mse: 6.8286 - val_loss: 6.4208 - val_mae: 1.7976 - val_mse: 6.4208\n",
            "Epoch 53/1000\n",
            "250/250 [==============================] - 0s 101us/sample - loss: 6.9739 - mae: 1.8793 - mse: 6.9739 - val_loss: 6.4596 - val_mae: 1.7868 - val_mse: 6.4596\n",
            "Epoch 54/1000\n",
            "250/250 [==============================] - 0s 105us/sample - loss: 6.8941 - mae: 1.9062 - mse: 6.8941 - val_loss: 6.6389 - val_mae: 1.8387 - val_mse: 6.6389\n",
            "Epoch 55/1000\n",
            "250/250 [==============================] - 0s 105us/sample - loss: 6.7017 - mae: 1.8451 - mse: 6.7017 - val_loss: 7.6072 - val_mae: 1.9902 - val_mse: 7.6072\n",
            "Epoch 56/1000\n",
            "250/250 [==============================] - 0s 110us/sample - loss: 6.8162 - mae: 1.8781 - mse: 6.8162 - val_loss: 7.2926 - val_mae: 1.9317 - val_mse: 7.2926\n",
            "Epoch 57/1000\n",
            "250/250 [==============================] - 0s 82us/sample - loss: 6.9927 - mae: 1.8857 - mse: 6.9927 - val_loss: 6.8203 - val_mae: 1.8678 - val_mse: 6.8203\n",
            "Epoch 58/1000\n",
            "250/250 [==============================] - 0s 92us/sample - loss: 6.9432 - mae: 1.8599 - mse: 6.9432 - val_loss: 6.8046 - val_mae: 1.8624 - val_mse: 6.8046\n",
            "Epoch 59/1000\n",
            "250/250 [==============================] - 0s 83us/sample - loss: 6.7219 - mae: 1.8516 - mse: 6.7219 - val_loss: 6.2838 - val_mae: 1.7637 - val_mse: 6.2838\n",
            "Epoch 60/1000\n",
            "250/250 [==============================] - 0s 94us/sample - loss: 6.9835 - mae: 1.9017 - mse: 6.9835 - val_loss: 7.0634 - val_mae: 1.9149 - val_mse: 7.0634\n",
            "Epoch 61/1000\n",
            "250/250 [==============================] - 0s 98us/sample - loss: 6.8292 - mae: 1.8526 - mse: 6.8292 - val_loss: 6.8597 - val_mae: 1.8692 - val_mse: 6.8597\n",
            "Epoch 62/1000\n",
            "250/250 [==============================] - 0s 102us/sample - loss: 6.9076 - mae: 1.8746 - mse: 6.9076 - val_loss: 6.8825 - val_mae: 1.8811 - val_mse: 6.8825\n",
            "Epoch 63/1000\n",
            "250/250 [==============================] - 0s 90us/sample - loss: 6.8348 - mae: 1.8467 - mse: 6.8348 - val_loss: 6.5439 - val_mae: 1.8383 - val_mse: 6.5439\n",
            "Epoch 64/1000\n",
            "250/250 [==============================] - 0s 129us/sample - loss: 6.7303 - mae: 1.8277 - mse: 6.7303 - val_loss: 6.3995 - val_mae: 1.8121 - val_mse: 6.3995\n",
            "Epoch 65/1000\n",
            "250/250 [==============================] - 0s 111us/sample - loss: 6.6960 - mae: 1.8234 - mse: 6.6960 - val_loss: 6.5659 - val_mae: 1.8290 - val_mse: 6.5659\n",
            "Epoch 66/1000\n",
            "250/250 [==============================] - 0s 89us/sample - loss: 6.7831 - mae: 1.8758 - mse: 6.7831 - val_loss: 6.6865 - val_mae: 1.8398 - val_mse: 6.6865\n",
            "Epoch 67/1000\n",
            "250/250 [==============================] - 0s 105us/sample - loss: 6.6028 - mae: 1.8351 - mse: 6.6028 - val_loss: 7.1646 - val_mae: 1.9413 - val_mse: 7.1646\n",
            "Epoch 68/1000\n",
            "250/250 [==============================] - 0s 131us/sample - loss: 6.5223 - mae: 1.8074 - mse: 6.5223 - val_loss: 6.2037 - val_mae: 1.7475 - val_mse: 6.2037\n",
            "Epoch 69/1000\n",
            "250/250 [==============================] - 0s 124us/sample - loss: 6.6181 - mae: 1.8469 - mse: 6.6181 - val_loss: 6.3460 - val_mae: 1.8010 - val_mse: 6.3460\n",
            "Epoch 70/1000\n",
            "250/250 [==============================] - 0s 97us/sample - loss: 6.7088 - mae: 1.8296 - mse: 6.7088 - val_loss: 6.9959 - val_mae: 1.8875 - val_mse: 6.9959\n",
            "Epoch 71/1000\n",
            "250/250 [==============================] - 0s 102us/sample - loss: 6.4764 - mae: 1.8037 - mse: 6.4764 - val_loss: 6.5259 - val_mae: 1.8509 - val_mse: 6.5259\n",
            "Epoch 72/1000\n",
            "250/250 [==============================] - 0s 107us/sample - loss: 6.6892 - mae: 1.8398 - mse: 6.6892 - val_loss: 6.3542 - val_mae: 1.7993 - val_mse: 6.3542\n",
            "Epoch 73/1000\n",
            "250/250 [==============================] - 0s 108us/sample - loss: 6.8234 - mae: 1.8511 - mse: 6.8234 - val_loss: 6.5076 - val_mae: 1.8346 - val_mse: 6.5076\n",
            "Epoch 74/1000\n",
            "250/250 [==============================] - 0s 104us/sample - loss: 6.6160 - mae: 1.7988 - mse: 6.6160 - val_loss: 6.6528 - val_mae: 1.8443 - val_mse: 6.6528\n",
            "Epoch 75/1000\n",
            "250/250 [==============================] - 0s 86us/sample - loss: 6.4462 - mae: 1.7704 - mse: 6.4462 - val_loss: 7.4767 - val_mae: 1.9951 - val_mse: 7.4767\n",
            "Epoch 76/1000\n",
            "250/250 [==============================] - 0s 94us/sample - loss: 6.8789 - mae: 1.8613 - mse: 6.8789 - val_loss: 6.4690 - val_mae: 1.8235 - val_mse: 6.4690\n",
            "Epoch 77/1000\n",
            "250/250 [==============================] - 0s 89us/sample - loss: 6.3839 - mae: 1.7921 - mse: 6.3839 - val_loss: 6.1858 - val_mae: 1.7665 - val_mse: 6.1858\n",
            "Epoch 78/1000\n",
            "250/250 [==============================] - 0s 98us/sample - loss: 6.4691 - mae: 1.7908 - mse: 6.4691 - val_loss: 7.1111 - val_mae: 1.9154 - val_mse: 7.1111\n",
            "Epoch 79/1000\n",
            "250/250 [==============================] - 0s 123us/sample - loss: 6.5654 - mae: 1.8045 - mse: 6.5654 - val_loss: 6.3272 - val_mae: 1.8086 - val_mse: 6.3272\n",
            "Epoch 80/1000\n",
            "250/250 [==============================] - 0s 84us/sample - loss: 6.4543 - mae: 1.8140 - mse: 6.4543 - val_loss: 6.5950 - val_mae: 1.8496 - val_mse: 6.5950\n",
            "Epoch 81/1000\n",
            "250/250 [==============================] - 0s 110us/sample - loss: 6.3534 - mae: 1.7764 - mse: 6.3534 - val_loss: 6.4719 - val_mae: 1.8228 - val_mse: 6.4719\n",
            "Epoch 82/1000\n",
            "250/250 [==============================] - 0s 128us/sample - loss: 6.4595 - mae: 1.7897 - mse: 6.4595 - val_loss: 6.2884 - val_mae: 1.7863 - val_mse: 6.2884\n",
            "Epoch 83/1000\n",
            "250/250 [==============================] - 0s 83us/sample - loss: 6.6054 - mae: 1.7990 - mse: 6.6054 - val_loss: 6.2239 - val_mae: 1.7813 - val_mse: 6.2239\n",
            "Epoch 84/1000\n",
            "250/250 [==============================] - 0s 85us/sample - loss: 6.4659 - mae: 1.8247 - mse: 6.4659 - val_loss: 6.9963 - val_mae: 1.8968 - val_mse: 6.9963\n",
            "Epoch 85/1000\n",
            "250/250 [==============================] - 0s 82us/sample - loss: 6.3671 - mae: 1.7741 - mse: 6.3671 - val_loss: 6.9281 - val_mae: 1.8864 - val_mse: 6.9281\n",
            "Epoch 86/1000\n",
            "250/250 [==============================] - 0s 96us/sample - loss: 6.5456 - mae: 1.8314 - mse: 6.5456 - val_loss: 7.7040 - val_mae: 2.0204 - val_mse: 7.7040\n",
            "Epoch 87/1000\n",
            "250/250 [==============================] - 0s 98us/sample - loss: 6.4444 - mae: 1.7915 - mse: 6.4444 - val_loss: 6.3383 - val_mae: 1.8005 - val_mse: 6.3383\n",
            "Epoch 88/1000\n",
            "250/250 [==============================] - 0s 98us/sample - loss: 6.4687 - mae: 1.8086 - mse: 6.4687 - val_loss: 6.4093 - val_mae: 1.8203 - val_mse: 6.4093\n",
            "Epoch 89/1000\n",
            "250/250 [==============================] - 0s 100us/sample - loss: 6.3567 - mae: 1.7712 - mse: 6.3567 - val_loss: 6.4611 - val_mae: 1.8243 - val_mse: 6.4611\n",
            "Epoch 90/1000\n",
            "250/250 [==============================] - 0s 105us/sample - loss: 6.3951 - mae: 1.7813 - mse: 6.3951 - val_loss: 6.2604 - val_mae: 1.7825 - val_mse: 6.2604\n",
            "Epoch 91/1000\n",
            "250/250 [==============================] - 0s 92us/sample - loss: 6.4718 - mae: 1.7886 - mse: 6.4718 - val_loss: 6.8955 - val_mae: 1.8796 - val_mse: 6.8955\n",
            "Epoch 92/1000\n",
            "250/250 [==============================] - 0s 90us/sample - loss: 6.3072 - mae: 1.7711 - mse: 6.3072 - val_loss: 6.8163 - val_mae: 1.8696 - val_mse: 6.8163\n",
            "Epoch 93/1000\n",
            "250/250 [==============================] - 0s 97us/sample - loss: 6.1973 - mae: 1.7734 - mse: 6.1973 - val_loss: 6.5809 - val_mae: 1.8261 - val_mse: 6.5809\n",
            "Epoch 94/1000\n",
            "250/250 [==============================] - 0s 98us/sample - loss: 6.5669 - mae: 1.8303 - mse: 6.5669 - val_loss: 6.6532 - val_mae: 1.8414 - val_mse: 6.6532\n",
            "Epoch 95/1000\n",
            "250/250 [==============================] - 0s 130us/sample - loss: 6.3416 - mae: 1.7769 - mse: 6.3416 - val_loss: 7.0771 - val_mae: 1.9128 - val_mse: 7.0771\n",
            "Epoch 96/1000\n",
            "250/250 [==============================] - 0s 94us/sample - loss: 6.2572 - mae: 1.7766 - mse: 6.2572 - val_loss: 6.4594 - val_mae: 1.8127 - val_mse: 6.4594\n",
            "Epoch 97/1000\n",
            "250/250 [==============================] - 0s 113us/sample - loss: 6.3194 - mae: 1.7626 - mse: 6.3194 - val_loss: 6.9743 - val_mae: 1.8969 - val_mse: 6.9743\n",
            "Epoch 98/1000\n",
            "250/250 [==============================] - 0s 93us/sample - loss: 6.4778 - mae: 1.7808 - mse: 6.4778 - val_loss: 6.1210 - val_mae: 1.7707 - val_mse: 6.1210\n",
            "Epoch 99/1000\n",
            "250/250 [==============================] - 0s 88us/sample - loss: 6.1425 - mae: 1.7628 - mse: 6.1425 - val_loss: 7.6969 - val_mae: 2.0327 - val_mse: 7.6969\n",
            "Epoch 100/1000\n",
            "250/250 [==============================] - 0s 119us/sample - loss: 6.3926 - mae: 1.7989 - mse: 6.3926 - val_loss: 6.1062 - val_mae: 1.7608 - val_mse: 6.1062\n",
            "Epoch 101/1000\n",
            "250/250 [==============================] - 0s 104us/sample - loss: 6.3978 - mae: 1.7860 - mse: 6.3978 - val_loss: 6.8171 - val_mae: 1.8673 - val_mse: 6.8171\n",
            "Epoch 102/1000\n",
            "250/250 [==============================] - 0s 86us/sample - loss: 6.1222 - mae: 1.7513 - mse: 6.1222 - val_loss: 6.2757 - val_mae: 1.7864 - val_mse: 6.2757\n",
            "Epoch 103/1000\n",
            "250/250 [==============================] - 0s 94us/sample - loss: 6.1352 - mae: 1.7398 - mse: 6.1353 - val_loss: 5.8234 - val_mae: 1.7178 - val_mse: 5.8234\n",
            "Epoch 104/1000\n",
            "250/250 [==============================] - 0s 83us/sample - loss: 6.3144 - mae: 1.7467 - mse: 6.3144 - val_loss: 6.0446 - val_mae: 1.7684 - val_mse: 6.0446\n",
            "Epoch 105/1000\n",
            "250/250 [==============================] - 0s 108us/sample - loss: 6.0444 - mae: 1.7297 - mse: 6.0444 - val_loss: 6.5242 - val_mae: 1.8557 - val_mse: 6.5242\n",
            "Epoch 106/1000\n",
            "250/250 [==============================] - 0s 105us/sample - loss: 6.2579 - mae: 1.7694 - mse: 6.2579 - val_loss: 6.0887 - val_mae: 1.7688 - val_mse: 6.0887\n",
            "Epoch 107/1000\n",
            "250/250 [==============================] - 0s 113us/sample - loss: 6.0936 - mae: 1.7360 - mse: 6.0936 - val_loss: 6.2865 - val_mae: 1.8010 - val_mse: 6.2865\n",
            "Epoch 108/1000\n",
            "250/250 [==============================] - 0s 109us/sample - loss: 6.3892 - mae: 1.7755 - mse: 6.3892 - val_loss: 6.2876 - val_mae: 1.7938 - val_mse: 6.2876\n",
            "Epoch 109/1000\n",
            "250/250 [==============================] - 0s 92us/sample - loss: 6.1182 - mae: 1.7494 - mse: 6.1182 - val_loss: 5.8327 - val_mae: 1.7182 - val_mse: 5.8327\n",
            "Epoch 110/1000\n",
            "250/250 [==============================] - 0s 88us/sample - loss: 6.3634 - mae: 1.7775 - mse: 6.3634 - val_loss: 5.9294 - val_mae: 1.7380 - val_mse: 5.9294\n",
            "Epoch 111/1000\n",
            "250/250 [==============================] - 0s 85us/sample - loss: 6.1023 - mae: 1.7348 - mse: 6.1023 - val_loss: 6.7064 - val_mae: 1.8525 - val_mse: 6.7064\n",
            "Epoch 112/1000\n",
            "250/250 [==============================] - 0s 97us/sample - loss: 6.0724 - mae: 1.7123 - mse: 6.0724 - val_loss: 6.0961 - val_mae: 1.7674 - val_mse: 6.0961\n",
            "Epoch 113/1000\n",
            "250/250 [==============================] - 0s 87us/sample - loss: 5.9947 - mae: 1.7233 - mse: 5.9947 - val_loss: 5.6929 - val_mae: 1.7081 - val_mse: 5.6929\n",
            "Epoch 114/1000\n",
            "250/250 [==============================] - 0s 84us/sample - loss: 6.2445 - mae: 1.7479 - mse: 6.2445 - val_loss: 6.1945 - val_mae: 1.7846 - val_mse: 6.1945\n",
            "Epoch 115/1000\n",
            "250/250 [==============================] - 0s 91us/sample - loss: 6.1834 - mae: 1.7613 - mse: 6.1834 - val_loss: 6.1707 - val_mae: 1.7809 - val_mse: 6.1707\n",
            "Epoch 116/1000\n",
            "250/250 [==============================] - 0s 80us/sample - loss: 6.1586 - mae: 1.7447 - mse: 6.1586 - val_loss: 7.4040 - val_mae: 1.9806 - val_mse: 7.4040\n",
            "Epoch 117/1000\n",
            "250/250 [==============================] - 0s 103us/sample - loss: 6.2593 - mae: 1.7844 - mse: 6.2593 - val_loss: 5.9998 - val_mae: 1.7482 - val_mse: 5.9998\n",
            "Epoch 118/1000\n",
            "250/250 [==============================] - 0s 90us/sample - loss: 5.9909 - mae: 1.7300 - mse: 5.9909 - val_loss: 5.8381 - val_mae: 1.7086 - val_mse: 5.8381\n",
            "Epoch 119/1000\n",
            "250/250 [==============================] - 0s 105us/sample - loss: 6.0626 - mae: 1.7473 - mse: 6.0626 - val_loss: 7.0645 - val_mae: 1.9049 - val_mse: 7.0645\n",
            "Epoch 120/1000\n",
            "250/250 [==============================] - 0s 86us/sample - loss: 6.2388 - mae: 1.7339 - mse: 6.2388 - val_loss: 5.7004 - val_mae: 1.6917 - val_mse: 5.7004\n",
            "Epoch 121/1000\n",
            "250/250 [==============================] - 0s 100us/sample - loss: 5.9794 - mae: 1.7410 - mse: 5.9794 - val_loss: 6.7441 - val_mae: 1.8521 - val_mse: 6.7441\n",
            "Epoch 122/1000\n",
            "250/250 [==============================] - 0s 113us/sample - loss: 6.0248 - mae: 1.7327 - mse: 6.0248 - val_loss: 6.3772 - val_mae: 1.7985 - val_mse: 6.3772\n",
            "Epoch 123/1000\n",
            "250/250 [==============================] - 0s 104us/sample - loss: 5.9924 - mae: 1.7130 - mse: 5.9924 - val_loss: 6.6059 - val_mae: 1.8623 - val_mse: 6.6059\n",
            "Epoch 124/1000\n",
            "250/250 [==============================] - 0s 98us/sample - loss: 6.2234 - mae: 1.7408 - mse: 6.2234 - val_loss: 6.0526 - val_mae: 1.7531 - val_mse: 6.0526\n",
            "Epoch 125/1000\n",
            "250/250 [==============================] - 0s 95us/sample - loss: 5.9651 - mae: 1.7190 - mse: 5.9651 - val_loss: 6.9226 - val_mae: 1.8840 - val_mse: 6.9226\n",
            "Epoch 126/1000\n",
            "250/250 [==============================] - 0s 102us/sample - loss: 6.0977 - mae: 1.7370 - mse: 6.0977 - val_loss: 6.6800 - val_mae: 1.8463 - val_mse: 6.6800\n",
            "Epoch 127/1000\n",
            "250/250 [==============================] - 0s 89us/sample - loss: 6.2408 - mae: 1.7257 - mse: 6.2408 - val_loss: 5.9361 - val_mae: 1.7419 - val_mse: 5.9361\n",
            "Epoch 128/1000\n",
            "250/250 [==============================] - 0s 107us/sample - loss: 6.0018 - mae: 1.7182 - mse: 6.0018 - val_loss: 5.9856 - val_mae: 1.7477 - val_mse: 5.9856\n",
            "Epoch 129/1000\n",
            "250/250 [==============================] - 0s 119us/sample - loss: 6.0828 - mae: 1.7178 - mse: 6.0828 - val_loss: 5.7300 - val_mae: 1.7097 - val_mse: 5.7300\n",
            "Epoch 130/1000\n",
            "250/250 [==============================] - 0s 97us/sample - loss: 6.1011 - mae: 1.7235 - mse: 6.1011 - val_loss: 6.0169 - val_mae: 1.7487 - val_mse: 6.0169\n",
            "Epoch 131/1000\n",
            "250/250 [==============================] - 0s 102us/sample - loss: 5.9373 - mae: 1.7087 - mse: 5.9373 - val_loss: 5.7925 - val_mae: 1.7207 - val_mse: 5.7925\n",
            "Epoch 132/1000\n",
            "250/250 [==============================] - 0s 87us/sample - loss: 5.8615 - mae: 1.7110 - mse: 5.8615 - val_loss: 5.6721 - val_mae: 1.7207 - val_mse: 5.6721\n",
            "Epoch 133/1000\n",
            "250/250 [==============================] - 0s 95us/sample - loss: 5.7979 - mae: 1.7123 - mse: 5.7979 - val_loss: 8.1635 - val_mae: 2.1054 - val_mse: 8.1635\n",
            "Epoch 134/1000\n",
            "250/250 [==============================] - 0s 84us/sample - loss: 5.8857 - mae: 1.7143 - mse: 5.8857 - val_loss: 5.6354 - val_mae: 1.7005 - val_mse: 5.6354\n",
            "Epoch 135/1000\n",
            "250/250 [==============================] - 0s 86us/sample - loss: 6.0564 - mae: 1.7130 - mse: 6.0564 - val_loss: 5.9122 - val_mae: 1.7524 - val_mse: 5.9122\n",
            "Epoch 136/1000\n",
            "250/250 [==============================] - 0s 104us/sample - loss: 6.0277 - mae: 1.7276 - mse: 6.0277 - val_loss: 6.1458 - val_mae: 1.7703 - val_mse: 6.1458\n",
            "Epoch 137/1000\n",
            "250/250 [==============================] - 0s 97us/sample - loss: 5.8381 - mae: 1.6984 - mse: 5.8381 - val_loss: 5.5447 - val_mae: 1.6800 - val_mse: 5.5447\n",
            "Epoch 138/1000\n",
            "250/250 [==============================] - 0s 89us/sample - loss: 6.0935 - mae: 1.7424 - mse: 6.0935 - val_loss: 7.2570 - val_mae: 1.9482 - val_mse: 7.2570\n",
            "Epoch 139/1000\n",
            "250/250 [==============================] - 0s 117us/sample - loss: 5.6978 - mae: 1.6601 - mse: 5.6978 - val_loss: 5.6529 - val_mae: 1.7229 - val_mse: 5.6529\n",
            "Epoch 140/1000\n",
            "250/250 [==============================] - 0s 93us/sample - loss: 5.9221 - mae: 1.7229 - mse: 5.9221 - val_loss: 6.1055 - val_mae: 1.7666 - val_mse: 6.1055\n",
            "Epoch 141/1000\n",
            "250/250 [==============================] - 0s 85us/sample - loss: 5.9458 - mae: 1.7116 - mse: 5.9458 - val_loss: 6.1059 - val_mae: 1.7633 - val_mse: 6.1059\n",
            "Epoch 142/1000\n",
            "250/250 [==============================] - 0s 89us/sample - loss: 5.9981 - mae: 1.7162 - mse: 5.9981 - val_loss: 6.2693 - val_mae: 1.7785 - val_mse: 6.2693\n",
            "Epoch 143/1000\n",
            "250/250 [==============================] - 0s 103us/sample - loss: 5.9592 - mae: 1.7164 - mse: 5.9592 - val_loss: 5.7465 - val_mae: 1.7269 - val_mse: 5.7465\n",
            "Epoch 144/1000\n",
            "250/250 [==============================] - 0s 102us/sample - loss: 5.9597 - mae: 1.7178 - mse: 5.9597 - val_loss: 5.8839 - val_mae: 1.7542 - val_mse: 5.8839\n",
            "Epoch 145/1000\n",
            "250/250 [==============================] - 0s 95us/sample - loss: 5.9314 - mae: 1.7407 - mse: 5.9314 - val_loss: 5.5114 - val_mae: 1.6812 - val_mse: 5.5114\n",
            "Epoch 146/1000\n",
            "250/250 [==============================] - 0s 114us/sample - loss: 5.8239 - mae: 1.6796 - mse: 5.8239 - val_loss: 5.9892 - val_mae: 1.7444 - val_mse: 5.9892\n",
            "Epoch 147/1000\n",
            "250/250 [==============================] - 0s 102us/sample - loss: 5.9078 - mae: 1.7072 - mse: 5.9078 - val_loss: 5.6604 - val_mae: 1.7023 - val_mse: 5.6604\n",
            "Epoch 148/1000\n",
            "250/250 [==============================] - 0s 93us/sample - loss: 5.8683 - mae: 1.6959 - mse: 5.8683 - val_loss: 5.5195 - val_mae: 1.6831 - val_mse: 5.5195\n",
            "Epoch 149/1000\n",
            "250/250 [==============================] - 0s 100us/sample - loss: 5.8648 - mae: 1.7163 - mse: 5.8648 - val_loss: 5.9967 - val_mae: 1.7518 - val_mse: 5.9967\n",
            "Epoch 150/1000\n",
            "250/250 [==============================] - 0s 94us/sample - loss: 5.8705 - mae: 1.7006 - mse: 5.8705 - val_loss: 6.2251 - val_mae: 1.7777 - val_mse: 6.2251\n",
            "Epoch 151/1000\n",
            "250/250 [==============================] - 0s 88us/sample - loss: 5.8535 - mae: 1.6836 - mse: 5.8535 - val_loss: 5.5031 - val_mae: 1.6780 - val_mse: 5.5031\n",
            "Epoch 152/1000\n",
            "250/250 [==============================] - 0s 96us/sample - loss: 5.7231 - mae: 1.7054 - mse: 5.7231 - val_loss: 5.9927 - val_mae: 1.7444 - val_mse: 5.9927\n",
            "Epoch 153/1000\n",
            "250/250 [==============================] - 0s 101us/sample - loss: 5.6681 - mae: 1.6630 - mse: 5.6681 - val_loss: 6.4518 - val_mae: 1.8572 - val_mse: 6.4518\n",
            "Epoch 154/1000\n",
            "250/250 [==============================] - 0s 100us/sample - loss: 5.6952 - mae: 1.6849 - mse: 5.6952 - val_loss: 5.7048 - val_mae: 1.7154 - val_mse: 5.7048\n",
            "Epoch 155/1000\n",
            "250/250 [==============================] - 0s 90us/sample - loss: 6.0645 - mae: 1.7501 - mse: 6.0645 - val_loss: 6.3909 - val_mae: 1.8103 - val_mse: 6.3909\n",
            "Epoch 156/1000\n",
            "250/250 [==============================] - 0s 138us/sample - loss: 5.7467 - mae: 1.6769 - mse: 5.7467 - val_loss: 5.9500 - val_mae: 1.7673 - val_mse: 5.9500\n",
            "Epoch 157/1000\n",
            "250/250 [==============================] - 0s 91us/sample - loss: 5.9125 - mae: 1.6859 - mse: 5.9125 - val_loss: 5.7944 - val_mae: 1.7282 - val_mse: 5.7944\n",
            "Epoch 158/1000\n",
            "250/250 [==============================] - 0s 98us/sample - loss: 5.8337 - mae: 1.6769 - mse: 5.8337 - val_loss: 5.9509 - val_mae: 1.7445 - val_mse: 5.9509\n",
            "Epoch 159/1000\n",
            "250/250 [==============================] - 0s 104us/sample - loss: 5.6832 - mae: 1.6793 - mse: 5.6832 - val_loss: 5.8085 - val_mae: 1.7377 - val_mse: 5.8085\n",
            "Epoch 160/1000\n",
            "250/250 [==============================] - 0s 99us/sample - loss: 5.8899 - mae: 1.7056 - mse: 5.8899 - val_loss: 6.7070 - val_mae: 1.8621 - val_mse: 6.7070\n",
            "Epoch 161/1000\n",
            "250/250 [==============================] - 0s 95us/sample - loss: 5.6450 - mae: 1.6660 - mse: 5.6450 - val_loss: 5.5770 - val_mae: 1.7011 - val_mse: 5.5770\n",
            "Epoch 162/1000\n",
            "250/250 [==============================] - 0s 97us/sample - loss: 5.7606 - mae: 1.6571 - mse: 5.7606 - val_loss: 5.5851 - val_mae: 1.7027 - val_mse: 5.5851\n",
            "Epoch 163/1000\n",
            "250/250 [==============================] - 0s 108us/sample - loss: 5.8611 - mae: 1.6857 - mse: 5.8611 - val_loss: 5.6052 - val_mae: 1.7064 - val_mse: 5.6052\n",
            "Epoch 164/1000\n",
            "250/250 [==============================] - 0s 85us/sample - loss: 5.6763 - mae: 1.6641 - mse: 5.6763 - val_loss: 5.5920 - val_mae: 1.6994 - val_mse: 5.5920\n",
            "Epoch 165/1000\n",
            "250/250 [==============================] - 0s 94us/sample - loss: 5.6137 - mae: 1.6724 - mse: 5.6137 - val_loss: 6.6120 - val_mae: 1.8352 - val_mse: 6.6120\n",
            "Epoch 166/1000\n",
            "250/250 [==============================] - 0s 107us/sample - loss: 5.7111 - mae: 1.6890 - mse: 5.7111 - val_loss: 6.2575 - val_mae: 1.7863 - val_mse: 6.2575\n",
            "Epoch 167/1000\n",
            "250/250 [==============================] - 0s 81us/sample - loss: 5.9097 - mae: 1.7228 - mse: 5.9097 - val_loss: 6.1168 - val_mae: 1.7861 - val_mse: 6.1168\n",
            "Epoch 168/1000\n",
            "250/250 [==============================] - 0s 91us/sample - loss: 5.9147 - mae: 1.7205 - mse: 5.9147 - val_loss: 6.0212 - val_mae: 1.7399 - val_mse: 6.0212\n",
            "Epoch 169/1000\n",
            "250/250 [==============================] - 0s 99us/sample - loss: 5.6196 - mae: 1.6624 - mse: 5.6196 - val_loss: 5.8344 - val_mae: 1.7437 - val_mse: 5.8344\n",
            "Epoch 170/1000\n",
            "250/250 [==============================] - 0s 95us/sample - loss: 5.6792 - mae: 1.6535 - mse: 5.6792 - val_loss: 6.1420 - val_mae: 1.7641 - val_mse: 6.1420\n",
            "Epoch 171/1000\n",
            "250/250 [==============================] - 0s 101us/sample - loss: 5.7514 - mae: 1.6770 - mse: 5.7514 - val_loss: 5.5971 - val_mae: 1.7118 - val_mse: 5.5971\n",
            "Epoch 172/1000\n",
            "250/250 [==============================] - 0s 104us/sample - loss: 5.6386 - mae: 1.6701 - mse: 5.6386 - val_loss: 6.1844 - val_mae: 1.7731 - val_mse: 6.1844\n",
            "Epoch 173/1000\n",
            "250/250 [==============================] - 0s 90us/sample - loss: 5.6934 - mae: 1.6703 - mse: 5.6934 - val_loss: 6.1473 - val_mae: 1.7669 - val_mse: 6.1473\n",
            "Epoch 174/1000\n",
            "250/250 [==============================] - 0s 91us/sample - loss: 5.4789 - mae: 1.5949 - mse: 5.4789 - val_loss: 5.7755 - val_mae: 1.7276 - val_mse: 5.7755\n",
            "Epoch 175/1000\n",
            "250/250 [==============================] - 0s 104us/sample - loss: 5.6962 - mae: 1.6651 - mse: 5.6962 - val_loss: 5.6659 - val_mae: 1.7160 - val_mse: 5.6659\n",
            "Epoch 176/1000\n",
            "250/250 [==============================] - 0s 91us/sample - loss: 5.5992 - mae: 1.6276 - mse: 5.5992 - val_loss: 5.6342 - val_mae: 1.7029 - val_mse: 5.6342\n",
            "Epoch 177/1000\n",
            "250/250 [==============================] - 0s 98us/sample - loss: 5.7050 - mae: 1.6837 - mse: 5.7050 - val_loss: 5.6213 - val_mae: 1.7009 - val_mse: 5.6213\n",
            "Epoch 178/1000\n",
            "250/250 [==============================] - 0s 113us/sample - loss: 5.7593 - mae: 1.6726 - mse: 5.7593 - val_loss: 5.4861 - val_mae: 1.6827 - val_mse: 5.4861\n",
            "Epoch 179/1000\n",
            "250/250 [==============================] - 0s 84us/sample - loss: 5.5878 - mae: 1.6264 - mse: 5.5878 - val_loss: 5.4646 - val_mae: 1.6773 - val_mse: 5.4646\n",
            "Epoch 180/1000\n",
            "250/250 [==============================] - 0s 90us/sample - loss: 5.5021 - mae: 1.6339 - mse: 5.5021 - val_loss: 5.4865 - val_mae: 1.6683 - val_mse: 5.4865\n",
            "Epoch 181/1000\n",
            "250/250 [==============================] - 0s 96us/sample - loss: 5.8229 - mae: 1.6779 - mse: 5.8229 - val_loss: 5.7255 - val_mae: 1.7059 - val_mse: 5.7255\n",
            "Epoch 182/1000\n",
            "250/250 [==============================] - 0s 80us/sample - loss: 5.5973 - mae: 1.6598 - mse: 5.5973 - val_loss: 5.9055 - val_mae: 1.7420 - val_mse: 5.9055\n",
            "Epoch 183/1000\n",
            "250/250 [==============================] - 0s 87us/sample - loss: 5.6733 - mae: 1.6610 - mse: 5.6733 - val_loss: 6.4284 - val_mae: 1.7901 - val_mse: 6.4284\n",
            "Epoch 184/1000\n",
            "250/250 [==============================] - 0s 99us/sample - loss: 5.6035 - mae: 1.6488 - mse: 5.6035 - val_loss: 5.3734 - val_mae: 1.6640 - val_mse: 5.3734\n",
            "Epoch 185/1000\n",
            "250/250 [==============================] - 0s 96us/sample - loss: 5.6029 - mae: 1.6408 - mse: 5.6029 - val_loss: 5.4639 - val_mae: 1.6715 - val_mse: 5.4639\n",
            "Epoch 186/1000\n",
            "250/250 [==============================] - 0s 91us/sample - loss: 5.4989 - mae: 1.6121 - mse: 5.4989 - val_loss: 5.3975 - val_mae: 1.6571 - val_mse: 5.3975\n",
            "Epoch 187/1000\n",
            "250/250 [==============================] - 0s 89us/sample - loss: 5.6439 - mae: 1.6224 - mse: 5.6439 - val_loss: 5.4316 - val_mae: 1.6601 - val_mse: 5.4316\n",
            "Epoch 188/1000\n",
            "250/250 [==============================] - 0s 107us/sample - loss: 5.4818 - mae: 1.6170 - mse: 5.4818 - val_loss: 5.9594 - val_mae: 1.7342 - val_mse: 5.9594\n",
            "Epoch 189/1000\n",
            "250/250 [==============================] - 0s 96us/sample - loss: 5.6723 - mae: 1.6789 - mse: 5.6723 - val_loss: 5.5006 - val_mae: 1.6821 - val_mse: 5.5006\n",
            "Epoch 190/1000\n",
            "250/250 [==============================] - 0s 98us/sample - loss: 5.5905 - mae: 1.6555 - mse: 5.5905 - val_loss: 5.4171 - val_mae: 1.6730 - val_mse: 5.4171\n",
            "Epoch 191/1000\n",
            "250/250 [==============================] - 0s 95us/sample - loss: 5.4461 - mae: 1.6106 - mse: 5.4461 - val_loss: 6.1543 - val_mae: 1.7627 - val_mse: 6.1543\n",
            "Epoch 192/1000\n",
            "250/250 [==============================] - 0s 126us/sample - loss: 5.4192 - mae: 1.6402 - mse: 5.4192 - val_loss: 6.7020 - val_mae: 1.8452 - val_mse: 6.7020\n",
            "Epoch 193/1000\n",
            "250/250 [==============================] - 0s 96us/sample - loss: 5.5186 - mae: 1.6244 - mse: 5.5186 - val_loss: 5.4802 - val_mae: 1.7007 - val_mse: 5.4802\n",
            "Epoch 194/1000\n",
            "250/250 [==============================] - 0s 100us/sample - loss: 5.5335 - mae: 1.6434 - mse: 5.5335 - val_loss: 5.5047 - val_mae: 1.6707 - val_mse: 5.5047\n",
            "Epoch 195/1000\n",
            "250/250 [==============================] - 0s 100us/sample - loss: 5.5345 - mae: 1.6280 - mse: 5.5345 - val_loss: 6.4366 - val_mae: 1.8311 - val_mse: 6.4366\n",
            "Epoch 196/1000\n",
            "250/250 [==============================] - 0s 131us/sample - loss: 5.4611 - mae: 1.6126 - mse: 5.4611 - val_loss: 6.0955 - val_mae: 1.7865 - val_mse: 6.0955\n",
            "Epoch 197/1000\n",
            "250/250 [==============================] - 0s 125us/sample - loss: 5.4984 - mae: 1.6191 - mse: 5.4984 - val_loss: 6.6335 - val_mae: 1.8336 - val_mse: 6.6335\n",
            "Epoch 198/1000\n",
            "250/250 [==============================] - 0s 107us/sample - loss: 5.4561 - mae: 1.6391 - mse: 5.4561 - val_loss: 6.2874 - val_mae: 1.7998 - val_mse: 6.2874\n",
            "Epoch 199/1000\n",
            "250/250 [==============================] - 0s 105us/sample - loss: 5.6529 - mae: 1.6562 - mse: 5.6529 - val_loss: 6.0687 - val_mae: 1.7376 - val_mse: 6.0687\n",
            "Epoch 200/1000\n",
            "250/250 [==============================] - 0s 121us/sample - loss: 5.4280 - mae: 1.6225 - mse: 5.4280 - val_loss: 5.3864 - val_mae: 1.6605 - val_mse: 5.3864\n",
            "Epoch 201/1000\n",
            "250/250 [==============================] - 0s 80us/sample - loss: 5.6093 - mae: 1.6260 - mse: 5.6093 - val_loss: 5.4000 - val_mae: 1.6645 - val_mse: 5.4000\n",
            "Epoch 202/1000\n",
            "250/250 [==============================] - 0s 105us/sample - loss: 5.6837 - mae: 1.6490 - mse: 5.6837 - val_loss: 6.1746 - val_mae: 1.7819 - val_mse: 6.1746\n",
            "Epoch 203/1000\n",
            "250/250 [==============================] - 0s 94us/sample - loss: 5.5481 - mae: 1.6311 - mse: 5.5481 - val_loss: 6.5311 - val_mae: 1.8295 - val_mse: 6.5311\n",
            "Epoch 204/1000\n",
            "250/250 [==============================] - 0s 86us/sample - loss: 5.4395 - mae: 1.5986 - mse: 5.4395 - val_loss: 5.7499 - val_mae: 1.7130 - val_mse: 5.7499\n",
            "Epoch 205/1000\n",
            "250/250 [==============================] - 0s 97us/sample - loss: 5.4283 - mae: 1.6138 - mse: 5.4283 - val_loss: 5.5905 - val_mae: 1.6864 - val_mse: 5.5905\n",
            "Epoch 206/1000\n",
            "250/250 [==============================] - 0s 90us/sample - loss: 5.5655 - mae: 1.5957 - mse: 5.5655 - val_loss: 5.3930 - val_mae: 1.6505 - val_mse: 5.3930\n",
            "Epoch 207/1000\n",
            "250/250 [==============================] - 0s 91us/sample - loss: 5.3024 - mae: 1.6100 - mse: 5.3024 - val_loss: 5.8092 - val_mae: 1.7286 - val_mse: 5.8092\n",
            "Epoch 208/1000\n",
            "250/250 [==============================] - 0s 82us/sample - loss: 5.3820 - mae: 1.6005 - mse: 5.3820 - val_loss: 6.0275 - val_mae: 1.7560 - val_mse: 6.0275\n",
            "Epoch 209/1000\n",
            "250/250 [==============================] - 0s 101us/sample - loss: 5.6558 - mae: 1.6578 - mse: 5.6558 - val_loss: 5.3965 - val_mae: 1.6936 - val_mse: 5.3965\n",
            "Epoch 210/1000\n",
            "250/250 [==============================] - 0s 82us/sample - loss: 5.4280 - mae: 1.6147 - mse: 5.4280 - val_loss: 6.0510 - val_mae: 1.7645 - val_mse: 6.0510\n",
            "Epoch 211/1000\n",
            "250/250 [==============================] - 0s 86us/sample - loss: 5.5589 - mae: 1.6607 - mse: 5.5589 - val_loss: 6.5362 - val_mae: 1.8160 - val_mse: 6.5362\n",
            "Epoch 212/1000\n",
            "250/250 [==============================] - 0s 98us/sample - loss: 5.3065 - mae: 1.5849 - mse: 5.3065 - val_loss: 5.3678 - val_mae: 1.6486 - val_mse: 5.3678\n",
            "Epoch 213/1000\n",
            "250/250 [==============================] - 0s 102us/sample - loss: 5.4185 - mae: 1.6221 - mse: 5.4185 - val_loss: 5.7483 - val_mae: 1.7221 - val_mse: 5.7483\n",
            "Epoch 214/1000\n",
            "250/250 [==============================] - 0s 118us/sample - loss: 5.4093 - mae: 1.6070 - mse: 5.4093 - val_loss: 5.9114 - val_mae: 1.7285 - val_mse: 5.9114\n",
            "Epoch 215/1000\n",
            "250/250 [==============================] - 0s 128us/sample - loss: 5.3723 - mae: 1.5972 - mse: 5.3723 - val_loss: 5.5133 - val_mae: 1.6700 - val_mse: 5.5133\n",
            "Epoch 216/1000\n",
            "250/250 [==============================] - 0s 92us/sample - loss: 5.3944 - mae: 1.6145 - mse: 5.3944 - val_loss: 7.3298 - val_mae: 1.9680 - val_mse: 7.3298\n",
            "Epoch 217/1000\n",
            "250/250 [==============================] - 0s 99us/sample - loss: 5.5814 - mae: 1.6251 - mse: 5.5814 - val_loss: 5.4168 - val_mae: 1.6613 - val_mse: 5.4168\n",
            "Epoch 218/1000\n",
            "250/250 [==============================] - 0s 94us/sample - loss: 5.4608 - mae: 1.6111 - mse: 5.4608 - val_loss: 6.3076 - val_mae: 1.7994 - val_mse: 6.3076\n",
            "Epoch 219/1000\n",
            "250/250 [==============================] - 0s 91us/sample - loss: 5.5861 - mae: 1.6491 - mse: 5.5861 - val_loss: 6.2431 - val_mae: 1.7673 - val_mse: 6.2431\n",
            "Epoch 220/1000\n",
            "250/250 [==============================] - 0s 84us/sample - loss: 5.2229 - mae: 1.5879 - mse: 5.2229 - val_loss: 6.1514 - val_mae: 1.7513 - val_mse: 6.1514\n",
            "Epoch 221/1000\n",
            "250/250 [==============================] - 0s 88us/sample - loss: 5.3856 - mae: 1.5897 - mse: 5.3856 - val_loss: 5.5624 - val_mae: 1.6720 - val_mse: 5.5624\n",
            "Epoch 222/1000\n",
            "250/250 [==============================] - 0s 101us/sample - loss: 5.4181 - mae: 1.6072 - mse: 5.4181 - val_loss: 6.2581 - val_mae: 1.7666 - val_mse: 6.2581\n",
            "Epoch 223/1000\n",
            "250/250 [==============================] - 0s 93us/sample - loss: 5.4401 - mae: 1.5884 - mse: 5.4401 - val_loss: 5.9345 - val_mae: 1.7549 - val_mse: 5.9345\n",
            "Epoch 224/1000\n",
            "250/250 [==============================] - 0s 99us/sample - loss: 5.2967 - mae: 1.5913 - mse: 5.2967 - val_loss: 7.2115 - val_mae: 1.9089 - val_mse: 7.2115\n",
            "Epoch 225/1000\n",
            "250/250 [==============================] - 0s 101us/sample - loss: 5.3624 - mae: 1.5698 - mse: 5.3624 - val_loss: 5.5415 - val_mae: 1.6773 - val_mse: 5.5415\n",
            "Epoch 226/1000\n",
            "250/250 [==============================] - 0s 85us/sample - loss: 5.2680 - mae: 1.6002 - mse: 5.2680 - val_loss: 6.3563 - val_mae: 1.7814 - val_mse: 6.3563\n",
            "Epoch 227/1000\n",
            "250/250 [==============================] - 0s 92us/sample - loss: 5.3085 - mae: 1.5638 - mse: 5.3085 - val_loss: 5.7467 - val_mae: 1.7165 - val_mse: 5.7467\n",
            "Epoch 228/1000\n",
            "250/250 [==============================] - 0s 89us/sample - loss: 5.3363 - mae: 1.5742 - mse: 5.3363 - val_loss: 5.6538 - val_mae: 1.7002 - val_mse: 5.6538\n",
            "Epoch 229/1000\n",
            "250/250 [==============================] - 0s 84us/sample - loss: 5.3932 - mae: 1.5959 - mse: 5.3932 - val_loss: 5.3758 - val_mae: 1.6752 - val_mse: 5.3758\n",
            "Epoch 230/1000\n",
            "250/250 [==============================] - 0s 93us/sample - loss: 5.4217 - mae: 1.6149 - mse: 5.4217 - val_loss: 5.4782 - val_mae: 1.6698 - val_mse: 5.4782\n",
            "Epoch 231/1000\n",
            "250/250 [==============================] - 0s 86us/sample - loss: 5.4005 - mae: 1.6217 - mse: 5.4005 - val_loss: 5.9087 - val_mae: 1.7304 - val_mse: 5.9087\n",
            "Epoch 232/1000\n",
            "250/250 [==============================] - 0s 102us/sample - loss: 5.2347 - mae: 1.5788 - mse: 5.2347 - val_loss: 5.3433 - val_mae: 1.6553 - val_mse: 5.3433\n",
            "Epoch 233/1000\n",
            "250/250 [==============================] - 0s 99us/sample - loss: 5.3770 - mae: 1.5811 - mse: 5.3770 - val_loss: 5.6042 - val_mae: 1.6877 - val_mse: 5.6042\n",
            "Epoch 234/1000\n",
            "250/250 [==============================] - 0s 139us/sample - loss: 5.4086 - mae: 1.6022 - mse: 5.4086 - val_loss: 5.7763 - val_mae: 1.7124 - val_mse: 5.7763\n",
            "Epoch 235/1000\n",
            "250/250 [==============================] - 0s 92us/sample - loss: 5.1935 - mae: 1.5713 - mse: 5.1935 - val_loss: 6.0888 - val_mae: 1.7438 - val_mse: 6.0888\n",
            "Epoch 236/1000\n",
            "250/250 [==============================] - 0s 86us/sample - loss: 5.2543 - mae: 1.5782 - mse: 5.2543 - val_loss: 5.5209 - val_mae: 1.6814 - val_mse: 5.5209\n",
            "Epoch 237/1000\n",
            "250/250 [==============================] - 0s 108us/sample - loss: 5.3275 - mae: 1.5945 - mse: 5.3275 - val_loss: 5.3236 - val_mae: 1.6612 - val_mse: 5.3236\n",
            "Epoch 238/1000\n",
            "250/250 [==============================] - 0s 119us/sample - loss: 5.2854 - mae: 1.6142 - mse: 5.2854 - val_loss: 5.4953 - val_mae: 1.7246 - val_mse: 5.4953\n",
            "Epoch 239/1000\n",
            "250/250 [==============================] - 0s 97us/sample - loss: 5.2377 - mae: 1.5994 - mse: 5.2377 - val_loss: 5.8279 - val_mae: 1.7408 - val_mse: 5.8279\n",
            "Epoch 240/1000\n",
            "250/250 [==============================] - 0s 104us/sample - loss: 5.1936 - mae: 1.6005 - mse: 5.1936 - val_loss: 5.9505 - val_mae: 1.7407 - val_mse: 5.9505\n",
            "Epoch 241/1000\n",
            "250/250 [==============================] - 0s 94us/sample - loss: 5.0520 - mae: 1.5475 - mse: 5.0520 - val_loss: 8.4283 - val_mae: 2.0988 - val_mse: 8.4283\n",
            "Epoch 242/1000\n",
            "250/250 [==============================] - 0s 91us/sample - loss: 5.3226 - mae: 1.5862 - mse: 5.3226 - val_loss: 5.3507 - val_mae: 1.6744 - val_mse: 5.3507\n",
            "Epoch 243/1000\n",
            "250/250 [==============================] - 0s 100us/sample - loss: 5.3014 - mae: 1.5777 - mse: 5.3014 - val_loss: 6.1296 - val_mae: 1.7674 - val_mse: 6.1296\n",
            "Epoch 244/1000\n",
            "250/250 [==============================] - 0s 106us/sample - loss: 5.2439 - mae: 1.5890 - mse: 5.2439 - val_loss: 6.0171 - val_mae: 1.7451 - val_mse: 6.0171\n",
            "Epoch 245/1000\n",
            "250/250 [==============================] - 0s 121us/sample - loss: 5.3722 - mae: 1.5701 - mse: 5.3722 - val_loss: 6.1953 - val_mae: 1.7608 - val_mse: 6.1953\n",
            "Epoch 246/1000\n",
            "250/250 [==============================] - 0s 102us/sample - loss: 5.1855 - mae: 1.5294 - mse: 5.1855 - val_loss: 6.6699 - val_mae: 1.8639 - val_mse: 6.6699\n",
            "Epoch 247/1000\n",
            "250/250 [==============================] - 0s 96us/sample - loss: 5.3346 - mae: 1.5986 - mse: 5.3346 - val_loss: 6.4140 - val_mae: 1.8411 - val_mse: 6.4140\n",
            "Epoch 248/1000\n",
            "250/250 [==============================] - 0s 121us/sample - loss: 5.1320 - mae: 1.5501 - mse: 5.1320 - val_loss: 5.7293 - val_mae: 1.7249 - val_mse: 5.7293\n",
            "Epoch 249/1000\n",
            "250/250 [==============================] - 0s 100us/sample - loss: 5.1655 - mae: 1.5833 - mse: 5.1655 - val_loss: 7.2877 - val_mae: 1.9209 - val_mse: 7.2877\n",
            "Epoch 250/1000\n",
            "250/250 [==============================] - 0s 95us/sample - loss: 5.3365 - mae: 1.5723 - mse: 5.3365 - val_loss: 5.8192 - val_mae: 1.7067 - val_mse: 5.8192\n",
            "Epoch 251/1000\n",
            "250/250 [==============================] - 0s 105us/sample - loss: 5.3444 - mae: 1.5879 - mse: 5.3444 - val_loss: 6.4543 - val_mae: 1.7830 - val_mse: 6.4543\n",
            "Epoch 252/1000\n",
            "250/250 [==============================] - 0s 95us/sample - loss: 5.2542 - mae: 1.5879 - mse: 5.2542 - val_loss: 6.5289 - val_mae: 1.7922 - val_mse: 6.5289\n",
            "Epoch 253/1000\n",
            "250/250 [==============================] - 0s 91us/sample - loss: 5.3616 - mae: 1.5767 - mse: 5.3616 - val_loss: 6.3167 - val_mae: 1.7862 - val_mse: 6.3167\n",
            "Epoch 254/1000\n",
            "250/250 [==============================] - 0s 127us/sample - loss: 5.2595 - mae: 1.5568 - mse: 5.2595 - val_loss: 5.4592 - val_mae: 1.6691 - val_mse: 5.4592\n",
            "Epoch 255/1000\n",
            "250/250 [==============================] - 0s 118us/sample - loss: 5.0987 - mae: 1.5540 - mse: 5.0987 - val_loss: 6.5906 - val_mae: 1.7986 - val_mse: 6.5906\n",
            "Epoch 256/1000\n",
            "250/250 [==============================] - 0s 92us/sample - loss: 5.5047 - mae: 1.6063 - mse: 5.5047 - val_loss: 5.6827 - val_mae: 1.7050 - val_mse: 5.6827\n",
            "Epoch 257/1000\n",
            "250/250 [==============================] - 0s 94us/sample - loss: 5.1667 - mae: 1.5579 - mse: 5.1667 - val_loss: 5.7088 - val_mae: 1.7207 - val_mse: 5.7088\n",
            "Epoch 258/1000\n",
            "250/250 [==============================] - 0s 98us/sample - loss: 5.0690 - mae: 1.5493 - mse: 5.0690 - val_loss: 5.6034 - val_mae: 1.6913 - val_mse: 5.6034\n",
            "Epoch 259/1000\n",
            "250/250 [==============================] - 0s 96us/sample - loss: 5.3542 - mae: 1.5803 - mse: 5.3542 - val_loss: 6.5889 - val_mae: 1.8318 - val_mse: 6.5889\n",
            "Epoch 260/1000\n",
            "250/250 [==============================] - 0s 88us/sample - loss: 5.0368 - mae: 1.5201 - mse: 5.0368 - val_loss: 5.5203 - val_mae: 1.6977 - val_mse: 5.5203\n",
            "Epoch 261/1000\n",
            "250/250 [==============================] - 0s 89us/sample - loss: 5.1357 - mae: 1.5358 - mse: 5.1357 - val_loss: 6.3927 - val_mae: 1.7910 - val_mse: 6.3927\n",
            "Epoch 262/1000\n",
            "250/250 [==============================] - 0s 123us/sample - loss: 5.3517 - mae: 1.5655 - mse: 5.3517 - val_loss: 5.6946 - val_mae: 1.7101 - val_mse: 5.6946\n",
            "Epoch 263/1000\n",
            "250/250 [==============================] - 0s 98us/sample - loss: 5.1217 - mae: 1.5459 - mse: 5.1217 - val_loss: 6.3472 - val_mae: 1.8024 - val_mse: 6.3472\n",
            "Epoch 264/1000\n",
            "250/250 [==============================] - 0s 89us/sample - loss: 5.2495 - mae: 1.5467 - mse: 5.2495 - val_loss: 6.2802 - val_mae: 1.7996 - val_mse: 6.2802\n",
            "Epoch 265/1000\n",
            "250/250 [==============================] - 0s 122us/sample - loss: 5.1270 - mae: 1.5281 - mse: 5.1270 - val_loss: 6.4920 - val_mae: 1.8233 - val_mse: 6.4920\n",
            "Epoch 266/1000\n",
            "250/250 [==============================] - 0s 94us/sample - loss: 5.2898 - mae: 1.5839 - mse: 5.2898 - val_loss: 6.1716 - val_mae: 1.7683 - val_mse: 6.1716\n",
            "Epoch 267/1000\n",
            "250/250 [==============================] - 0s 102us/sample - loss: 5.1005 - mae: 1.5233 - mse: 5.1005 - val_loss: 5.6898 - val_mae: 1.6993 - val_mse: 5.6898\n",
            "Epoch 268/1000\n",
            "250/250 [==============================] - 0s 111us/sample - loss: 5.0812 - mae: 1.5343 - mse: 5.0812 - val_loss: 5.7095 - val_mae: 1.7043 - val_mse: 5.7095\n",
            "Epoch 269/1000\n",
            "250/250 [==============================] - 0s 118us/sample - loss: 5.2648 - mae: 1.5608 - mse: 5.2648 - val_loss: 6.6217 - val_mae: 1.8353 - val_mse: 6.6217\n",
            "Epoch 270/1000\n",
            "250/250 [==============================] - 0s 102us/sample - loss: 5.0922 - mae: 1.5649 - mse: 5.0922 - val_loss: 6.0998 - val_mae: 1.7477 - val_mse: 6.0998\n",
            "Epoch 271/1000\n",
            "250/250 [==============================] - 0s 93us/sample - loss: 5.0900 - mae: 1.5250 - mse: 5.0900 - val_loss: 5.2769 - val_mae: 1.6488 - val_mse: 5.2769\n",
            "Epoch 272/1000\n",
            "250/250 [==============================] - 0s 94us/sample - loss: 5.0902 - mae: 1.5136 - mse: 5.0902 - val_loss: 5.6331 - val_mae: 1.7161 - val_mse: 5.6331\n",
            "Epoch 273/1000\n",
            "250/250 [==============================] - 0s 91us/sample - loss: 5.2003 - mae: 1.5476 - mse: 5.2003 - val_loss: 5.9471 - val_mae: 1.7441 - val_mse: 5.9471\n",
            "Epoch 274/1000\n",
            "250/250 [==============================] - 0s 88us/sample - loss: 5.1929 - mae: 1.5926 - mse: 5.1929 - val_loss: 6.0338 - val_mae: 1.7454 - val_mse: 6.0338\n",
            "Epoch 275/1000\n",
            "250/250 [==============================] - 0s 100us/sample - loss: 5.0888 - mae: 1.5355 - mse: 5.0888 - val_loss: 6.3778 - val_mae: 1.8106 - val_mse: 6.3778\n",
            "Epoch 276/1000\n",
            "250/250 [==============================] - 0s 90us/sample - loss: 5.0313 - mae: 1.5037 - mse: 5.0314 - val_loss: 5.6774 - val_mae: 1.7262 - val_mse: 5.6774\n",
            "Epoch 277/1000\n",
            "250/250 [==============================] - 0s 87us/sample - loss: 4.9367 - mae: 1.5178 - mse: 4.9367 - val_loss: 6.4584 - val_mae: 1.7999 - val_mse: 6.4584\n",
            "Epoch 278/1000\n",
            "250/250 [==============================] - 0s 111us/sample - loss: 5.1508 - mae: 1.5253 - mse: 5.1508 - val_loss: 6.3546 - val_mae: 1.7844 - val_mse: 6.3546\n",
            "Epoch 279/1000\n",
            "250/250 [==============================] - 0s 95us/sample - loss: 5.2012 - mae: 1.5628 - mse: 5.2012 - val_loss: 5.9885 - val_mae: 1.7388 - val_mse: 5.9885\n",
            "Epoch 280/1000\n",
            "250/250 [==============================] - 0s 109us/sample - loss: 5.0635 - mae: 1.5191 - mse: 5.0635 - val_loss: 5.7220 - val_mae: 1.7218 - val_mse: 5.7220\n",
            "Epoch 281/1000\n",
            "250/250 [==============================] - 0s 86us/sample - loss: 4.9663 - mae: 1.5182 - mse: 4.9663 - val_loss: 6.0597 - val_mae: 1.7830 - val_mse: 6.0597\n",
            "Epoch 282/1000\n",
            "250/250 [==============================] - 0s 102us/sample - loss: 5.1199 - mae: 1.5278 - mse: 5.1199 - val_loss: 5.6004 - val_mae: 1.7005 - val_mse: 5.6004\n",
            "Epoch 283/1000\n",
            "250/250 [==============================] - 0s 99us/sample - loss: 4.9989 - mae: 1.5250 - mse: 4.9989 - val_loss: 6.0375 - val_mae: 1.7590 - val_mse: 6.0375\n",
            "Epoch 284/1000\n",
            "250/250 [==============================] - 0s 110us/sample - loss: 5.1244 - mae: 1.5317 - mse: 5.1244 - val_loss: 5.5674 - val_mae: 1.6996 - val_mse: 5.5674\n",
            "Epoch 285/1000\n",
            "250/250 [==============================] - 0s 97us/sample - loss: 4.8871 - mae: 1.4743 - mse: 4.8871 - val_loss: 5.3286 - val_mae: 1.6939 - val_mse: 5.3286\n",
            "Epoch 286/1000\n",
            "250/250 [==============================] - 0s 99us/sample - loss: 5.1446 - mae: 1.5738 - mse: 5.1446 - val_loss: 6.0781 - val_mae: 1.7384 - val_mse: 6.0781\n",
            "Epoch 287/1000\n",
            "250/250 [==============================] - 0s 130us/sample - loss: 5.2248 - mae: 1.5265 - mse: 5.2248 - val_loss: 5.6415 - val_mae: 1.6892 - val_mse: 5.6415\n",
            "Epoch 288/1000\n",
            "250/250 [==============================] - 0s 94us/sample - loss: 5.0825 - mae: 1.5371 - mse: 5.0825 - val_loss: 5.4750 - val_mae: 1.6851 - val_mse: 5.4750\n",
            "Epoch 289/1000\n",
            "250/250 [==============================] - 0s 76us/sample - loss: 5.0580 - mae: 1.5032 - mse: 5.0580 - val_loss: 6.8169 - val_mae: 1.8402 - val_mse: 6.8169\n",
            "Epoch 290/1000\n",
            "250/250 [==============================] - 0s 88us/sample - loss: 5.1798 - mae: 1.5420 - mse: 5.1798 - val_loss: 5.9591 - val_mae: 1.7450 - val_mse: 5.9591\n",
            "Epoch 291/1000\n",
            "250/250 [==============================] - 0s 87us/sample - loss: 5.0209 - mae: 1.5035 - mse: 5.0209 - val_loss: 5.5215 - val_mae: 1.6995 - val_mse: 5.5215\n",
            "Epoch 292/1000\n",
            "250/250 [==============================] - 0s 122us/sample - loss: 5.0284 - mae: 1.5349 - mse: 5.0284 - val_loss: 6.3693 - val_mae: 1.7871 - val_mse: 6.3693\n",
            "Epoch 293/1000\n",
            "250/250 [==============================] - 0s 97us/sample - loss: 4.9770 - mae: 1.5068 - mse: 4.9770 - val_loss: 6.7592 - val_mae: 1.8389 - val_mse: 6.7592\n",
            "Epoch 294/1000\n",
            "250/250 [==============================] - 0s 97us/sample - loss: 5.0601 - mae: 1.5452 - mse: 5.0601 - val_loss: 5.5821 - val_mae: 1.6918 - val_mse: 5.5821\n",
            "Epoch 295/1000\n",
            "250/250 [==============================] - 0s 124us/sample - loss: 4.8637 - mae: 1.5065 - mse: 4.8637 - val_loss: 6.2248 - val_mae: 1.7818 - val_mse: 6.2248\n",
            "Epoch 296/1000\n",
            "250/250 [==============================] - 0s 101us/sample - loss: 5.1922 - mae: 1.5551 - mse: 5.1922 - val_loss: 6.2352 - val_mae: 1.7837 - val_mse: 6.2352\n",
            "Epoch 297/1000\n",
            "250/250 [==============================] - 0s 93us/sample - loss: 5.0486 - mae: 1.5187 - mse: 5.0486 - val_loss: 6.4843 - val_mae: 1.8050 - val_mse: 6.4843\n",
            "Epoch 298/1000\n",
            "250/250 [==============================] - 0s 87us/sample - loss: 4.9711 - mae: 1.4811 - mse: 4.9711 - val_loss: 5.4683 - val_mae: 1.6868 - val_mse: 5.4683\n",
            "Epoch 299/1000\n",
            "250/250 [==============================] - 0s 85us/sample - loss: 5.2120 - mae: 1.5509 - mse: 5.2120 - val_loss: 6.0706 - val_mae: 1.7522 - val_mse: 6.0706\n",
            "Epoch 300/1000\n",
            "250/250 [==============================] - 0s 91us/sample - loss: 4.9827 - mae: 1.5242 - mse: 4.9827 - val_loss: 6.5334 - val_mae: 1.8032 - val_mse: 6.5334\n",
            "Epoch 301/1000\n",
            "250/250 [==============================] - 0s 90us/sample - loss: 5.1286 - mae: 1.5180 - mse: 5.1286 - val_loss: 6.4762 - val_mae: 1.7987 - val_mse: 6.4762\n",
            "Epoch 302/1000\n",
            "250/250 [==============================] - 0s 104us/sample - loss: 5.0577 - mae: 1.5124 - mse: 5.0577 - val_loss: 5.7704 - val_mae: 1.7159 - val_mse: 5.7704\n",
            "Epoch 303/1000\n",
            "250/250 [==============================] - 0s 83us/sample - loss: 5.0004 - mae: 1.5167 - mse: 5.0004 - val_loss: 5.7035 - val_mae: 1.6981 - val_mse: 5.7035\n",
            "Epoch 304/1000\n",
            "250/250 [==============================] - 0s 88us/sample - loss: 4.9776 - mae: 1.5104 - mse: 4.9776 - val_loss: 6.6301 - val_mae: 1.8083 - val_mse: 6.6301\n",
            "Epoch 305/1000\n",
            "250/250 [==============================] - 0s 115us/sample - loss: 4.8509 - mae: 1.4846 - mse: 4.8509 - val_loss: 6.8463 - val_mae: 1.8646 - val_mse: 6.8463\n",
            "Epoch 306/1000\n",
            "250/250 [==============================] - 0s 122us/sample - loss: 5.0738 - mae: 1.5137 - mse: 5.0738 - val_loss: 5.3866 - val_mae: 1.6702 - val_mse: 5.3866\n",
            "Epoch 307/1000\n",
            "250/250 [==============================] - 0s 96us/sample - loss: 5.1435 - mae: 1.5783 - mse: 5.1435 - val_loss: 6.8613 - val_mae: 1.8599 - val_mse: 6.8613\n",
            "Epoch 308/1000\n",
            "250/250 [==============================] - 0s 94us/sample - loss: 4.9389 - mae: 1.4923 - mse: 4.9389 - val_loss: 5.9686 - val_mae: 1.7339 - val_mse: 5.9686\n",
            "Epoch 309/1000\n",
            "250/250 [==============================] - 0s 130us/sample - loss: 4.8746 - mae: 1.4925 - mse: 4.8746 - val_loss: 5.5337 - val_mae: 1.6820 - val_mse: 5.5337\n",
            "Epoch 310/1000\n",
            "250/250 [==============================] - 0s 92us/sample - loss: 4.8769 - mae: 1.4990 - mse: 4.8769 - val_loss: 6.1017 - val_mae: 1.7587 - val_mse: 6.1017\n",
            "Epoch 311/1000\n",
            "250/250 [==============================] - 0s 98us/sample - loss: 4.9426 - mae: 1.5155 - mse: 4.9426 - val_loss: 6.4773 - val_mae: 1.8032 - val_mse: 6.4773\n",
            "Epoch 312/1000\n",
            "250/250 [==============================] - 0s 105us/sample - loss: 4.9988 - mae: 1.5139 - mse: 4.9988 - val_loss: 6.8516 - val_mae: 1.8390 - val_mse: 6.8516\n",
            "Epoch 313/1000\n",
            "250/250 [==============================] - 0s 96us/sample - loss: 4.7616 - mae: 1.4628 - mse: 4.7616 - val_loss: 5.2587 - val_mae: 1.6585 - val_mse: 5.2587\n",
            "Epoch 314/1000\n",
            "250/250 [==============================] - 0s 95us/sample - loss: 5.0056 - mae: 1.5012 - mse: 5.0056 - val_loss: 5.4701 - val_mae: 1.6726 - val_mse: 5.4701\n",
            "Epoch 315/1000\n",
            "250/250 [==============================] - 0s 89us/sample - loss: 4.7909 - mae: 1.4847 - mse: 4.7909 - val_loss: 7.2835 - val_mae: 1.9000 - val_mse: 7.2835\n",
            "Epoch 316/1000\n",
            "250/250 [==============================] - 0s 120us/sample - loss: 5.0659 - mae: 1.4993 - mse: 5.0659 - val_loss: 6.0354 - val_mae: 1.7651 - val_mse: 6.0354\n",
            "Epoch 317/1000\n",
            "250/250 [==============================] - 0s 108us/sample - loss: 4.9153 - mae: 1.4954 - mse: 4.9153 - val_loss: 5.7135 - val_mae: 1.7104 - val_mse: 5.7135\n",
            "Epoch 318/1000\n",
            "250/250 [==============================] - 0s 105us/sample - loss: 4.9401 - mae: 1.4866 - mse: 4.9401 - val_loss: 6.6984 - val_mae: 1.8482 - val_mse: 6.6984\n",
            "Epoch 319/1000\n",
            "250/250 [==============================] - 0s 113us/sample - loss: 4.9397 - mae: 1.4900 - mse: 4.9397 - val_loss: 5.5747 - val_mae: 1.7046 - val_mse: 5.5747\n",
            "Epoch 320/1000\n",
            "250/250 [==============================] - 0s 91us/sample - loss: 5.2046 - mae: 1.5470 - mse: 5.2046 - val_loss: 5.4907 - val_mae: 1.6789 - val_mse: 5.4907\n",
            "Epoch 321/1000\n",
            "250/250 [==============================] - 0s 74us/sample - loss: 4.8351 - mae: 1.5110 - mse: 4.8351 - val_loss: 6.1571 - val_mae: 1.7623 - val_mse: 6.1571\n",
            "Epoch 322/1000\n",
            "250/250 [==============================] - 0s 68us/sample - loss: 4.9352 - mae: 1.4877 - mse: 4.9352 - val_loss: 6.0776 - val_mae: 1.7515 - val_mse: 6.0776\n",
            "Epoch 323/1000\n",
            "250/250 [==============================] - 0s 70us/sample - loss: 4.9198 - mae: 1.4709 - mse: 4.9198 - val_loss: 5.2347 - val_mae: 1.6506 - val_mse: 5.2347\n",
            "Epoch 324/1000\n",
            "250/250 [==============================] - 0s 63us/sample - loss: 5.1740 - mae: 1.5489 - mse: 5.1740 - val_loss: 5.4146 - val_mae: 1.6718 - val_mse: 5.4146\n",
            "Epoch 325/1000\n",
            "250/250 [==============================] - 0s 68us/sample - loss: 4.9033 - mae: 1.4887 - mse: 4.9033 - val_loss: 5.8346 - val_mae: 1.7365 - val_mse: 5.8346\n",
            "Epoch 326/1000\n",
            "250/250 [==============================] - 0s 63us/sample - loss: 4.9020 - mae: 1.4922 - mse: 4.9020 - val_loss: 6.1175 - val_mae: 1.7671 - val_mse: 6.1175\n",
            "Epoch 327/1000\n",
            "250/250 [==============================] - 0s 71us/sample - loss: 4.8221 - mae: 1.4823 - mse: 4.8221 - val_loss: 6.0206 - val_mae: 1.7324 - val_mse: 6.0206\n",
            "Epoch 328/1000\n",
            "250/250 [==============================] - 0s 65us/sample - loss: 4.9384 - mae: 1.4765 - mse: 4.9384 - val_loss: 6.2224 - val_mae: 1.7936 - val_mse: 6.2224\n",
            "Epoch 329/1000\n",
            "250/250 [==============================] - 0s 71us/sample - loss: 4.8453 - mae: 1.4916 - mse: 4.8453 - val_loss: 5.6072 - val_mae: 1.6846 - val_mse: 5.6072\n",
            "Epoch 330/1000\n",
            "250/250 [==============================] - 0s 77us/sample - loss: 4.8269 - mae: 1.4837 - mse: 4.8269 - val_loss: 5.8051 - val_mae: 1.7159 - val_mse: 5.8051\n",
            "Epoch 331/1000\n",
            "250/250 [==============================] - 0s 76us/sample - loss: 4.9264 - mae: 1.4950 - mse: 4.9264 - val_loss: 6.2059 - val_mae: 1.7565 - val_mse: 6.2059\n",
            "Epoch 332/1000\n",
            "250/250 [==============================] - 0s 69us/sample - loss: 4.8321 - mae: 1.4907 - mse: 4.8321 - val_loss: 5.7853 - val_mae: 1.7096 - val_mse: 5.7853\n",
            "Epoch 333/1000\n",
            "250/250 [==============================] - 0s 73us/sample - loss: 4.8358 - mae: 1.4698 - mse: 4.8358 - val_loss: 6.0190 - val_mae: 1.7458 - val_mse: 6.0190\n",
            "Epoch 334/1000\n",
            "250/250 [==============================] - 0s 85us/sample - loss: 4.8614 - mae: 1.4870 - mse: 4.8614 - val_loss: 5.4670 - val_mae: 1.6765 - val_mse: 5.4670\n",
            "Epoch 335/1000\n",
            "250/250 [==============================] - 0s 65us/sample - loss: 4.9039 - mae: 1.4910 - mse: 4.9039 - val_loss: 5.6555 - val_mae: 1.7150 - val_mse: 5.6555\n",
            "Epoch 336/1000\n",
            "250/250 [==============================] - 0s 76us/sample - loss: 4.7426 - mae: 1.4837 - mse: 4.7426 - val_loss: 5.5791 - val_mae: 1.7237 - val_mse: 5.5791\n",
            "Epoch 337/1000\n",
            "250/250 [==============================] - 0s 76us/sample - loss: 4.9067 - mae: 1.5021 - mse: 4.9067 - val_loss: 5.6628 - val_mae: 1.6977 - val_mse: 5.6628\n",
            "Epoch 338/1000\n",
            "250/250 [==============================] - 0s 77us/sample - loss: 4.8675 - mae: 1.4924 - mse: 4.8675 - val_loss: 6.2671 - val_mae: 1.7686 - val_mse: 6.2671\n",
            "Epoch 339/1000\n",
            "250/250 [==============================] - 0s 94us/sample - loss: 4.8301 - mae: 1.4643 - mse: 4.8301 - val_loss: 5.9381 - val_mae: 1.7233 - val_mse: 5.9381\n",
            "Epoch 340/1000\n",
            "250/250 [==============================] - 0s 86us/sample - loss: 4.7991 - mae: 1.4503 - mse: 4.7991 - val_loss: 5.9708 - val_mae: 1.7289 - val_mse: 5.9708\n",
            "Epoch 341/1000\n",
            "250/250 [==============================] - 0s 90us/sample - loss: 4.9278 - mae: 1.4726 - mse: 4.9278 - val_loss: 5.4262 - val_mae: 1.6711 - val_mse: 5.4262\n",
            "Epoch 342/1000\n",
            "250/250 [==============================] - 0s 77us/sample - loss: 4.9514 - mae: 1.4835 - mse: 4.9514 - val_loss: 5.2979 - val_mae: 1.6915 - val_mse: 5.2979\n",
            "Epoch 343/1000\n",
            "250/250 [==============================] - 0s 71us/sample - loss: 4.9009 - mae: 1.5142 - mse: 4.9009 - val_loss: 6.8518 - val_mae: 1.8564 - val_mse: 6.8518\n",
            "Epoch 344/1000\n",
            "250/250 [==============================] - 0s 72us/sample - loss: 4.7507 - mae: 1.4568 - mse: 4.7507 - val_loss: 5.8669 - val_mae: 1.7308 - val_mse: 5.8669\n",
            "Epoch 345/1000\n",
            "250/250 [==============================] - 0s 74us/sample - loss: 4.6581 - mae: 1.4292 - mse: 4.6581 - val_loss: 6.3509 - val_mae: 1.7775 - val_mse: 6.3509\n",
            "Epoch 346/1000\n",
            "250/250 [==============================] - 0s 67us/sample - loss: 4.7722 - mae: 1.4429 - mse: 4.7722 - val_loss: 5.3082 - val_mae: 1.6425 - val_mse: 5.3082\n",
            "Epoch 347/1000\n",
            "250/250 [==============================] - 0s 67us/sample - loss: 4.9937 - mae: 1.4983 - mse: 4.9937 - val_loss: 5.3291 - val_mae: 1.6773 - val_mse: 5.3291\n",
            "Epoch 348/1000\n",
            "250/250 [==============================] - 0s 89us/sample - loss: 4.7600 - mae: 1.4665 - mse: 4.7600 - val_loss: 6.0515 - val_mae: 1.7377 - val_mse: 6.0515\n",
            "Epoch 349/1000\n",
            "250/250 [==============================] - 0s 72us/sample - loss: 4.8336 - mae: 1.4533 - mse: 4.8336 - val_loss: 6.0199 - val_mae: 1.7347 - val_mse: 6.0199\n",
            "Epoch 350/1000\n",
            "250/250 [==============================] - 0s 79us/sample - loss: 4.8123 - mae: 1.4800 - mse: 4.8123 - val_loss: 5.7740 - val_mae: 1.7073 - val_mse: 5.7740\n",
            "Epoch 351/1000\n",
            "250/250 [==============================] - 0s 64us/sample - loss: 4.7688 - mae: 1.4666 - mse: 4.7688 - val_loss: 5.5569 - val_mae: 1.6836 - val_mse: 5.5569\n",
            "Epoch 352/1000\n",
            "250/250 [==============================] - 0s 62us/sample - loss: 4.6625 - mae: 1.4542 - mse: 4.6625 - val_loss: 6.1640 - val_mae: 1.7384 - val_mse: 6.1640\n",
            "Epoch 353/1000\n",
            "250/250 [==============================] - 0s 90us/sample - loss: 4.8451 - mae: 1.4832 - mse: 4.8451 - val_loss: 5.3336 - val_mae: 1.6620 - val_mse: 5.3336\n",
            "Epoch 354/1000\n",
            "250/250 [==============================] - 0s 67us/sample - loss: 4.8577 - mae: 1.4975 - mse: 4.8577 - val_loss: 5.5189 - val_mae: 1.6687 - val_mse: 5.5189\n",
            "Epoch 355/1000\n",
            "250/250 [==============================] - 0s 82us/sample - loss: 4.8078 - mae: 1.4375 - mse: 4.8078 - val_loss: 5.8503 - val_mae: 1.7607 - val_mse: 5.8503\n",
            "Epoch 356/1000\n",
            "250/250 [==============================] - 0s 84us/sample - loss: 4.6975 - mae: 1.4526 - mse: 4.6975 - val_loss: 5.8431 - val_mae: 1.6977 - val_mse: 5.8431\n",
            "Epoch 357/1000\n",
            "250/250 [==============================] - 0s 79us/sample - loss: 4.9269 - mae: 1.4993 - mse: 4.9269 - val_loss: 6.0246 - val_mae: 1.7267 - val_mse: 6.0246\n",
            "Epoch 358/1000\n",
            "250/250 [==============================] - 0s 79us/sample - loss: 4.7783 - mae: 1.4588 - mse: 4.7783 - val_loss: 6.2405 - val_mae: 1.7508 - val_mse: 6.2405\n",
            "Epoch 359/1000\n",
            "250/250 [==============================] - 0s 96us/sample - loss: 4.7726 - mae: 1.4806 - mse: 4.7726 - val_loss: 5.3690 - val_mae: 1.6549 - val_mse: 5.3690\n",
            "Epoch 360/1000\n",
            "250/250 [==============================] - 0s 66us/sample - loss: 4.7034 - mae: 1.4355 - mse: 4.7034 - val_loss: 5.4230 - val_mae: 1.6775 - val_mse: 5.4230\n",
            "Epoch 361/1000\n",
            "250/250 [==============================] - 0s 69us/sample - loss: 4.7082 - mae: 1.4618 - mse: 4.7082 - val_loss: 5.8019 - val_mae: 1.7005 - val_mse: 5.8019\n",
            "Epoch 362/1000\n",
            "250/250 [==============================] - 0s 71us/sample - loss: 4.7893 - mae: 1.4649 - mse: 4.7893 - val_loss: 6.4053 - val_mae: 1.7840 - val_mse: 6.4053\n",
            "Epoch 363/1000\n",
            "250/250 [==============================] - 0s 75us/sample - loss: 4.7511 - mae: 1.4708 - mse: 4.7511 - val_loss: 5.5469 - val_mae: 1.6796 - val_mse: 5.5469\n",
            "Epoch 364/1000\n",
            "250/250 [==============================] - 0s 77us/sample - loss: 4.7102 - mae: 1.4759 - mse: 4.7102 - val_loss: 5.7392 - val_mae: 1.7299 - val_mse: 5.7392\n",
            "Epoch 365/1000\n",
            "250/250 [==============================] - 0s 103us/sample - loss: 4.6359 - mae: 1.4357 - mse: 4.6359 - val_loss: 5.4255 - val_mae: 1.6685 - val_mse: 5.4255\n",
            "Epoch 366/1000\n",
            "250/250 [==============================] - 0s 80us/sample - loss: 4.6847 - mae: 1.4636 - mse: 4.6847 - val_loss: 5.5694 - val_mae: 1.6942 - val_mse: 5.5694\n",
            "Epoch 367/1000\n",
            "250/250 [==============================] - 0s 95us/sample - loss: 4.6229 - mae: 1.4396 - mse: 4.6229 - val_loss: 6.1753 - val_mae: 1.7661 - val_mse: 6.1753\n",
            "Epoch 368/1000\n",
            "250/250 [==============================] - 0s 94us/sample - loss: 4.6577 - mae: 1.4679 - mse: 4.6577 - val_loss: 6.5805 - val_mae: 1.8029 - val_mse: 6.5805\n",
            "Epoch 369/1000\n",
            "250/250 [==============================] - 0s 83us/sample - loss: 4.6846 - mae: 1.4556 - mse: 4.6846 - val_loss: 5.4924 - val_mae: 1.6794 - val_mse: 5.4924\n",
            "Epoch 370/1000\n",
            "250/250 [==============================] - 0s 90us/sample - loss: 4.7900 - mae: 1.4645 - mse: 4.7900 - val_loss: 5.4240 - val_mae: 1.6752 - val_mse: 5.4240\n",
            "Epoch 371/1000\n",
            "250/250 [==============================] - 0s 124us/sample - loss: 4.7602 - mae: 1.4581 - mse: 4.7602 - val_loss: 5.5005 - val_mae: 1.7211 - val_mse: 5.5005\n",
            "Epoch 372/1000\n",
            "250/250 [==============================] - 0s 87us/sample - loss: 4.7598 - mae: 1.4733 - mse: 4.7598 - val_loss: 6.0265 - val_mae: 1.7612 - val_mse: 6.0265\n",
            "Epoch 373/1000\n",
            "250/250 [==============================] - 0s 96us/sample - loss: 4.5974 - mae: 1.4172 - mse: 4.5974 - val_loss: 5.7612 - val_mae: 1.7027 - val_mse: 5.7612\n",
            "Epoch 374/1000\n",
            "250/250 [==============================] - 0s 151us/sample - loss: 4.7017 - mae: 1.4543 - mse: 4.7017 - val_loss: 5.8757 - val_mae: 1.7536 - val_mse: 5.8757\n",
            "Epoch 375/1000\n",
            "250/250 [==============================] - 0s 99us/sample - loss: 4.5418 - mae: 1.4104 - mse: 4.5418 - val_loss: 5.8972 - val_mae: 1.7458 - val_mse: 5.8972\n",
            "Epoch 376/1000\n",
            "250/250 [==============================] - 0s 91us/sample - loss: 4.7602 - mae: 1.4775 - mse: 4.7602 - val_loss: 5.9160 - val_mae: 1.7415 - val_mse: 5.9160\n",
            "Epoch 377/1000\n",
            "250/250 [==============================] - 0s 114us/sample - loss: 4.7456 - mae: 1.4257 - mse: 4.7456 - val_loss: 5.5208 - val_mae: 1.6995 - val_mse: 5.5208\n",
            "Epoch 378/1000\n",
            "250/250 [==============================] - 0s 96us/sample - loss: 4.7727 - mae: 1.4873 - mse: 4.7727 - val_loss: 5.9452 - val_mae: 1.7312 - val_mse: 5.9452\n",
            "Epoch 379/1000\n",
            "250/250 [==============================] - 0s 107us/sample - loss: 4.6838 - mae: 1.4840 - mse: 4.6838 - val_loss: 5.9070 - val_mae: 1.7373 - val_mse: 5.9070\n",
            "Epoch 380/1000\n",
            "250/250 [==============================] - 0s 124us/sample - loss: 4.7239 - mae: 1.4417 - mse: 4.7239 - val_loss: 5.8286 - val_mae: 1.7367 - val_mse: 5.8286\n",
            "Epoch 381/1000\n",
            "250/250 [==============================] - 0s 100us/sample - loss: 4.5473 - mae: 1.4177 - mse: 4.5473 - val_loss: 6.8789 - val_mae: 1.8605 - val_mse: 6.8789\n",
            "Epoch 382/1000\n",
            "250/250 [==============================] - 0s 92us/sample - loss: 4.6657 - mae: 1.4335 - mse: 4.6657 - val_loss: 5.6837 - val_mae: 1.6978 - val_mse: 5.6837\n",
            "Epoch 383/1000\n",
            "250/250 [==============================] - 0s 108us/sample - loss: 4.5425 - mae: 1.4106 - mse: 4.5425 - val_loss: 6.2372 - val_mae: 1.7588 - val_mse: 6.2372\n",
            "Epoch 384/1000\n",
            "250/250 [==============================] - 0s 109us/sample - loss: 4.6334 - mae: 1.4518 - mse: 4.6334 - val_loss: 6.1222 - val_mae: 1.7394 - val_mse: 6.1222\n",
            "Epoch 385/1000\n",
            "250/250 [==============================] - 0s 104us/sample - loss: 4.5550 - mae: 1.4374 - mse: 4.5550 - val_loss: 6.2605 - val_mae: 1.7404 - val_mse: 6.2605\n",
            "Epoch 386/1000\n",
            "250/250 [==============================] - 0s 89us/sample - loss: 4.5640 - mae: 1.4452 - mse: 4.5640 - val_loss: 6.9426 - val_mae: 1.8544 - val_mse: 6.9426\n",
            "Epoch 387/1000\n",
            "250/250 [==============================] - 0s 100us/sample - loss: 4.6277 - mae: 1.4455 - mse: 4.6277 - val_loss: 5.9669 - val_mae: 1.7736 - val_mse: 5.9669\n",
            "Epoch 388/1000\n",
            "250/250 [==============================] - 0s 91us/sample - loss: 4.6242 - mae: 1.4307 - mse: 4.6242 - val_loss: 5.4704 - val_mae: 1.6914 - val_mse: 5.4704\n",
            "Epoch 389/1000\n",
            "250/250 [==============================] - 0s 108us/sample - loss: 4.6094 - mae: 1.4404 - mse: 4.6094 - val_loss: 5.9576 - val_mae: 1.7435 - val_mse: 5.9576\n",
            "Epoch 390/1000\n",
            "250/250 [==============================] - 0s 101us/sample - loss: 4.7027 - mae: 1.4349 - mse: 4.7027 - val_loss: 6.8841 - val_mae: 1.8987 - val_mse: 6.8841\n",
            "Epoch 391/1000\n",
            "250/250 [==============================] - 0s 106us/sample - loss: 4.7046 - mae: 1.4404 - mse: 4.7046 - val_loss: 5.8684 - val_mae: 1.7291 - val_mse: 5.8684\n",
            "Epoch 392/1000\n",
            "250/250 [==============================] - 0s 88us/sample - loss: 4.5881 - mae: 1.4273 - mse: 4.5881 - val_loss: 6.7023 - val_mae: 1.8123 - val_mse: 6.7023\n",
            "Epoch 393/1000\n",
            "250/250 [==============================] - 0s 91us/sample - loss: 4.9039 - mae: 1.4852 - mse: 4.9039 - val_loss: 6.3596 - val_mae: 1.7931 - val_mse: 6.3596\n",
            "Epoch 394/1000\n",
            "250/250 [==============================] - 0s 104us/sample - loss: 4.5236 - mae: 1.4055 - mse: 4.5236 - val_loss: 6.7493 - val_mae: 1.8249 - val_mse: 6.7493\n",
            "Epoch 395/1000\n",
            "250/250 [==============================] - 0s 80us/sample - loss: 4.5403 - mae: 1.4203 - mse: 4.5403 - val_loss: 6.0817 - val_mae: 1.7779 - val_mse: 6.0817\n",
            "Epoch 396/1000\n",
            "250/250 [==============================] - 0s 79us/sample - loss: 4.6832 - mae: 1.4584 - mse: 4.6832 - val_loss: 5.8482 - val_mae: 1.7225 - val_mse: 5.8482\n",
            "Epoch 397/1000\n",
            "250/250 [==============================] - 0s 112us/sample - loss: 4.6743 - mae: 1.4544 - mse: 4.6743 - val_loss: 5.9745 - val_mae: 1.7228 - val_mse: 5.9745\n",
            "Epoch 398/1000\n",
            "250/250 [==============================] - 0s 92us/sample - loss: 4.5431 - mae: 1.3995 - mse: 4.5431 - val_loss: 5.3693 - val_mae: 1.6888 - val_mse: 5.3693\n",
            "Epoch 399/1000\n",
            "250/250 [==============================] - 0s 73us/sample - loss: 4.5421 - mae: 1.4316 - mse: 4.5421 - val_loss: 5.4776 - val_mae: 1.6615 - val_mse: 5.4776\n",
            "Epoch 400/1000\n",
            "250/250 [==============================] - 0s 75us/sample - loss: 4.5625 - mae: 1.4158 - mse: 4.5625 - val_loss: 7.0778 - val_mae: 1.8513 - val_mse: 7.0778\n",
            "Epoch 401/1000\n",
            "250/250 [==============================] - 0s 91us/sample - loss: 4.4862 - mae: 1.4053 - mse: 4.4862 - val_loss: 5.5621 - val_mae: 1.6756 - val_mse: 5.5621\n",
            "Epoch 402/1000\n",
            "250/250 [==============================] - 0s 74us/sample - loss: 4.6579 - mae: 1.4669 - mse: 4.6579 - val_loss: 5.5335 - val_mae: 1.7028 - val_mse: 5.5335\n",
            "Epoch 403/1000\n",
            "250/250 [==============================] - 0s 78us/sample - loss: 4.5378 - mae: 1.3910 - mse: 4.5378 - val_loss: 6.2769 - val_mae: 1.7970 - val_mse: 6.2769\n",
            "Epoch 404/1000\n",
            "250/250 [==============================] - 0s 71us/sample - loss: 4.3273 - mae: 1.3782 - mse: 4.3273 - val_loss: 5.4605 - val_mae: 1.7372 - val_mse: 5.4605\n",
            "Epoch 405/1000\n",
            "250/250 [==============================] - 0s 83us/sample - loss: 4.7586 - mae: 1.4540 - mse: 4.7586 - val_loss: 5.8621 - val_mae: 1.7170 - val_mse: 5.8621\n",
            "Epoch 406/1000\n",
            "250/250 [==============================] - 0s 87us/sample - loss: 4.5228 - mae: 1.4213 - mse: 4.5228 - val_loss: 6.1654 - val_mae: 1.7969 - val_mse: 6.1654\n",
            "Epoch 407/1000\n",
            "250/250 [==============================] - 0s 75us/sample - loss: 4.5206 - mae: 1.4294 - mse: 4.5206 - val_loss: 6.8365 - val_mae: 1.8292 - val_mse: 6.8365\n",
            "Epoch 408/1000\n",
            "250/250 [==============================] - 0s 72us/sample - loss: 4.7136 - mae: 1.4502 - mse: 4.7136 - val_loss: 5.9119 - val_mae: 1.7298 - val_mse: 5.9119\n",
            "Epoch 409/1000\n",
            "250/250 [==============================] - 0s 71us/sample - loss: 4.5775 - mae: 1.4256 - mse: 4.5775 - val_loss: 5.5119 - val_mae: 1.6991 - val_mse: 5.5119\n",
            "Epoch 410/1000\n",
            "250/250 [==============================] - 0s 82us/sample - loss: 4.3938 - mae: 1.4010 - mse: 4.3938 - val_loss: 7.3048 - val_mae: 1.9037 - val_mse: 7.3048\n",
            "Epoch 411/1000\n",
            "250/250 [==============================] - 0s 71us/sample - loss: 4.7038 - mae: 1.4085 - mse: 4.7038 - val_loss: 5.6401 - val_mae: 1.7118 - val_mse: 5.6401\n",
            "Epoch 412/1000\n",
            "250/250 [==============================] - 0s 63us/sample - loss: 4.4401 - mae: 1.4334 - mse: 4.4401 - val_loss: 6.1566 - val_mae: 1.7473 - val_mse: 6.1566\n",
            "Epoch 413/1000\n",
            "250/250 [==============================] - 0s 65us/sample - loss: 4.5248 - mae: 1.4090 - mse: 4.5248 - val_loss: 5.9850 - val_mae: 1.7405 - val_mse: 5.9850\n",
            "Epoch 414/1000\n",
            "250/250 [==============================] - 0s 65us/sample - loss: 4.6063 - mae: 1.4460 - mse: 4.6063 - val_loss: 6.0873 - val_mae: 1.7394 - val_mse: 6.0873\n",
            "Epoch 415/1000\n",
            "250/250 [==============================] - 0s 64us/sample - loss: 4.5264 - mae: 1.4148 - mse: 4.5264 - val_loss: 6.2665 - val_mae: 1.7739 - val_mse: 6.2665\n",
            "Epoch 416/1000\n",
            "250/250 [==============================] - 0s 86us/sample - loss: 4.5413 - mae: 1.4117 - mse: 4.5413 - val_loss: 6.6577 - val_mae: 1.8096 - val_mse: 6.6577\n",
            "Epoch 417/1000\n",
            "250/250 [==============================] - 0s 65us/sample - loss: 4.4123 - mae: 1.3719 - mse: 4.4123 - val_loss: 5.5176 - val_mae: 1.7959 - val_mse: 5.5176\n",
            "Epoch 418/1000\n",
            "250/250 [==============================] - 0s 84us/sample - loss: 4.6662 - mae: 1.4691 - mse: 4.6662 - val_loss: 5.4976 - val_mae: 1.6919 - val_mse: 5.4976\n",
            "Epoch 419/1000\n",
            "250/250 [==============================] - 0s 97us/sample - loss: 4.5453 - mae: 1.4358 - mse: 4.5453 - val_loss: 6.2089 - val_mae: 1.7465 - val_mse: 6.2089\n",
            "Epoch 420/1000\n",
            "250/250 [==============================] - 0s 78us/sample - loss: 4.6387 - mae: 1.4192 - mse: 4.6387 - val_loss: 6.3232 - val_mae: 1.7606 - val_mse: 6.3232\n",
            "Epoch 421/1000\n",
            "250/250 [==============================] - 0s 71us/sample - loss: 4.4539 - mae: 1.4025 - mse: 4.4539 - val_loss: 6.3261 - val_mae: 1.7870 - val_mse: 6.3261\n",
            "Epoch 422/1000\n",
            "250/250 [==============================] - 0s 70us/sample - loss: 4.4548 - mae: 1.4013 - mse: 4.4548 - val_loss: 7.0003 - val_mae: 1.8581 - val_mse: 7.0003\n",
            "Epoch 423/1000\n",
            "250/250 [==============================] - 0s 75us/sample - loss: 4.4119 - mae: 1.3874 - mse: 4.4119 - val_loss: 6.8886 - val_mae: 1.8191 - val_mse: 6.8886\n",
            "Epoch 424/1000\n",
            "250/250 [==============================] - 0s 90us/sample - loss: 4.6021 - mae: 1.4145 - mse: 4.6021 - val_loss: 6.2667 - val_mae: 1.7776 - val_mse: 6.2667\n",
            "Epoch 425/1000\n",
            "250/250 [==============================] - 0s 82us/sample - loss: 4.4529 - mae: 1.4052 - mse: 4.4529 - val_loss: 5.7113 - val_mae: 1.7304 - val_mse: 5.7113\n",
            "Epoch 426/1000\n",
            "250/250 [==============================] - 0s 72us/sample - loss: 4.4204 - mae: 1.3970 - mse: 4.4204 - val_loss: 5.9780 - val_mae: 1.7246 - val_mse: 5.9780\n",
            "Epoch 427/1000\n",
            "250/250 [==============================] - 0s 81us/sample - loss: 4.6326 - mae: 1.4540 - mse: 4.6326 - val_loss: 5.7483 - val_mae: 1.7145 - val_mse: 5.7483\n",
            "Epoch 428/1000\n",
            "250/250 [==============================] - 0s 67us/sample - loss: 4.5281 - mae: 1.4081 - mse: 4.5281 - val_loss: 6.8237 - val_mae: 1.8201 - val_mse: 6.8237\n",
            "Epoch 429/1000\n",
            "250/250 [==============================] - 0s 96us/sample - loss: 4.4683 - mae: 1.3765 - mse: 4.4683 - val_loss: 6.3205 - val_mae: 1.7682 - val_mse: 6.3205\n",
            "Epoch 430/1000\n",
            "250/250 [==============================] - 0s 78us/sample - loss: 4.3996 - mae: 1.3952 - mse: 4.3996 - val_loss: 6.3497 - val_mae: 1.7609 - val_mse: 6.3497\n",
            "Epoch 431/1000\n",
            "250/250 [==============================] - 0s 74us/sample - loss: 4.4476 - mae: 1.4017 - mse: 4.4476 - val_loss: 5.6987 - val_mae: 1.7029 - val_mse: 5.6987\n",
            "Epoch 432/1000\n",
            "250/250 [==============================] - 0s 70us/sample - loss: 4.2782 - mae: 1.3862 - mse: 4.2782 - val_loss: 5.9397 - val_mae: 1.7226 - val_mse: 5.9397\n",
            "Epoch 433/1000\n",
            "250/250 [==============================] - 0s 66us/sample - loss: 4.5592 - mae: 1.4464 - mse: 4.5592 - val_loss: 6.0007 - val_mae: 1.7458 - val_mse: 6.0007\n",
            "Epoch 434/1000\n",
            "250/250 [==============================] - 0s 64us/sample - loss: 4.4516 - mae: 1.3914 - mse: 4.4516 - val_loss: 6.1817 - val_mae: 1.7577 - val_mse: 6.1817\n",
            "Epoch 435/1000\n",
            "250/250 [==============================] - 0s 69us/sample - loss: 4.3931 - mae: 1.3632 - mse: 4.3931 - val_loss: 6.1383 - val_mae: 1.7315 - val_mse: 6.1383\n",
            "Epoch 436/1000\n",
            "250/250 [==============================] - 0s 100us/sample - loss: 4.3223 - mae: 1.3648 - mse: 4.3223 - val_loss: 5.5416 - val_mae: 1.6831 - val_mse: 5.5416\n",
            "Epoch 437/1000\n",
            "250/250 [==============================] - 0s 78us/sample - loss: 4.5119 - mae: 1.4181 - mse: 4.5119 - val_loss: 6.1384 - val_mae: 1.7484 - val_mse: 6.1384\n",
            "Epoch 438/1000\n",
            "250/250 [==============================] - 0s 63us/sample - loss: 4.3194 - mae: 1.3802 - mse: 4.3194 - val_loss: 6.1431 - val_mae: 1.7533 - val_mse: 6.1431\n",
            "Epoch 439/1000\n",
            "250/250 [==============================] - 0s 71us/sample - loss: 4.4876 - mae: 1.3843 - mse: 4.4876 - val_loss: 6.3168 - val_mae: 1.7745 - val_mse: 6.3168\n",
            "Epoch 440/1000\n",
            "250/250 [==============================] - 0s 75us/sample - loss: 4.3230 - mae: 1.3693 - mse: 4.3230 - val_loss: 6.4890 - val_mae: 1.8356 - val_mse: 6.4890\n",
            "Epoch 441/1000\n",
            "250/250 [==============================] - 0s 90us/sample - loss: 4.4519 - mae: 1.3882 - mse: 4.4519 - val_loss: 6.6594 - val_mae: 1.8155 - val_mse: 6.6594\n",
            "Epoch 442/1000\n",
            "250/250 [==============================] - 0s 95us/sample - loss: 4.3463 - mae: 1.3879 - mse: 4.3463 - val_loss: 5.6824 - val_mae: 1.7014 - val_mse: 5.6824\n",
            "Epoch 443/1000\n",
            "250/250 [==============================] - 0s 92us/sample - loss: 4.4065 - mae: 1.4092 - mse: 4.4065 - val_loss: 6.1672 - val_mae: 1.7556 - val_mse: 6.1672\n",
            "Epoch 444/1000\n",
            "250/250 [==============================] - 0s 88us/sample - loss: 4.3025 - mae: 1.3558 - mse: 4.3025 - val_loss: 7.2999 - val_mae: 1.8963 - val_mse: 7.2999\n",
            "Epoch 445/1000\n",
            "250/250 [==============================] - 0s 92us/sample - loss: 4.3583 - mae: 1.4030 - mse: 4.3583 - val_loss: 6.1155 - val_mae: 1.7692 - val_mse: 6.1155\n",
            "Epoch 446/1000\n",
            "250/250 [==============================] - 0s 135us/sample - loss: 4.3689 - mae: 1.3934 - mse: 4.3689 - val_loss: 6.5619 - val_mae: 1.7956 - val_mse: 6.5619\n",
            "Epoch 447/1000\n",
            "250/250 [==============================] - 0s 87us/sample - loss: 4.4418 - mae: 1.3778 - mse: 4.4418 - val_loss: 6.3235 - val_mae: 1.7758 - val_mse: 6.3235\n",
            "Epoch 448/1000\n",
            "250/250 [==============================] - 0s 101us/sample - loss: 4.3377 - mae: 1.4058 - mse: 4.3377 - val_loss: 6.2939 - val_mae: 1.7465 - val_mse: 6.2939\n",
            "Epoch 449/1000\n",
            "250/250 [==============================] - 0s 104us/sample - loss: 4.3554 - mae: 1.3924 - mse: 4.3554 - val_loss: 6.9514 - val_mae: 1.8262 - val_mse: 6.9514\n",
            "Epoch 450/1000\n",
            "250/250 [==============================] - 0s 103us/sample - loss: 4.4744 - mae: 1.4113 - mse: 4.4744 - val_loss: 6.5509 - val_mae: 1.7862 - val_mse: 6.5509\n",
            "Epoch 451/1000\n",
            "250/250 [==============================] - 0s 100us/sample - loss: 4.3446 - mae: 1.3868 - mse: 4.3446 - val_loss: 5.5040 - val_mae: 1.6787 - val_mse: 5.5040\n",
            "Epoch 452/1000\n",
            "250/250 [==============================] - 0s 105us/sample - loss: 4.3881 - mae: 1.3742 - mse: 4.3881 - val_loss: 6.9596 - val_mae: 1.8365 - val_mse: 6.9596\n",
            "Epoch 453/1000\n",
            "250/250 [==============================] - 0s 113us/sample - loss: 4.2682 - mae: 1.3745 - mse: 4.2682 - val_loss: 7.5620 - val_mae: 1.9452 - val_mse: 7.5620\n",
            "Epoch 454/1000\n",
            "250/250 [==============================] - 0s 87us/sample - loss: 4.4156 - mae: 1.4258 - mse: 4.4156 - val_loss: 6.3052 - val_mae: 1.7665 - val_mse: 6.3052\n",
            "Epoch 455/1000\n",
            "250/250 [==============================] - 0s 91us/sample - loss: 4.2421 - mae: 1.3472 - mse: 4.2421 - val_loss: 5.6705 - val_mae: 1.6985 - val_mse: 5.6705\n",
            "Epoch 456/1000\n",
            "250/250 [==============================] - 0s 98us/sample - loss: 4.3988 - mae: 1.3712 - mse: 4.3988 - val_loss: 5.7824 - val_mae: 1.7016 - val_mse: 5.7824\n",
            "Epoch 457/1000\n",
            "250/250 [==============================] - 0s 75us/sample - loss: 4.4348 - mae: 1.3973 - mse: 4.4348 - val_loss: 6.5150 - val_mae: 1.7777 - val_mse: 6.5150\n",
            "Epoch 458/1000\n",
            "250/250 [==============================] - 0s 83us/sample - loss: 4.3487 - mae: 1.3744 - mse: 4.3487 - val_loss: 6.3369 - val_mae: 1.7669 - val_mse: 6.3369\n",
            "Epoch 459/1000\n",
            "250/250 [==============================] - 0s 109us/sample - loss: 4.2488 - mae: 1.3925 - mse: 4.2488 - val_loss: 6.7377 - val_mae: 1.8452 - val_mse: 6.7377\n",
            "Epoch 460/1000\n",
            "250/250 [==============================] - 0s 87us/sample - loss: 4.2528 - mae: 1.3397 - mse: 4.2528 - val_loss: 6.3429 - val_mae: 1.7764 - val_mse: 6.3429\n",
            "Epoch 461/1000\n",
            "250/250 [==============================] - 0s 87us/sample - loss: 4.2025 - mae: 1.3575 - mse: 4.2025 - val_loss: 5.4365 - val_mae: 1.7040 - val_mse: 5.4365\n",
            "Epoch 462/1000\n",
            "250/250 [==============================] - 0s 115us/sample - loss: 4.5340 - mae: 1.3903 - mse: 4.5340 - val_loss: 5.5864 - val_mae: 1.7267 - val_mse: 5.5864\n",
            "Epoch 463/1000\n",
            "250/250 [==============================] - 0s 79us/sample - loss: 4.2846 - mae: 1.4114 - mse: 4.2846 - val_loss: 7.3923 - val_mae: 1.9139 - val_mse: 7.3923\n",
            "Epoch 464/1000\n",
            "250/250 [==============================] - 0s 82us/sample - loss: 4.4847 - mae: 1.3919 - mse: 4.4847 - val_loss: 5.5562 - val_mae: 1.7208 - val_mse: 5.5562\n",
            "Epoch 465/1000\n",
            "250/250 [==============================] - 0s 86us/sample - loss: 4.2941 - mae: 1.4007 - mse: 4.2941 - val_loss: 6.3693 - val_mae: 1.8309 - val_mse: 6.3693\n",
            "Epoch 466/1000\n",
            "250/250 [==============================] - 0s 83us/sample - loss: 4.1879 - mae: 1.3787 - mse: 4.1879 - val_loss: 5.6363 - val_mae: 1.7050 - val_mse: 5.6363\n",
            "Epoch 467/1000\n",
            "250/250 [==============================] - 0s 79us/sample - loss: 4.2475 - mae: 1.3407 - mse: 4.2475 - val_loss: 5.7697 - val_mae: 1.7002 - val_mse: 5.7697\n",
            "Epoch 468/1000\n",
            "250/250 [==============================] - 0s 86us/sample - loss: 4.3379 - mae: 1.3656 - mse: 4.3379 - val_loss: 5.5124 - val_mae: 1.6681 - val_mse: 5.5124\n",
            "Epoch 469/1000\n",
            "250/250 [==============================] - 0s 79us/sample - loss: 4.2811 - mae: 1.3426 - mse: 4.2811 - val_loss: 5.4946 - val_mae: 1.7620 - val_mse: 5.4946\n",
            "Epoch 470/1000\n",
            "250/250 [==============================] - 0s 86us/sample - loss: 4.3031 - mae: 1.3563 - mse: 4.3031 - val_loss: 5.8831 - val_mae: 1.7107 - val_mse: 5.8831\n",
            "Epoch 471/1000\n",
            "250/250 [==============================] - 0s 86us/sample - loss: 4.3558 - mae: 1.3827 - mse: 4.3558 - val_loss: 6.8018 - val_mae: 1.8295 - val_mse: 6.8018\n",
            "Epoch 472/1000\n",
            "250/250 [==============================] - 0s 76us/sample - loss: 4.1790 - mae: 1.3646 - mse: 4.1790 - val_loss: 6.2609 - val_mae: 1.7751 - val_mse: 6.2609\n",
            "Epoch 473/1000\n",
            "250/250 [==============================] - 0s 91us/sample - loss: 4.2448 - mae: 1.3577 - mse: 4.2448 - val_loss: 6.5029 - val_mae: 1.8063 - val_mse: 6.5029\n",
            "Epoch 474/1000\n",
            "250/250 [==============================] - 0s 85us/sample - loss: 4.1284 - mae: 1.3454 - mse: 4.1284 - val_loss: 6.0110 - val_mae: 1.7360 - val_mse: 6.0110\n",
            "Epoch 475/1000\n",
            "250/250 [==============================] - 0s 112us/sample - loss: 4.1162 - mae: 1.3339 - mse: 4.1162 - val_loss: 5.5886 - val_mae: 1.6908 - val_mse: 5.5886\n",
            "Epoch 476/1000\n",
            "250/250 [==============================] - 0s 84us/sample - loss: 4.4074 - mae: 1.4346 - mse: 4.4074 - val_loss: 6.3127 - val_mae: 1.7658 - val_mse: 6.3127\n",
            "Epoch 477/1000\n",
            "250/250 [==============================] - 0s 98us/sample - loss: 4.2207 - mae: 1.3657 - mse: 4.2207 - val_loss: 7.3779 - val_mae: 1.9041 - val_mse: 7.3779\n",
            "Epoch 478/1000\n",
            "250/250 [==============================] - 0s 93us/sample - loss: 4.0052 - mae: 1.3199 - mse: 4.0052 - val_loss: 5.5436 - val_mae: 1.7562 - val_mse: 5.5436\n",
            "Epoch 479/1000\n",
            "250/250 [==============================] - 0s 99us/sample - loss: 4.5348 - mae: 1.4175 - mse: 4.5348 - val_loss: 5.8785 - val_mae: 1.7570 - val_mse: 5.8785\n",
            "Epoch 480/1000\n",
            "250/250 [==============================] - 0s 93us/sample - loss: 4.2840 - mae: 1.3600 - mse: 4.2840 - val_loss: 6.3836 - val_mae: 1.8064 - val_mse: 6.3836\n",
            "Epoch 481/1000\n",
            "250/250 [==============================] - 0s 78us/sample - loss: 4.0657 - mae: 1.3155 - mse: 4.0657 - val_loss: 5.4444 - val_mae: 1.7226 - val_mse: 5.4444\n",
            "Epoch 482/1000\n",
            "250/250 [==============================] - 0s 82us/sample - loss: 4.4061 - mae: 1.4467 - mse: 4.4061 - val_loss: 6.7826 - val_mae: 1.8203 - val_mse: 6.7826\n",
            "Epoch 483/1000\n",
            "250/250 [==============================] - 0s 76us/sample - loss: 4.3855 - mae: 1.3824 - mse: 4.3855 - val_loss: 5.9604 - val_mae: 1.7339 - val_mse: 5.9604\n",
            "Epoch 484/1000\n",
            "250/250 [==============================] - 0s 94us/sample - loss: 4.2247 - mae: 1.3527 - mse: 4.2247 - val_loss: 6.3442 - val_mae: 1.7724 - val_mse: 6.3442\n",
            "Epoch 485/1000\n",
            "250/250 [==============================] - 0s 72us/sample - loss: 4.1948 - mae: 1.3346 - mse: 4.1948 - val_loss: 5.8241 - val_mae: 1.7038 - val_mse: 5.8241\n",
            "Epoch 486/1000\n",
            "250/250 [==============================] - 0s 90us/sample - loss: 4.1956 - mae: 1.3291 - mse: 4.1956 - val_loss: 5.8151 - val_mae: 1.7193 - val_mse: 5.8151\n",
            "Epoch 487/1000\n",
            "250/250 [==============================] - 0s 88us/sample - loss: 4.3777 - mae: 1.3845 - mse: 4.3777 - val_loss: 6.6813 - val_mae: 1.8117 - val_mse: 6.6813\n",
            "Epoch 488/1000\n",
            "250/250 [==============================] - 0s 107us/sample - loss: 4.1562 - mae: 1.3469 - mse: 4.1562 - val_loss: 5.9517 - val_mae: 1.7425 - val_mse: 5.9517\n",
            "Epoch 489/1000\n",
            "250/250 [==============================] - 0s 87us/sample - loss: 4.5285 - mae: 1.4163 - mse: 4.5285 - val_loss: 5.9337 - val_mae: 1.7111 - val_mse: 5.9337\n",
            "Epoch 490/1000\n",
            "250/250 [==============================] - 0s 108us/sample - loss: 4.1088 - mae: 1.3418 - mse: 4.1088 - val_loss: 5.9422 - val_mae: 1.7772 - val_mse: 5.9422\n",
            "Epoch 491/1000\n",
            "250/250 [==============================] - 0s 93us/sample - loss: 4.2096 - mae: 1.3697 - mse: 4.2096 - val_loss: 5.3703 - val_mae: 1.7106 - val_mse: 5.3703\n",
            "Epoch 492/1000\n",
            "250/250 [==============================] - 0s 96us/sample - loss: 4.4912 - mae: 1.3953 - mse: 4.4912 - val_loss: 6.5494 - val_mae: 1.8019 - val_mse: 6.5494\n",
            "Epoch 493/1000\n",
            "250/250 [==============================] - 0s 97us/sample - loss: 4.2334 - mae: 1.3458 - mse: 4.2334 - val_loss: 6.1126 - val_mae: 1.7690 - val_mse: 6.1126\n",
            "Epoch 494/1000\n",
            "250/250 [==============================] - 0s 111us/sample - loss: 4.1917 - mae: 1.3619 - mse: 4.1917 - val_loss: 6.1991 - val_mae: 1.7581 - val_mse: 6.1991\n",
            "Epoch 495/1000\n",
            "250/250 [==============================] - 0s 141us/sample - loss: 4.1535 - mae: 1.3326 - mse: 4.1535 - val_loss: 6.8048 - val_mae: 1.8416 - val_mse: 6.8048\n",
            "Epoch 496/1000\n",
            "250/250 [==============================] - 0s 92us/sample - loss: 4.2039 - mae: 1.3640 - mse: 4.2039 - val_loss: 5.9201 - val_mae: 1.7408 - val_mse: 5.9201\n",
            "Epoch 497/1000\n",
            "250/250 [==============================] - 0s 106us/sample - loss: 4.3975 - mae: 1.3628 - mse: 4.3975 - val_loss: 6.9847 - val_mae: 1.8378 - val_mse: 6.9847\n",
            "Epoch 498/1000\n",
            "250/250 [==============================] - 0s 104us/sample - loss: 4.3419 - mae: 1.3805 - mse: 4.3419 - val_loss: 6.0195 - val_mae: 1.7454 - val_mse: 6.0195\n",
            "Epoch 499/1000\n",
            "250/250 [==============================] - 0s 96us/sample - loss: 4.2602 - mae: 1.3753 - mse: 4.2602 - val_loss: 7.1800 - val_mae: 1.8859 - val_mse: 7.1800\n",
            "Epoch 500/1000\n",
            "250/250 [==============================] - 0s 91us/sample - loss: 4.2820 - mae: 1.3412 - mse: 4.2820 - val_loss: 6.3826 - val_mae: 1.7865 - val_mse: 6.3826\n",
            "Epoch 501/1000\n",
            "250/250 [==============================] - 0s 79us/sample - loss: 4.1450 - mae: 1.3298 - mse: 4.1450 - val_loss: 5.5865 - val_mae: 1.6946 - val_mse: 5.5865\n",
            "Epoch 502/1000\n",
            "250/250 [==============================] - 0s 92us/sample - loss: 4.3275 - mae: 1.3716 - mse: 4.3275 - val_loss: 6.3175 - val_mae: 1.7647 - val_mse: 6.3175\n",
            "Epoch 503/1000\n",
            "250/250 [==============================] - 0s 89us/sample - loss: 4.1060 - mae: 1.3381 - mse: 4.1060 - val_loss: 6.3065 - val_mae: 1.7627 - val_mse: 6.3065\n",
            "Epoch 504/1000\n",
            "250/250 [==============================] - 0s 101us/sample - loss: 4.1108 - mae: 1.3300 - mse: 4.1108 - val_loss: 5.9239 - val_mae: 1.7822 - val_mse: 5.9239\n",
            "Epoch 505/1000\n",
            "250/250 [==============================] - 0s 84us/sample - loss: 4.3162 - mae: 1.3383 - mse: 4.3162 - val_loss: 5.9998 - val_mae: 1.7220 - val_mse: 5.9998\n",
            "Epoch 506/1000\n",
            "250/250 [==============================] - 0s 100us/sample - loss: 4.1521 - mae: 1.3193 - mse: 4.1521 - val_loss: 6.2553 - val_mae: 1.7643 - val_mse: 6.2553\n",
            "Epoch 507/1000\n",
            "250/250 [==============================] - 0s 86us/sample - loss: 4.0441 - mae: 1.3460 - mse: 4.0441 - val_loss: 5.7640 - val_mae: 1.6878 - val_mse: 5.7640\n",
            "Epoch 508/1000\n",
            "250/250 [==============================] - 0s 77us/sample - loss: 4.1241 - mae: 1.3217 - mse: 4.1241 - val_loss: 5.6369 - val_mae: 1.6924 - val_mse: 5.6369\n",
            "Epoch 509/1000\n",
            "250/250 [==============================] - 0s 72us/sample - loss: 4.3496 - mae: 1.3734 - mse: 4.3496 - val_loss: 5.7030 - val_mae: 1.6771 - val_mse: 5.7030\n",
            "Epoch 510/1000\n",
            "250/250 [==============================] - 0s 89us/sample - loss: 4.0373 - mae: 1.3020 - mse: 4.0373 - val_loss: 5.8919 - val_mae: 1.7344 - val_mse: 5.8919\n",
            "Epoch 511/1000\n",
            "250/250 [==============================] - 0s 78us/sample - loss: 4.0950 - mae: 1.3534 - mse: 4.0950 - val_loss: 5.9288 - val_mae: 1.7420 - val_mse: 5.9288\n",
            "Epoch 512/1000\n",
            "250/250 [==============================] - 0s 87us/sample - loss: 4.3783 - mae: 1.3813 - mse: 4.3783 - val_loss: 6.3218 - val_mae: 1.8076 - val_mse: 6.3218\n",
            "Epoch 513/1000\n",
            "250/250 [==============================] - 0s 96us/sample - loss: 4.1149 - mae: 1.3105 - mse: 4.1149 - val_loss: 5.9200 - val_mae: 1.7145 - val_mse: 5.9200\n",
            "Epoch 514/1000\n",
            "250/250 [==============================] - 0s 92us/sample - loss: 4.1795 - mae: 1.3549 - mse: 4.1795 - val_loss: 5.9085 - val_mae: 1.7375 - val_mse: 5.9085\n",
            "Epoch 515/1000\n",
            "250/250 [==============================] - 0s 104us/sample - loss: 4.1737 - mae: 1.3083 - mse: 4.1737 - val_loss: 5.9506 - val_mae: 1.7619 - val_mse: 5.9506\n",
            "Epoch 516/1000\n",
            "250/250 [==============================] - 0s 99us/sample - loss: 4.3544 - mae: 1.3503 - mse: 4.3544 - val_loss: 6.1680 - val_mae: 1.7417 - val_mse: 6.1680\n",
            "Epoch 517/1000\n",
            "250/250 [==============================] - 0s 99us/sample - loss: 4.0077 - mae: 1.2805 - mse: 4.0077 - val_loss: 5.8718 - val_mae: 1.7068 - val_mse: 5.8718\n",
            "Epoch 518/1000\n",
            "250/250 [==============================] - 0s 99us/sample - loss: 4.2161 - mae: 1.3381 - mse: 4.2161 - val_loss: 6.4839 - val_mae: 1.8082 - val_mse: 6.4839\n",
            "Epoch 519/1000\n",
            "250/250 [==============================] - 0s 96us/sample - loss: 4.2191 - mae: 1.3663 - mse: 4.2191 - val_loss: 5.9882 - val_mae: 1.7391 - val_mse: 5.9882\n",
            "Epoch 520/1000\n",
            "250/250 [==============================] - 0s 102us/sample - loss: 4.2775 - mae: 1.3950 - mse: 4.2775 - val_loss: 6.9874 - val_mae: 1.8506 - val_mse: 6.9874\n",
            "Epoch 521/1000\n",
            "250/250 [==============================] - 0s 131us/sample - loss: 4.0629 - mae: 1.3060 - mse: 4.0629 - val_loss: 5.7619 - val_mae: 1.7080 - val_mse: 5.7619\n",
            "Epoch 522/1000\n",
            "250/250 [==============================] - 0s 80us/sample - loss: 4.0517 - mae: 1.3121 - mse: 4.0517 - val_loss: 5.6720 - val_mae: 1.7204 - val_mse: 5.6720\n",
            "Epoch 523/1000\n",
            "250/250 [==============================] - 0s 97us/sample - loss: 4.1943 - mae: 1.3210 - mse: 4.1943 - val_loss: 5.7807 - val_mae: 1.7008 - val_mse: 5.7807\n",
            "Epoch 524/1000\n",
            "250/250 [==============================] - 0s 101us/sample - loss: 4.0322 - mae: 1.2952 - mse: 4.0322 - val_loss: 5.7829 - val_mae: 1.7218 - val_mse: 5.7829\n",
            "Epoch 525/1000\n",
            "250/250 [==============================] - 0s 96us/sample - loss: 3.9932 - mae: 1.3014 - mse: 3.9932 - val_loss: 5.7850 - val_mae: 1.6915 - val_mse: 5.7850\n",
            "Epoch 526/1000\n",
            "250/250 [==============================] - 0s 81us/sample - loss: 4.1647 - mae: 1.3541 - mse: 4.1647 - val_loss: 6.0482 - val_mae: 1.7855 - val_mse: 6.0482\n",
            "Epoch 527/1000\n",
            "250/250 [==============================] - 0s 89us/sample - loss: 4.0786 - mae: 1.3370 - mse: 4.0786 - val_loss: 6.3920 - val_mae: 1.7482 - val_mse: 6.3920\n",
            "Epoch 528/1000\n",
            "250/250 [==============================] - 0s 91us/sample - loss: 4.2918 - mae: 1.3510 - mse: 4.2918 - val_loss: 5.9518 - val_mae: 1.7179 - val_mse: 5.9518\n",
            "Epoch 529/1000\n",
            "250/250 [==============================] - 0s 123us/sample - loss: 4.0033 - mae: 1.3266 - mse: 4.0033 - val_loss: 6.1260 - val_mae: 1.7315 - val_mse: 6.1260\n",
            "Epoch 530/1000\n",
            "250/250 [==============================] - 0s 89us/sample - loss: 4.0537 - mae: 1.3252 - mse: 4.0537 - val_loss: 6.7756 - val_mae: 1.8675 - val_mse: 6.7756\n",
            "Epoch 531/1000\n",
            "250/250 [==============================] - 0s 85us/sample - loss: 4.0733 - mae: 1.3163 - mse: 4.0733 - val_loss: 5.9735 - val_mae: 1.7611 - val_mse: 5.9735\n",
            "Epoch 532/1000\n",
            "250/250 [==============================] - 0s 95us/sample - loss: 4.0830 - mae: 1.3435 - mse: 4.0830 - val_loss: 6.0365 - val_mae: 1.7195 - val_mse: 6.0365\n",
            "Epoch 533/1000\n",
            "250/250 [==============================] - 0s 110us/sample - loss: 4.1416 - mae: 1.3258 - mse: 4.1416 - val_loss: 6.2784 - val_mae: 1.7758 - val_mse: 6.2784\n",
            "Epoch 534/1000\n",
            "250/250 [==============================] - 0s 124us/sample - loss: 4.0889 - mae: 1.3233 - mse: 4.0889 - val_loss: 6.5467 - val_mae: 1.7960 - val_mse: 6.5467\n",
            "Epoch 535/1000\n",
            "250/250 [==============================] - 0s 88us/sample - loss: 3.9380 - mae: 1.2970 - mse: 3.9380 - val_loss: 6.3401 - val_mae: 1.7666 - val_mse: 6.3401\n",
            "Epoch 536/1000\n",
            "250/250 [==============================] - 0s 86us/sample - loss: 4.1901 - mae: 1.3150 - mse: 4.1901 - val_loss: 5.7197 - val_mae: 1.7100 - val_mse: 5.7197\n",
            "Epoch 537/1000\n",
            "250/250 [==============================] - 0s 113us/sample - loss: 4.1411 - mae: 1.3190 - mse: 4.1411 - val_loss: 5.6960 - val_mae: 1.7438 - val_mse: 5.6960\n",
            "Epoch 538/1000\n",
            "250/250 [==============================] - 0s 102us/sample - loss: 4.0451 - mae: 1.2890 - mse: 4.0451 - val_loss: 5.6343 - val_mae: 1.6837 - val_mse: 5.6343\n",
            "Epoch 539/1000\n",
            "250/250 [==============================] - 0s 89us/sample - loss: 4.3095 - mae: 1.3664 - mse: 4.3095 - val_loss: 6.4864 - val_mae: 1.8014 - val_mse: 6.4864\n",
            "Epoch 540/1000\n",
            "250/250 [==============================] - 0s 97us/sample - loss: 3.9259 - mae: 1.2729 - mse: 3.9259 - val_loss: 5.7921 - val_mae: 1.7052 - val_mse: 5.7921\n",
            "Epoch 541/1000\n",
            "250/250 [==============================] - 0s 95us/sample - loss: 4.0342 - mae: 1.3385 - mse: 4.0342 - val_loss: 6.2008 - val_mae: 1.7379 - val_mse: 6.2008\n",
            "Epoch 542/1000\n",
            "250/250 [==============================] - 0s 86us/sample - loss: 4.1739 - mae: 1.3109 - mse: 4.1739 - val_loss: 5.8143 - val_mae: 1.7306 - val_mse: 5.8143\n",
            "Epoch 543/1000\n",
            "250/250 [==============================] - 0s 83us/sample - loss: 4.0385 - mae: 1.3019 - mse: 4.0385 - val_loss: 5.5602 - val_mae: 1.7143 - val_mse: 5.5602\n",
            "Epoch 544/1000\n",
            "250/250 [==============================] - 0s 85us/sample - loss: 3.9369 - mae: 1.3122 - mse: 3.9369 - val_loss: 5.6887 - val_mae: 1.7009 - val_mse: 5.6887\n",
            "Epoch 545/1000\n",
            "250/250 [==============================] - 0s 81us/sample - loss: 3.9815 - mae: 1.3167 - mse: 3.9815 - val_loss: 6.2588 - val_mae: 1.7561 - val_mse: 6.2588\n",
            "Epoch 546/1000\n",
            "250/250 [==============================] - 0s 82us/sample - loss: 3.9370 - mae: 1.2682 - mse: 3.9370 - val_loss: 6.9027 - val_mae: 1.8576 - val_mse: 6.9027\n",
            "Epoch 547/1000\n",
            "250/250 [==============================] - 0s 87us/sample - loss: 4.0464 - mae: 1.3302 - mse: 4.0464 - val_loss: 6.4880 - val_mae: 1.7859 - val_mse: 6.4880\n",
            "Epoch 548/1000\n",
            "250/250 [==============================] - 0s 89us/sample - loss: 3.8737 - mae: 1.2914 - mse: 3.8737 - val_loss: 8.5378 - val_mae: 2.0556 - val_mse: 8.5378\n",
            "Epoch 549/1000\n",
            "250/250 [==============================] - 0s 109us/sample - loss: 4.2651 - mae: 1.3772 - mse: 4.2651 - val_loss: 6.2820 - val_mae: 1.7845 - val_mse: 6.2820\n",
            "Epoch 550/1000\n",
            "250/250 [==============================] - 0s 104us/sample - loss: 3.9039 - mae: 1.3055 - mse: 3.9039 - val_loss: 6.9287 - val_mae: 1.8625 - val_mse: 6.9287\n",
            "Epoch 551/1000\n",
            "250/250 [==============================] - 0s 88us/sample - loss: 3.9602 - mae: 1.3184 - mse: 3.9602 - val_loss: 5.7665 - val_mae: 1.7271 - val_mse: 5.7665\n",
            "Epoch 552/1000\n",
            "250/250 [==============================] - 0s 112us/sample - loss: 4.0645 - mae: 1.3196 - mse: 4.0645 - val_loss: 6.8158 - val_mae: 1.8196 - val_mse: 6.8158\n",
            "Epoch 553/1000\n",
            "250/250 [==============================] - 0s 102us/sample - loss: 4.2385 - mae: 1.3201 - mse: 4.2385 - val_loss: 6.1719 - val_mae: 1.7689 - val_mse: 6.1719\n",
            "Epoch 554/1000\n",
            "250/250 [==============================] - 0s 88us/sample - loss: 3.9545 - mae: 1.2617 - mse: 3.9545 - val_loss: 5.5336 - val_mae: 1.6948 - val_mse: 5.5335\n",
            "Epoch 555/1000\n",
            "250/250 [==============================] - 0s 96us/sample - loss: 3.9532 - mae: 1.3346 - mse: 3.9532 - val_loss: 6.0755 - val_mae: 1.7527 - val_mse: 6.0755\n",
            "Epoch 556/1000\n",
            "250/250 [==============================] - 0s 101us/sample - loss: 4.1789 - mae: 1.3456 - mse: 4.1789 - val_loss: 6.5046 - val_mae: 1.7985 - val_mse: 6.5046\n",
            "Epoch 557/1000\n",
            "250/250 [==============================] - 0s 86us/sample - loss: 3.8558 - mae: 1.2757 - mse: 3.8558 - val_loss: 6.5904 - val_mae: 1.8074 - val_mse: 6.5904\n",
            "Epoch 558/1000\n",
            "250/250 [==============================] - 0s 79us/sample - loss: 3.9524 - mae: 1.2783 - mse: 3.9524 - val_loss: 5.7770 - val_mae: 1.7106 - val_mse: 5.7770\n",
            "Epoch 559/1000\n",
            "250/250 [==============================] - 0s 89us/sample - loss: 3.9414 - mae: 1.2886 - mse: 3.9414 - val_loss: 7.0479 - val_mae: 1.8822 - val_mse: 7.0479\n",
            "Epoch 560/1000\n",
            "250/250 [==============================] - 0s 83us/sample - loss: 4.1935 - mae: 1.3200 - mse: 4.1935 - val_loss: 6.8563 - val_mae: 1.8356 - val_mse: 6.8563\n",
            "Epoch 561/1000\n",
            "250/250 [==============================] - 0s 82us/sample - loss: 3.9489 - mae: 1.2940 - mse: 3.9489 - val_loss: 7.0389 - val_mae: 1.8425 - val_mse: 7.0389\n",
            "Epoch 562/1000\n",
            "250/250 [==============================] - 0s 80us/sample - loss: 3.8175 - mae: 1.2694 - mse: 3.8175 - val_loss: 5.7846 - val_mae: 1.6974 - val_mse: 5.7846\n",
            "Epoch 563/1000\n",
            "250/250 [==============================] - 0s 84us/sample - loss: 3.9536 - mae: 1.3077 - mse: 3.9536 - val_loss: 7.1153 - val_mae: 1.8677 - val_mse: 7.1153\n",
            "Epoch 564/1000\n",
            "250/250 [==============================] - 0s 100us/sample - loss: 3.9915 - mae: 1.3148 - mse: 3.9915 - val_loss: 5.4554 - val_mae: 1.7478 - val_mse: 5.4554\n",
            "Epoch 565/1000\n",
            "250/250 [==============================] - 0s 83us/sample - loss: 3.9440 - mae: 1.2609 - mse: 3.9440 - val_loss: 6.1664 - val_mae: 1.7394 - val_mse: 6.1664\n",
            "Epoch 566/1000\n",
            "250/250 [==============================] - 0s 87us/sample - loss: 4.0105 - mae: 1.3033 - mse: 4.0105 - val_loss: 6.4362 - val_mae: 1.7709 - val_mse: 6.4362\n",
            "Epoch 567/1000\n",
            "250/250 [==============================] - 0s 86us/sample - loss: 3.8176 - mae: 1.2880 - mse: 3.8176 - val_loss: 7.7189 - val_mae: 1.9523 - val_mse: 7.7189\n",
            "Epoch 568/1000\n",
            "250/250 [==============================] - 0s 92us/sample - loss: 3.9989 - mae: 1.2847 - mse: 3.9989 - val_loss: 5.8058 - val_mae: 1.7496 - val_mse: 5.8058\n",
            "Epoch 569/1000\n",
            "250/250 [==============================] - 0s 81us/sample - loss: 3.9052 - mae: 1.3045 - mse: 3.9052 - val_loss: 5.9321 - val_mae: 1.7003 - val_mse: 5.9321\n",
            "Epoch 570/1000\n",
            "250/250 [==============================] - 0s 81us/sample - loss: 4.2239 - mae: 1.3748 - mse: 4.2239 - val_loss: 6.5642 - val_mae: 1.7762 - val_mse: 6.5642\n",
            "Epoch 571/1000\n",
            "250/250 [==============================] - 0s 120us/sample - loss: 4.0284 - mae: 1.3005 - mse: 4.0284 - val_loss: 5.9224 - val_mae: 1.7059 - val_mse: 5.9224\n",
            "Epoch 572/1000\n",
            "250/250 [==============================] - 0s 88us/sample - loss: 4.0172 - mae: 1.3117 - mse: 4.0172 - val_loss: 6.4746 - val_mae: 1.7852 - val_mse: 6.4746\n",
            "Epoch 573/1000\n",
            "250/250 [==============================] - 0s 91us/sample - loss: 3.9905 - mae: 1.3001 - mse: 3.9905 - val_loss: 6.2218 - val_mae: 1.7401 - val_mse: 6.2218\n",
            "Epoch 574/1000\n",
            "250/250 [==============================] - 0s 93us/sample - loss: 3.9445 - mae: 1.2685 - mse: 3.9445 - val_loss: 5.7026 - val_mae: 1.7381 - val_mse: 5.7026\n",
            "Epoch 575/1000\n",
            "250/250 [==============================] - 0s 113us/sample - loss: 3.9569 - mae: 1.2884 - mse: 3.9569 - val_loss: 6.3856 - val_mae: 1.7595 - val_mse: 6.3856\n",
            "Epoch 576/1000\n",
            "250/250 [==============================] - 0s 97us/sample - loss: 3.8517 - mae: 1.3065 - mse: 3.8517 - val_loss: 6.4663 - val_mae: 1.8341 - val_mse: 6.4663\n",
            "Epoch 577/1000\n",
            "250/250 [==============================] - 0s 91us/sample - loss: 4.1635 - mae: 1.3461 - mse: 4.1635 - val_loss: 5.8033 - val_mae: 1.7076 - val_mse: 5.8033\n",
            "Epoch 578/1000\n",
            "250/250 [==============================] - 0s 99us/sample - loss: 4.0160 - mae: 1.2838 - mse: 4.0160 - val_loss: 6.0315 - val_mae: 1.7505 - val_mse: 6.0315\n",
            "Epoch 579/1000\n",
            "250/250 [==============================] - 0s 113us/sample - loss: 3.8342 - mae: 1.2726 - mse: 3.8342 - val_loss: 6.5524 - val_mae: 1.7841 - val_mse: 6.5524\n",
            "Epoch 580/1000\n",
            "250/250 [==============================] - 0s 80us/sample - loss: 3.8357 - mae: 1.2929 - mse: 3.8357 - val_loss: 8.0324 - val_mae: 1.9889 - val_mse: 8.0324\n",
            "Epoch 581/1000\n",
            "250/250 [==============================] - 0s 98us/sample - loss: 3.8772 - mae: 1.3047 - mse: 3.8772 - val_loss: 6.3813 - val_mae: 1.7707 - val_mse: 6.3813\n",
            "Epoch 582/1000\n",
            "250/250 [==============================] - 0s 86us/sample - loss: 3.8019 - mae: 1.2635 - mse: 3.8019 - val_loss: 6.6381 - val_mae: 1.7829 - val_mse: 6.6381\n",
            "Epoch 583/1000\n",
            "250/250 [==============================] - 0s 94us/sample - loss: 4.0866 - mae: 1.3195 - mse: 4.0866 - val_loss: 5.6023 - val_mae: 1.7074 - val_mse: 5.6023\n",
            "Epoch 584/1000\n",
            "250/250 [==============================] - 0s 95us/sample - loss: 3.9973 - mae: 1.2940 - mse: 3.9973 - val_loss: 6.8103 - val_mae: 1.8117 - val_mse: 6.8103\n",
            "Epoch 585/1000\n",
            "250/250 [==============================] - 0s 79us/sample - loss: 3.8436 - mae: 1.2745 - mse: 3.8436 - val_loss: 7.0114 - val_mae: 1.8792 - val_mse: 7.0114\n",
            "Epoch 586/1000\n",
            "250/250 [==============================] - 0s 91us/sample - loss: 3.9555 - mae: 1.2951 - mse: 3.9555 - val_loss: 7.1924 - val_mae: 1.8559 - val_mse: 7.1924\n",
            "Epoch 587/1000\n",
            "250/250 [==============================] - 0s 84us/sample - loss: 3.8191 - mae: 1.2237 - mse: 3.8191 - val_loss: 6.7780 - val_mae: 1.7993 - val_mse: 6.7780\n",
            "Epoch 588/1000\n",
            "250/250 [==============================] - 0s 104us/sample - loss: 3.9502 - mae: 1.3207 - mse: 3.9502 - val_loss: 6.6634 - val_mae: 1.8067 - val_mse: 6.6634\n",
            "Epoch 589/1000\n",
            "250/250 [==============================] - 0s 100us/sample - loss: 4.0458 - mae: 1.2897 - mse: 4.0458 - val_loss: 6.2731 - val_mae: 1.7312 - val_mse: 6.2731\n",
            "Epoch 590/1000\n",
            "250/250 [==============================] - 0s 106us/sample - loss: 3.8091 - mae: 1.2896 - mse: 3.8091 - val_loss: 6.6362 - val_mae: 1.8444 - val_mse: 6.6362\n",
            "Epoch 591/1000\n",
            "250/250 [==============================] - 0s 100us/sample - loss: 3.9143 - mae: 1.2925 - mse: 3.9143 - val_loss: 6.1722 - val_mae: 1.7490 - val_mse: 6.1722\n",
            "Epoch 592/1000\n",
            "250/250 [==============================] - 0s 139us/sample - loss: 3.8096 - mae: 1.2888 - mse: 3.8096 - val_loss: 5.5805 - val_mae: 1.7104 - val_mse: 5.5805\n",
            "Epoch 593/1000\n",
            "250/250 [==============================] - 0s 83us/sample - loss: 3.7893 - mae: 1.2742 - mse: 3.7893 - val_loss: 6.4523 - val_mae: 1.7912 - val_mse: 6.4523\n",
            "Epoch 594/1000\n",
            "250/250 [==============================] - 0s 84us/sample - loss: 3.8484 - mae: 1.2687 - mse: 3.8484 - val_loss: 6.1568 - val_mae: 1.7615 - val_mse: 6.1568\n",
            "Epoch 595/1000\n",
            "250/250 [==============================] - 0s 82us/sample - loss: 4.0483 - mae: 1.3084 - mse: 4.0483 - val_loss: 5.9969 - val_mae: 1.7140 - val_mse: 5.9969\n",
            "Epoch 596/1000\n",
            "250/250 [==============================] - 0s 111us/sample - loss: 3.7807 - mae: 1.2310 - mse: 3.7807 - val_loss: 6.2042 - val_mae: 1.7508 - val_mse: 6.2042\n",
            "Epoch 597/1000\n",
            "250/250 [==============================] - 0s 94us/sample - loss: 3.9255 - mae: 1.2576 - mse: 3.9255 - val_loss: 5.5429 - val_mae: 1.7519 - val_mse: 5.5429\n",
            "Epoch 598/1000\n",
            "250/250 [==============================] - 0s 91us/sample - loss: 3.9757 - mae: 1.3180 - mse: 3.9757 - val_loss: 6.6709 - val_mae: 1.8079 - val_mse: 6.6709\n",
            "Epoch 599/1000\n",
            "250/250 [==============================] - 0s 86us/sample - loss: 3.7646 - mae: 1.2763 - mse: 3.7646 - val_loss: 6.0383 - val_mae: 1.7773 - val_mse: 6.0383\n",
            "Epoch 600/1000\n",
            "250/250 [==============================] - 0s 93us/sample - loss: 3.9491 - mae: 1.2991 - mse: 3.9491 - val_loss: 7.5874 - val_mae: 1.9193 - val_mse: 7.5874\n",
            "Epoch 601/1000\n",
            "250/250 [==============================] - 0s 111us/sample - loss: 3.8292 - mae: 1.2935 - mse: 3.8292 - val_loss: 6.8168 - val_mae: 1.8166 - val_mse: 6.8168\n",
            "Epoch 602/1000\n",
            "250/250 [==============================] - 0s 109us/sample - loss: 3.9744 - mae: 1.2939 - mse: 3.9744 - val_loss: 7.2975 - val_mae: 1.8891 - val_mse: 7.2975\n",
            "Epoch 603/1000\n",
            "250/250 [==============================] - 0s 98us/sample - loss: 3.8133 - mae: 1.2636 - mse: 3.8133 - val_loss: 6.8716 - val_mae: 1.8239 - val_mse: 6.8716\n",
            "Epoch 604/1000\n",
            "250/250 [==============================] - 0s 86us/sample - loss: 3.8060 - mae: 1.2660 - mse: 3.8060 - val_loss: 6.3081 - val_mae: 1.8044 - val_mse: 6.3081\n",
            "Epoch 605/1000\n",
            "250/250 [==============================] - 0s 90us/sample - loss: 3.7862 - mae: 1.2748 - mse: 3.7862 - val_loss: 5.9569 - val_mae: 1.7059 - val_mse: 5.9569\n",
            "Epoch 606/1000\n",
            "250/250 [==============================] - 0s 91us/sample - loss: 3.8759 - mae: 1.2590 - mse: 3.8759 - val_loss: 6.2105 - val_mae: 1.7471 - val_mse: 6.2105\n",
            "Epoch 607/1000\n",
            "250/250 [==============================] - 0s 104us/sample - loss: 3.7688 - mae: 1.2621 - mse: 3.7688 - val_loss: 5.9733 - val_mae: 1.7441 - val_mse: 5.9733\n",
            "Epoch 608/1000\n",
            "250/250 [==============================] - 0s 81us/sample - loss: 3.9047 - mae: 1.2697 - mse: 3.9047 - val_loss: 6.1877 - val_mae: 1.7789 - val_mse: 6.1877\n",
            "Epoch 609/1000\n",
            "250/250 [==============================] - 0s 98us/sample - loss: 3.8322 - mae: 1.3113 - mse: 3.8322 - val_loss: 6.1435 - val_mae: 1.7695 - val_mse: 6.1435\n",
            "Epoch 610/1000\n",
            "250/250 [==============================] - 0s 111us/sample - loss: 3.9604 - mae: 1.2940 - mse: 3.9604 - val_loss: 6.6590 - val_mae: 1.7819 - val_mse: 6.6590\n",
            "Epoch 611/1000\n",
            "250/250 [==============================] - 0s 98us/sample - loss: 3.7768 - mae: 1.2645 - mse: 3.7768 - val_loss: 5.9865 - val_mae: 1.7142 - val_mse: 5.9865\n",
            "Epoch 612/1000\n",
            "250/250 [==============================] - 0s 103us/sample - loss: 3.8779 - mae: 1.2910 - mse: 3.8779 - val_loss: 8.4663 - val_mae: 2.0555 - val_mse: 8.4663\n",
            "Epoch 613/1000\n",
            "250/250 [==============================] - 0s 99us/sample - loss: 3.7371 - mae: 1.2783 - mse: 3.7371 - val_loss: 6.1779 - val_mae: 1.7316 - val_mse: 6.1779\n",
            "Epoch 614/1000\n",
            "250/250 [==============================] - 0s 140us/sample - loss: 3.8449 - mae: 1.2690 - mse: 3.8449 - val_loss: 6.2338 - val_mae: 1.7364 - val_mse: 6.2338\n",
            "Epoch 615/1000\n",
            "250/250 [==============================] - 0s 84us/sample - loss: 3.7646 - mae: 1.2518 - mse: 3.7646 - val_loss: 5.6671 - val_mae: 1.7345 - val_mse: 5.6671\n",
            "Epoch 616/1000\n",
            "250/250 [==============================] - 0s 102us/sample - loss: 3.7921 - mae: 1.2650 - mse: 3.7921 - val_loss: 7.6900 - val_mae: 1.9329 - val_mse: 7.6900\n",
            "Epoch 617/1000\n",
            "250/250 [==============================] - 0s 97us/sample - loss: 3.7429 - mae: 1.2270 - mse: 3.7429 - val_loss: 6.2877 - val_mae: 1.7477 - val_mse: 6.2877\n",
            "Epoch 618/1000\n",
            "250/250 [==============================] - 0s 100us/sample - loss: 3.8035 - mae: 1.2552 - mse: 3.8035 - val_loss: 6.4049 - val_mae: 1.7756 - val_mse: 6.4049\n",
            "Epoch 619/1000\n",
            "250/250 [==============================] - 0s 90us/sample - loss: 3.8678 - mae: 1.3024 - mse: 3.8678 - val_loss: 6.6565 - val_mae: 1.7877 - val_mse: 6.6565\n",
            "Epoch 620/1000\n",
            "250/250 [==============================] - 0s 79us/sample - loss: 3.7399 - mae: 1.2614 - mse: 3.7399 - val_loss: 6.7232 - val_mae: 1.8230 - val_mse: 6.7232\n",
            "Epoch 621/1000\n",
            "250/250 [==============================] - 0s 108us/sample - loss: 3.9548 - mae: 1.2820 - mse: 3.9548 - val_loss: 6.6110 - val_mae: 1.7761 - val_mse: 6.6110\n",
            "Epoch 622/1000\n",
            "250/250 [==============================] - 0s 110us/sample - loss: 3.6667 - mae: 1.2466 - mse: 3.6667 - val_loss: 5.7761 - val_mae: 1.8686 - val_mse: 5.7761\n",
            "Epoch 623/1000\n",
            "250/250 [==============================] - 0s 99us/sample - loss: 3.8972 - mae: 1.2821 - mse: 3.8972 - val_loss: 5.8734 - val_mae: 1.7043 - val_mse: 5.8734\n",
            "Epoch 624/1000\n",
            "250/250 [==============================] - 0s 108us/sample - loss: 3.7287 - mae: 1.2286 - mse: 3.7287 - val_loss: 6.7254 - val_mae: 1.8653 - val_mse: 6.7254\n",
            "Epoch 625/1000\n",
            "250/250 [==============================] - 0s 87us/sample - loss: 3.7870 - mae: 1.2951 - mse: 3.7870 - val_loss: 6.3487 - val_mae: 1.7742 - val_mse: 6.3487\n",
            "Epoch 626/1000\n",
            "250/250 [==============================] - 0s 97us/sample - loss: 3.7179 - mae: 1.2291 - mse: 3.7179 - val_loss: 5.7202 - val_mae: 1.7181 - val_mse: 5.7202\n",
            "Epoch 627/1000\n",
            "250/250 [==============================] - 0s 89us/sample - loss: 3.7142 - mae: 1.2622 - mse: 3.7142 - val_loss: 6.7336 - val_mae: 1.8203 - val_mse: 6.7336\n",
            "Epoch 628/1000\n",
            "250/250 [==============================] - 0s 111us/sample - loss: 3.6296 - mae: 1.2224 - mse: 3.6296 - val_loss: 5.9138 - val_mae: 1.7253 - val_mse: 5.9138\n",
            "Epoch 629/1000\n",
            "250/250 [==============================] - 0s 93us/sample - loss: 3.7774 - mae: 1.2763 - mse: 3.7774 - val_loss: 7.6491 - val_mae: 1.9267 - val_mse: 7.6491\n",
            "Epoch 630/1000\n",
            "250/250 [==============================] - 0s 93us/sample - loss: 3.9170 - mae: 1.2951 - mse: 3.9170 - val_loss: 6.2144 - val_mae: 1.7366 - val_mse: 6.2144\n",
            "Epoch 631/1000\n",
            "250/250 [==============================] - 0s 97us/sample - loss: 3.7316 - mae: 1.2344 - mse: 3.7316 - val_loss: 7.9034 - val_mae: 1.9616 - val_mse: 7.9034\n",
            "Epoch 632/1000\n",
            "250/250 [==============================] - 0s 103us/sample - loss: 3.6796 - mae: 1.2205 - mse: 3.6796 - val_loss: 6.4322 - val_mae: 1.8112 - val_mse: 6.4322\n",
            "Epoch 633/1000\n",
            "250/250 [==============================] - 0s 87us/sample - loss: 3.5865 - mae: 1.2197 - mse: 3.5865 - val_loss: 6.3583 - val_mae: 1.7534 - val_mse: 6.3583\n",
            "Epoch 634/1000\n",
            "250/250 [==============================] - 0s 86us/sample - loss: 3.9567 - mae: 1.3054 - mse: 3.9567 - val_loss: 6.6870 - val_mae: 1.8028 - val_mse: 6.6870\n",
            "Epoch 635/1000\n",
            "250/250 [==============================] - 0s 86us/sample - loss: 3.7226 - mae: 1.2356 - mse: 3.7226 - val_loss: 6.4232 - val_mae: 1.7677 - val_mse: 6.4232\n",
            "Epoch 636/1000\n",
            "250/250 [==============================] - 0s 101us/sample - loss: 3.9439 - mae: 1.2881 - mse: 3.9439 - val_loss: 6.9218 - val_mae: 1.8326 - val_mse: 6.9218\n",
            "Epoch 637/1000\n",
            "250/250 [==============================] - 0s 99us/sample - loss: 3.6046 - mae: 1.2115 - mse: 3.6046 - val_loss: 6.2719 - val_mae: 1.7495 - val_mse: 6.2719\n",
            "Epoch 638/1000\n",
            "250/250 [==============================] - 0s 99us/sample - loss: 3.6986 - mae: 1.2098 - mse: 3.6986 - val_loss: 6.6653 - val_mae: 1.7888 - val_mse: 6.6653\n",
            "Epoch 639/1000\n",
            "250/250 [==============================] - 0s 93us/sample - loss: 3.6300 - mae: 1.2365 - mse: 3.6300 - val_loss: 6.9460 - val_mae: 1.8167 - val_mse: 6.9460\n",
            "Epoch 640/1000\n",
            "250/250 [==============================] - 0s 101us/sample - loss: 3.7258 - mae: 1.2196 - mse: 3.7258 - val_loss: 7.0012 - val_mae: 1.8365 - val_mse: 7.0012\n",
            "Epoch 641/1000\n",
            "250/250 [==============================] - 0s 116us/sample - loss: 3.8273 - mae: 1.2465 - mse: 3.8273 - val_loss: 5.7660 - val_mae: 1.7702 - val_mse: 5.7660\n",
            "Epoch 642/1000\n",
            "250/250 [==============================] - 0s 78us/sample - loss: 4.0006 - mae: 1.2926 - mse: 4.0006 - val_loss: 6.2900 - val_mae: 1.7455 - val_mse: 6.2900\n",
            "Epoch 643/1000\n",
            "250/250 [==============================] - 0s 83us/sample - loss: 3.6700 - mae: 1.2159 - mse: 3.6700 - val_loss: 6.0533 - val_mae: 1.7366 - val_mse: 6.0533\n",
            "Epoch 644/1000\n",
            "250/250 [==============================] - 0s 124us/sample - loss: 3.7279 - mae: 1.2337 - mse: 3.7279 - val_loss: 6.5697 - val_mae: 1.8171 - val_mse: 6.5697\n",
            "Epoch 645/1000\n",
            "250/250 [==============================] - 0s 112us/sample - loss: 3.8045 - mae: 1.2375 - mse: 3.8045 - val_loss: 5.7983 - val_mae: 1.7275 - val_mse: 5.7983\n",
            "Epoch 646/1000\n",
            "250/250 [==============================] - 0s 86us/sample - loss: 3.5617 - mae: 1.1919 - mse: 3.5617 - val_loss: 5.6937 - val_mae: 1.7607 - val_mse: 5.6937\n",
            "Epoch 647/1000\n",
            "250/250 [==============================] - 0s 100us/sample - loss: 3.7176 - mae: 1.2426 - mse: 3.7176 - val_loss: 6.1599 - val_mae: 1.7293 - val_mse: 6.1599\n",
            "Epoch 648/1000\n",
            "250/250 [==============================] - 0s 93us/sample - loss: 3.7083 - mae: 1.2499 - mse: 3.7083 - val_loss: 6.4961 - val_mae: 1.7659 - val_mse: 6.4961\n",
            "Epoch 649/1000\n",
            "250/250 [==============================] - 0s 95us/sample - loss: 3.8605 - mae: 1.2606 - mse: 3.8605 - val_loss: 5.5345 - val_mae: 1.7580 - val_mse: 5.5345\n",
            "Epoch 650/1000\n",
            "250/250 [==============================] - 0s 97us/sample - loss: 3.6825 - mae: 1.2650 - mse: 3.6825 - val_loss: 6.6150 - val_mae: 1.7794 - val_mse: 6.6150\n",
            "Epoch 651/1000\n",
            "250/250 [==============================] - 0s 123us/sample - loss: 3.8166 - mae: 1.2749 - mse: 3.8166 - val_loss: 5.9061 - val_mae: 1.7972 - val_mse: 5.9061\n",
            "Epoch 652/1000\n",
            "250/250 [==============================] - 0s 85us/sample - loss: 3.7777 - mae: 1.2432 - mse: 3.7777 - val_loss: 6.0447 - val_mae: 1.7466 - val_mse: 6.0447\n",
            "Epoch 653/1000\n",
            "250/250 [==============================] - 0s 94us/sample - loss: 3.5157 - mae: 1.1874 - mse: 3.5157 - val_loss: 5.8824 - val_mae: 1.7121 - val_mse: 5.8824\n",
            "Epoch 654/1000\n",
            "250/250 [==============================] - 0s 119us/sample - loss: 3.4294 - mae: 1.2242 - mse: 3.4294 - val_loss: 10.1421 - val_mae: 2.2866 - val_mse: 10.1421\n",
            "Epoch 655/1000\n",
            "250/250 [==============================] - 0s 98us/sample - loss: 3.8980 - mae: 1.3023 - mse: 3.8980 - val_loss: 6.3886 - val_mae: 1.7582 - val_mse: 6.3886\n",
            "Epoch 656/1000\n",
            "250/250 [==============================] - 0s 96us/sample - loss: 3.6412 - mae: 1.2235 - mse: 3.6412 - val_loss: 7.7640 - val_mae: 1.9609 - val_mse: 7.7640\n",
            "Epoch 657/1000\n",
            "250/250 [==============================] - 0s 100us/sample - loss: 3.6410 - mae: 1.2348 - mse: 3.6410 - val_loss: 5.8283 - val_mae: 1.7268 - val_mse: 5.8283\n",
            "Epoch 658/1000\n",
            "250/250 [==============================] - 0s 94us/sample - loss: 3.7808 - mae: 1.2598 - mse: 3.7808 - val_loss: 6.1942 - val_mae: 1.8021 - val_mse: 6.1942\n",
            "Epoch 659/1000\n",
            "250/250 [==============================] - 0s 119us/sample - loss: 3.9222 - mae: 1.2869 - mse: 3.9222 - val_loss: 7.0026 - val_mae: 1.8593 - val_mse: 7.0026\n",
            "Epoch 660/1000\n",
            "250/250 [==============================] - 0s 87us/sample - loss: 3.6236 - mae: 1.1824 - mse: 3.6236 - val_loss: 6.1891 - val_mae: 1.7873 - val_mse: 6.1891\n",
            "Epoch 661/1000\n",
            "250/250 [==============================] - 0s 106us/sample - loss: 3.6184 - mae: 1.1989 - mse: 3.6184 - val_loss: 6.6020 - val_mae: 1.8075 - val_mse: 6.6020\n",
            "Epoch 662/1000\n",
            "250/250 [==============================] - 0s 78us/sample - loss: 3.6517 - mae: 1.2609 - mse: 3.6517 - val_loss: 8.0266 - val_mae: 1.9765 - val_mse: 8.0266\n",
            "Epoch 663/1000\n",
            "250/250 [==============================] - 0s 104us/sample - loss: 3.6557 - mae: 1.2311 - mse: 3.6557 - val_loss: 5.5231 - val_mae: 1.8087 - val_mse: 5.5231\n",
            "Epoch 664/1000\n",
            "250/250 [==============================] - 0s 124us/sample - loss: 3.7558 - mae: 1.2626 - mse: 3.7558 - val_loss: 7.2706 - val_mae: 1.8801 - val_mse: 7.2706\n",
            "Epoch 665/1000\n",
            "250/250 [==============================] - 0s 109us/sample - loss: 3.7029 - mae: 1.2173 - mse: 3.7029 - val_loss: 6.4033 - val_mae: 1.7777 - val_mse: 6.4033\n",
            "Epoch 666/1000\n",
            "250/250 [==============================] - 0s 88us/sample - loss: 3.6715 - mae: 1.2410 - mse: 3.6715 - val_loss: 5.9469 - val_mae: 1.7712 - val_mse: 5.9469\n",
            "Epoch 667/1000\n",
            "250/250 [==============================] - 0s 101us/sample - loss: 3.6406 - mae: 1.2430 - mse: 3.6406 - val_loss: 6.7908 - val_mae: 1.8058 - val_mse: 6.7908\n",
            "Epoch 668/1000\n",
            "250/250 [==============================] - 0s 97us/sample - loss: 3.9417 - mae: 1.2503 - mse: 3.9417 - val_loss: 6.6545 - val_mae: 1.7996 - val_mse: 6.6545\n",
            "Epoch 669/1000\n",
            "250/250 [==============================] - 0s 104us/sample - loss: 3.4844 - mae: 1.1894 - mse: 3.4844 - val_loss: 6.6976 - val_mae: 1.8270 - val_mse: 6.6976\n",
            "Epoch 670/1000\n",
            "250/250 [==============================] - 0s 93us/sample - loss: 3.7589 - mae: 1.2242 - mse: 3.7589 - val_loss: 5.6105 - val_mae: 1.7084 - val_mse: 5.6105\n",
            "Epoch 671/1000\n",
            "250/250 [==============================] - 0s 98us/sample - loss: 3.6574 - mae: 1.2068 - mse: 3.6574 - val_loss: 6.7318 - val_mae: 1.7991 - val_mse: 6.7318\n",
            "Epoch 672/1000\n",
            "250/250 [==============================] - 0s 86us/sample - loss: 3.6405 - mae: 1.1977 - mse: 3.6405 - val_loss: 5.9471 - val_mae: 1.7447 - val_mse: 5.9471\n",
            "Epoch 673/1000\n",
            "250/250 [==============================] - 0s 112us/sample - loss: 3.6856 - mae: 1.2420 - mse: 3.6856 - val_loss: 6.9257 - val_mae: 1.8400 - val_mse: 6.9257\n",
            "Epoch 674/1000\n",
            "250/250 [==============================] - 0s 92us/sample - loss: 3.5877 - mae: 1.2163 - mse: 3.5877 - val_loss: 6.7746 - val_mae: 1.8149 - val_mse: 6.7746\n",
            "Epoch 675/1000\n",
            "250/250 [==============================] - 0s 119us/sample - loss: 3.6437 - mae: 1.2265 - mse: 3.6437 - val_loss: 6.0783 - val_mae: 1.7177 - val_mse: 6.0783\n",
            "Epoch 676/1000\n",
            "250/250 [==============================] - 0s 99us/sample - loss: 3.6541 - mae: 1.2281 - mse: 3.6541 - val_loss: 7.3524 - val_mae: 1.8906 - val_mse: 7.3524\n",
            "Epoch 677/1000\n",
            "250/250 [==============================] - 0s 105us/sample - loss: 3.4851 - mae: 1.2072 - mse: 3.4851 - val_loss: 7.7271 - val_mae: 1.9809 - val_mse: 7.7271\n",
            "Epoch 678/1000\n",
            "250/250 [==============================] - 0s 98us/sample - loss: 3.7246 - mae: 1.2307 - mse: 3.7246 - val_loss: 6.6995 - val_mae: 1.8233 - val_mse: 6.6995\n",
            "Epoch 679/1000\n",
            "250/250 [==============================] - 0s 98us/sample - loss: 3.5980 - mae: 1.2184 - mse: 3.5980 - val_loss: 6.2875 - val_mae: 1.7523 - val_mse: 6.2875\n",
            "Epoch 680/1000\n",
            "250/250 [==============================] - 0s 96us/sample - loss: 3.5365 - mae: 1.1824 - mse: 3.5365 - val_loss: 5.9440 - val_mae: 1.7129 - val_mse: 5.9440\n",
            "Epoch 681/1000\n",
            "250/250 [==============================] - 0s 110us/sample - loss: 3.7627 - mae: 1.2338 - mse: 3.7627 - val_loss: 6.2076 - val_mae: 1.7896 - val_mse: 6.2076\n",
            "Epoch 682/1000\n",
            "250/250 [==============================] - 0s 97us/sample - loss: 3.7101 - mae: 1.2451 - mse: 3.7101 - val_loss: 5.7769 - val_mae: 1.7586 - val_mse: 5.7769\n",
            "Epoch 683/1000\n",
            "250/250 [==============================] - 0s 100us/sample - loss: 3.5533 - mae: 1.2147 - mse: 3.5533 - val_loss: 7.1036 - val_mae: 1.8354 - val_mse: 7.1036\n",
            "Epoch 684/1000\n",
            "250/250 [==============================] - 0s 101us/sample - loss: 3.6644 - mae: 1.2232 - mse: 3.6644 - val_loss: 6.4370 - val_mae: 1.8031 - val_mse: 6.4370\n",
            "Epoch 685/1000\n",
            "250/250 [==============================] - 0s 132us/sample - loss: 3.4822 - mae: 1.1931 - mse: 3.4822 - val_loss: 6.3175 - val_mae: 1.7488 - val_mse: 6.3175\n",
            "Epoch 686/1000\n",
            "250/250 [==============================] - 0s 88us/sample - loss: 3.4808 - mae: 1.2152 - mse: 3.4808 - val_loss: 6.0599 - val_mae: 1.7253 - val_mse: 6.0599\n",
            "Epoch 687/1000\n",
            "250/250 [==============================] - 0s 107us/sample - loss: 3.6591 - mae: 1.2154 - mse: 3.6591 - val_loss: 6.3348 - val_mae: 1.7447 - val_mse: 6.3348\n",
            "Epoch 688/1000\n",
            "250/250 [==============================] - 0s 90us/sample - loss: 3.3958 - mae: 1.1503 - mse: 3.3958 - val_loss: 6.7164 - val_mae: 1.7998 - val_mse: 6.7164\n",
            "Epoch 689/1000\n",
            "250/250 [==============================] - 0s 92us/sample - loss: 3.7934 - mae: 1.2714 - mse: 3.7934 - val_loss: 7.0726 - val_mae: 1.8487 - val_mse: 7.0726\n",
            "Epoch 690/1000\n",
            "250/250 [==============================] - 0s 76us/sample - loss: 3.6639 - mae: 1.1830 - mse: 3.6639 - val_loss: 5.7454 - val_mae: 1.7406 - val_mse: 5.7454\n",
            "Epoch 691/1000\n",
            "250/250 [==============================] - 0s 74us/sample - loss: 3.5760 - mae: 1.1904 - mse: 3.5760 - val_loss: 7.1100 - val_mae: 1.8693 - val_mse: 7.1100\n",
            "Epoch 692/1000\n",
            "250/250 [==============================] - 0s 70us/sample - loss: 3.5427 - mae: 1.2041 - mse: 3.5427 - val_loss: 5.7266 - val_mae: 1.7958 - val_mse: 5.7266\n",
            "Epoch 693/1000\n",
            "250/250 [==============================] - 0s 59us/sample - loss: 3.4783 - mae: 1.2449 - mse: 3.4783 - val_loss: 7.4018 - val_mae: 1.8595 - val_mse: 7.4018\n",
            "Epoch 694/1000\n",
            "250/250 [==============================] - 0s 62us/sample - loss: 3.4594 - mae: 1.1810 - mse: 3.4594 - val_loss: 5.6213 - val_mae: 1.7713 - val_mse: 5.6213\n",
            "Epoch 695/1000\n",
            "250/250 [==============================] - 0s 61us/sample - loss: 3.9990 - mae: 1.2770 - mse: 3.9990 - val_loss: 6.2783 - val_mae: 1.7495 - val_mse: 6.2783\n",
            "Epoch 696/1000\n",
            "250/250 [==============================] - 0s 65us/sample - loss: 3.4568 - mae: 1.1840 - mse: 3.4568 - val_loss: 6.9720 - val_mae: 1.8324 - val_mse: 6.9720\n",
            "Epoch 697/1000\n",
            "250/250 [==============================] - 0s 71us/sample - loss: 3.5443 - mae: 1.1939 - mse: 3.5443 - val_loss: 6.0520 - val_mae: 1.7227 - val_mse: 6.0520\n",
            "Epoch 698/1000\n",
            "250/250 [==============================] - 0s 66us/sample - loss: 3.7146 - mae: 1.2037 - mse: 3.7146 - val_loss: 6.1154 - val_mae: 1.7282 - val_mse: 6.1154\n",
            "Epoch 699/1000\n",
            "250/250 [==============================] - 0s 61us/sample - loss: 3.4541 - mae: 1.1671 - mse: 3.4541 - val_loss: 6.3073 - val_mae: 1.7409 - val_mse: 6.3073\n",
            "Epoch 700/1000\n",
            "250/250 [==============================] - 0s 71us/sample - loss: 3.5542 - mae: 1.2262 - mse: 3.5542 - val_loss: 6.3941 - val_mae: 1.7510 - val_mse: 6.3941\n",
            "Epoch 701/1000\n",
            "250/250 [==============================] - 0s 68us/sample - loss: 3.5423 - mae: 1.2051 - mse: 3.5423 - val_loss: 5.9183 - val_mae: 1.7595 - val_mse: 5.9183\n",
            "Epoch 702/1000\n",
            "250/250 [==============================] - 0s 72us/sample - loss: 3.4409 - mae: 1.2059 - mse: 3.4409 - val_loss: 7.3610 - val_mae: 1.9162 - val_mse: 7.3610\n",
            "Epoch 703/1000\n",
            "250/250 [==============================] - 0s 75us/sample - loss: 3.4666 - mae: 1.1939 - mse: 3.4666 - val_loss: 6.3129 - val_mae: 1.7471 - val_mse: 6.3129\n",
            "Epoch 704/1000\n",
            "250/250 [==============================] - 0s 73us/sample - loss: 3.4282 - mae: 1.1675 - mse: 3.4282 - val_loss: 5.7708 - val_mae: 1.7514 - val_mse: 5.7708\n",
            "Epoch 705/1000\n",
            "250/250 [==============================] - 0s 69us/sample - loss: 3.5121 - mae: 1.1978 - mse: 3.5121 - val_loss: 5.7442 - val_mae: 1.7483 - val_mse: 5.7442\n",
            "Epoch 706/1000\n",
            "250/250 [==============================] - 0s 64us/sample - loss: 3.6210 - mae: 1.2308 - mse: 3.6210 - val_loss: 5.4744 - val_mae: 1.7447 - val_mse: 5.4744\n",
            "Epoch 707/1000\n",
            "250/250 [==============================] - 0s 61us/sample - loss: 3.4307 - mae: 1.1829 - mse: 3.4307 - val_loss: 7.1478 - val_mae: 1.8492 - val_mse: 7.1478\n",
            "Epoch 708/1000\n",
            "250/250 [==============================] - 0s 72us/sample - loss: 3.6347 - mae: 1.2121 - mse: 3.6347 - val_loss: 5.9407 - val_mae: 1.7139 - val_mse: 5.9407\n",
            "Epoch 709/1000\n",
            "250/250 [==============================] - 0s 62us/sample - loss: 3.4204 - mae: 1.1829 - mse: 3.4204 - val_loss: 6.0226 - val_mae: 1.7152 - val_mse: 6.0226\n",
            "Epoch 710/1000\n",
            "250/250 [==============================] - 0s 70us/sample - loss: 3.4472 - mae: 1.1990 - mse: 3.4472 - val_loss: 6.7182 - val_mae: 1.7826 - val_mse: 6.7182\n",
            "Epoch 711/1000\n",
            "250/250 [==============================] - 0s 65us/sample - loss: 3.5134 - mae: 1.2144 - mse: 3.5134 - val_loss: 7.1702 - val_mae: 1.8397 - val_mse: 7.1702\n",
            "Epoch 712/1000\n",
            "250/250 [==============================] - 0s 69us/sample - loss: 3.4623 - mae: 1.1683 - mse: 3.4623 - val_loss: 5.6722 - val_mae: 1.7703 - val_mse: 5.6722\n",
            "Epoch 713/1000\n",
            "250/250 [==============================] - 0s 104us/sample - loss: 3.5380 - mae: 1.2269 - mse: 3.5380 - val_loss: 6.6642 - val_mae: 1.8019 - val_mse: 6.6642\n",
            "Epoch 714/1000\n",
            "250/250 [==============================] - 0s 67us/sample - loss: 3.6437 - mae: 1.1860 - mse: 3.6437 - val_loss: 5.8556 - val_mae: 1.7385 - val_mse: 5.8556\n",
            "Epoch 715/1000\n",
            "250/250 [==============================] - 0s 69us/sample - loss: 3.3649 - mae: 1.1455 - mse: 3.3649 - val_loss: 5.7648 - val_mae: 1.7376 - val_mse: 5.7648\n",
            "Epoch 716/1000\n",
            "250/250 [==============================] - 0s 66us/sample - loss: 3.4357 - mae: 1.1736 - mse: 3.4357 - val_loss: 6.0178 - val_mae: 1.8124 - val_mse: 6.0178\n",
            "Epoch 717/1000\n",
            "250/250 [==============================] - 0s 69us/sample - loss: 3.4436 - mae: 1.1893 - mse: 3.4436 - val_loss: 6.2276 - val_mae: 1.7426 - val_mse: 6.2276\n",
            "Epoch 718/1000\n",
            "250/250 [==============================] - 0s 80us/sample - loss: 3.4296 - mae: 1.1653 - mse: 3.4296 - val_loss: 6.3150 - val_mae: 1.7332 - val_mse: 6.3150\n",
            "Epoch 719/1000\n",
            "250/250 [==============================] - 0s 74us/sample - loss: 3.3666 - mae: 1.1840 - mse: 3.3666 - val_loss: 6.4678 - val_mae: 1.7682 - val_mse: 6.4678\n",
            "Epoch 720/1000\n",
            "250/250 [==============================] - 0s 86us/sample - loss: 3.4396 - mae: 1.1841 - mse: 3.4396 - val_loss: 6.2062 - val_mae: 1.7352 - val_mse: 6.2062\n",
            "Epoch 721/1000\n",
            "250/250 [==============================] - 0s 68us/sample - loss: 3.4657 - mae: 1.2085 - mse: 3.4657 - val_loss: 6.2766 - val_mae: 1.7504 - val_mse: 6.2766\n",
            "Epoch 722/1000\n",
            "250/250 [==============================] - 0s 75us/sample - loss: 3.5154 - mae: 1.1647 - mse: 3.5154 - val_loss: 6.6149 - val_mae: 1.7868 - val_mse: 6.6149\n",
            "Epoch 723/1000\n",
            "250/250 [==============================] - 0s 64us/sample - loss: 3.2661 - mae: 1.1480 - mse: 3.2661 - val_loss: 6.4981 - val_mae: 1.8018 - val_mse: 6.4981\n",
            "Epoch 724/1000\n",
            "250/250 [==============================] - 0s 85us/sample - loss: 3.4714 - mae: 1.2042 - mse: 3.4714 - val_loss: 6.0608 - val_mae: 1.7264 - val_mse: 6.0608\n",
            "Epoch 725/1000\n",
            "250/250 [==============================] - 0s 71us/sample - loss: 3.4878 - mae: 1.2013 - mse: 3.4878 - val_loss: 6.8450 - val_mae: 1.7887 - val_mse: 6.8450\n",
            "Epoch 726/1000\n",
            "250/250 [==============================] - 0s 87us/sample - loss: 3.3177 - mae: 1.1549 - mse: 3.3177 - val_loss: 6.9670 - val_mae: 1.8095 - val_mse: 6.9670\n",
            "Epoch 727/1000\n",
            "250/250 [==============================] - 0s 71us/sample - loss: 3.4535 - mae: 1.1730 - mse: 3.4535 - val_loss: 6.1880 - val_mae: 1.7151 - val_mse: 6.1880\n",
            "Epoch 728/1000\n",
            "250/250 [==============================] - 0s 69us/sample - loss: 3.3887 - mae: 1.1809 - mse: 3.3887 - val_loss: 6.6797 - val_mae: 1.8154 - val_mse: 6.6797\n",
            "Epoch 729/1000\n",
            "250/250 [==============================] - 0s 77us/sample - loss: 3.2788 - mae: 1.1724 - mse: 3.2788 - val_loss: 6.3224 - val_mae: 1.7382 - val_mse: 6.3224\n",
            "Epoch 730/1000\n",
            "250/250 [==============================] - 0s 79us/sample - loss: 3.3053 - mae: 1.1508 - mse: 3.3053 - val_loss: 6.6667 - val_mae: 1.7731 - val_mse: 6.6667\n",
            "Epoch 731/1000\n",
            "250/250 [==============================] - 0s 75us/sample - loss: 3.4108 - mae: 1.1877 - mse: 3.4108 - val_loss: 6.2653 - val_mae: 1.7399 - val_mse: 6.2653\n",
            "Epoch 732/1000\n",
            "250/250 [==============================] - 0s 67us/sample - loss: 3.1887 - mae: 1.1221 - mse: 3.1887 - val_loss: 6.2455 - val_mae: 1.7256 - val_mse: 6.2455\n",
            "Epoch 733/1000\n",
            "250/250 [==============================] - 0s 76us/sample - loss: 3.2303 - mae: 1.1711 - mse: 3.2303 - val_loss: 6.8690 - val_mae: 1.7992 - val_mse: 6.8690\n",
            "Epoch 734/1000\n",
            "250/250 [==============================] - 0s 73us/sample - loss: 3.5891 - mae: 1.2331 - mse: 3.5891 - val_loss: 6.4939 - val_mae: 1.7614 - val_mse: 6.4939\n",
            "Epoch 735/1000\n",
            "250/250 [==============================] - 0s 71us/sample - loss: 3.4940 - mae: 1.1904 - mse: 3.4940 - val_loss: 7.0141 - val_mae: 1.8701 - val_mse: 7.0141\n",
            "Epoch 736/1000\n",
            "250/250 [==============================] - 0s 109us/sample - loss: 3.2829 - mae: 1.1398 - mse: 3.2829 - val_loss: 8.6613 - val_mae: 2.0891 - val_mse: 8.6613\n",
            "Epoch 737/1000\n",
            "250/250 [==============================] - 0s 101us/sample - loss: 3.4097 - mae: 1.2027 - mse: 3.4097 - val_loss: 6.2288 - val_mae: 1.7424 - val_mse: 6.2288\n",
            "Epoch 738/1000\n",
            "250/250 [==============================] - 0s 84us/sample - loss: 3.4043 - mae: 1.1818 - mse: 3.4043 - val_loss: 7.1422 - val_mae: 1.8542 - val_mse: 7.1422\n",
            "Epoch 739/1000\n",
            "250/250 [==============================] - 0s 90us/sample - loss: 3.2041 - mae: 1.1294 - mse: 3.2041 - val_loss: 6.3273 - val_mae: 1.7970 - val_mse: 6.3273\n",
            "Epoch 740/1000\n",
            "250/250 [==============================] - 0s 108us/sample - loss: 3.3913 - mae: 1.1817 - mse: 3.3913 - val_loss: 5.8805 - val_mae: 1.7653 - val_mse: 5.8805\n",
            "Epoch 741/1000\n",
            "250/250 [==============================] - 0s 98us/sample - loss: 3.2769 - mae: 1.1780 - mse: 3.2769 - val_loss: 6.7245 - val_mae: 1.7959 - val_mse: 6.7245\n",
            "Epoch 742/1000\n",
            "250/250 [==============================] - 0s 81us/sample - loss: 3.3192 - mae: 1.1655 - mse: 3.3192 - val_loss: 6.9012 - val_mae: 1.8154 - val_mse: 6.9012\n",
            "Epoch 743/1000\n",
            "250/250 [==============================] - 0s 87us/sample - loss: 3.2610 - mae: 1.1376 - mse: 3.2610 - val_loss: 6.0477 - val_mae: 1.7417 - val_mse: 6.0477\n",
            "Epoch 744/1000\n",
            "250/250 [==============================] - 0s 90us/sample - loss: 3.4368 - mae: 1.2007 - mse: 3.4368 - val_loss: 6.6033 - val_mae: 1.7974 - val_mse: 6.6033\n",
            "Epoch 745/1000\n",
            "250/250 [==============================] - 0s 95us/sample - loss: 3.3510 - mae: 1.1519 - mse: 3.3510 - val_loss: 6.1887 - val_mae: 1.7551 - val_mse: 6.1887\n",
            "Epoch 746/1000\n",
            "250/250 [==============================] - 0s 97us/sample - loss: 3.1889 - mae: 1.1474 - mse: 3.1889 - val_loss: 6.5614 - val_mae: 1.7916 - val_mse: 6.5614\n",
            "Epoch 747/1000\n",
            "250/250 [==============================] - 0s 98us/sample - loss: 3.2723 - mae: 1.1688 - mse: 3.2723 - val_loss: 6.7782 - val_mae: 1.8223 - val_mse: 6.7782\n",
            "Epoch 748/1000\n",
            "250/250 [==============================] - 0s 101us/sample - loss: 3.2091 - mae: 1.1232 - mse: 3.2091 - val_loss: 6.7734 - val_mae: 1.8466 - val_mse: 6.7734\n",
            "Epoch 749/1000\n",
            "250/250 [==============================] - 0s 85us/sample - loss: 3.3306 - mae: 1.1851 - mse: 3.3306 - val_loss: 5.5500 - val_mae: 1.7737 - val_mse: 5.5500\n",
            "Epoch 750/1000\n",
            "250/250 [==============================] - 0s 91us/sample - loss: 3.3026 - mae: 1.1600 - mse: 3.3026 - val_loss: 5.6906 - val_mae: 1.7596 - val_mse: 5.6906\n",
            "Epoch 751/1000\n",
            "250/250 [==============================] - 0s 97us/sample - loss: 3.2265 - mae: 1.1473 - mse: 3.2265 - val_loss: 6.5215 - val_mae: 1.8098 - val_mse: 6.5215\n",
            "Epoch 752/1000\n",
            "250/250 [==============================] - 0s 100us/sample - loss: 3.2226 - mae: 1.1319 - mse: 3.2226 - val_loss: 5.8419 - val_mae: 1.7348 - val_mse: 5.8419\n",
            "Epoch 753/1000\n",
            "250/250 [==============================] - 0s 82us/sample - loss: 3.2456 - mae: 1.1813 - mse: 3.2456 - val_loss: 6.7334 - val_mae: 1.8100 - val_mse: 6.7334\n",
            "Epoch 754/1000\n",
            "250/250 [==============================] - 0s 84us/sample - loss: 3.2449 - mae: 1.1791 - mse: 3.2449 - val_loss: 6.3405 - val_mae: 1.7517 - val_mse: 6.3405\n",
            "Epoch 755/1000\n",
            "250/250 [==============================] - 0s 88us/sample - loss: 3.4231 - mae: 1.1901 - mse: 3.4231 - val_loss: 5.8080 - val_mae: 1.7281 - val_mse: 5.8080\n",
            "Epoch 756/1000\n",
            "250/250 [==============================] - 0s 103us/sample - loss: 3.1688 - mae: 1.1325 - mse: 3.1688 - val_loss: 5.7690 - val_mae: 1.7480 - val_mse: 5.7690\n",
            "Epoch 757/1000\n",
            "250/250 [==============================] - 0s 86us/sample - loss: 3.1898 - mae: 1.1223 - mse: 3.1898 - val_loss: 6.4734 - val_mae: 1.8029 - val_mse: 6.4734\n",
            "Epoch 758/1000\n",
            "250/250 [==============================] - 0s 79us/sample - loss: 3.3678 - mae: 1.1459 - mse: 3.3678 - val_loss: 6.2212 - val_mae: 1.7742 - val_mse: 6.2212\n",
            "Epoch 759/1000\n",
            "250/250 [==============================] - 0s 83us/sample - loss: 3.1503 - mae: 1.1177 - mse: 3.1503 - val_loss: 6.3777 - val_mae: 1.7813 - val_mse: 6.3777\n",
            "Epoch 760/1000\n",
            "250/250 [==============================] - 0s 88us/sample - loss: 3.1910 - mae: 1.1349 - mse: 3.1910 - val_loss: 7.2692 - val_mae: 1.8699 - val_mse: 7.2692\n",
            "Epoch 761/1000\n",
            "250/250 [==============================] - 0s 73us/sample - loss: 3.3595 - mae: 1.1795 - mse: 3.3595 - val_loss: 6.7025 - val_mae: 1.7930 - val_mse: 6.7025\n",
            "Epoch 762/1000\n",
            "250/250 [==============================] - 0s 92us/sample - loss: 3.1762 - mae: 1.1242 - mse: 3.1762 - val_loss: 5.9423 - val_mae: 1.7502 - val_mse: 5.9423\n",
            "Epoch 763/1000\n",
            "250/250 [==============================] - 0s 86us/sample - loss: 3.2736 - mae: 1.1104 - mse: 3.2736 - val_loss: 6.5229 - val_mae: 1.7829 - val_mse: 6.5229\n",
            "Epoch 764/1000\n",
            "250/250 [==============================] - 0s 85us/sample - loss: 3.1739 - mae: 1.1360 - mse: 3.1739 - val_loss: 7.7790 - val_mae: 1.9735 - val_mse: 7.7790\n",
            "Epoch 765/1000\n",
            "250/250 [==============================] - 0s 98us/sample - loss: 3.3215 - mae: 1.1335 - mse: 3.3215 - val_loss: 6.3199 - val_mae: 1.7424 - val_mse: 6.3199\n",
            "Epoch 766/1000\n",
            "250/250 [==============================] - 0s 88us/sample - loss: 3.2171 - mae: 1.1432 - mse: 3.2171 - val_loss: 6.8321 - val_mae: 1.8113 - val_mse: 6.8321\n",
            "Epoch 767/1000\n",
            "250/250 [==============================] - 0s 95us/sample - loss: 3.2785 - mae: 1.1961 - mse: 3.2785 - val_loss: 6.2551 - val_mae: 1.7349 - val_mse: 6.2551\n",
            "Epoch 768/1000\n",
            "250/250 [==============================] - 0s 84us/sample - loss: 3.2307 - mae: 1.1266 - mse: 3.2307 - val_loss: 6.7117 - val_mae: 1.8043 - val_mse: 6.7117\n",
            "Epoch 769/1000\n",
            "250/250 [==============================] - 0s 87us/sample - loss: 3.1888 - mae: 1.1680 - mse: 3.1888 - val_loss: 6.9068 - val_mae: 1.8137 - val_mse: 6.9068\n",
            "Epoch 770/1000\n",
            "250/250 [==============================] - 0s 99us/sample - loss: 3.1880 - mae: 1.1295 - mse: 3.1880 - val_loss: 6.9470 - val_mae: 1.8447 - val_mse: 6.9470\n",
            "Epoch 771/1000\n",
            "250/250 [==============================] - 0s 96us/sample - loss: 3.2206 - mae: 1.1250 - mse: 3.2206 - val_loss: 6.3821 - val_mae: 1.7473 - val_mse: 6.3821\n",
            "Epoch 772/1000\n",
            "250/250 [==============================] - 0s 98us/sample - loss: 3.2039 - mae: 1.1296 - mse: 3.2039 - val_loss: 6.8300 - val_mae: 1.8214 - val_mse: 6.8300\n",
            "Epoch 773/1000\n",
            "250/250 [==============================] - 0s 95us/sample - loss: 3.1025 - mae: 1.1117 - mse: 3.1025 - val_loss: 6.8404 - val_mae: 1.8193 - val_mse: 6.8404\n",
            "Epoch 774/1000\n",
            "250/250 [==============================] - 0s 105us/sample - loss: 3.4147 - mae: 1.1755 - mse: 3.4147 - val_loss: 5.5718 - val_mae: 1.7905 - val_mse: 5.5718\n",
            "Epoch 775/1000\n",
            "250/250 [==============================] - 0s 86us/sample - loss: 3.0806 - mae: 1.1342 - mse: 3.0806 - val_loss: 6.7749 - val_mae: 1.7897 - val_mse: 6.7749\n",
            "Epoch 776/1000\n",
            "250/250 [==============================] - 0s 113us/sample - loss: 3.2097 - mae: 1.1069 - mse: 3.2097 - val_loss: 5.9061 - val_mae: 1.7538 - val_mse: 5.9061\n",
            "Epoch 777/1000\n",
            "250/250 [==============================] - 0s 93us/sample - loss: 3.2344 - mae: 1.1513 - mse: 3.2344 - val_loss: 6.3338 - val_mae: 1.7728 - val_mse: 6.3338\n",
            "Epoch 778/1000\n",
            "250/250 [==============================] - 0s 114us/sample - loss: 3.1836 - mae: 1.1343 - mse: 3.1836 - val_loss: 5.8003 - val_mae: 1.7625 - val_mse: 5.8003\n",
            "Epoch 779/1000\n",
            "250/250 [==============================] - 0s 82us/sample - loss: 3.1999 - mae: 1.1300 - mse: 3.1999 - val_loss: 7.1646 - val_mae: 1.8404 - val_mse: 7.1646\n",
            "Epoch 780/1000\n",
            "250/250 [==============================] - 0s 104us/sample - loss: 3.1912 - mae: 1.1477 - mse: 3.1912 - val_loss: 6.8003 - val_mae: 1.8490 - val_mse: 6.8003\n",
            "Epoch 781/1000\n",
            "250/250 [==============================] - 0s 91us/sample - loss: 3.0834 - mae: 1.0920 - mse: 3.0834 - val_loss: 6.8069 - val_mae: 1.7868 - val_mse: 6.8069\n",
            "Epoch 782/1000\n",
            "250/250 [==============================] - 0s 103us/sample - loss: 3.1872 - mae: 1.1268 - mse: 3.1872 - val_loss: 6.7511 - val_mae: 1.8392 - val_mse: 6.7511\n",
            "Epoch 783/1000\n",
            "250/250 [==============================] - 0s 110us/sample - loss: 3.1060 - mae: 1.1125 - mse: 3.1060 - val_loss: 6.4465 - val_mae: 1.7820 - val_mse: 6.4465\n",
            "Epoch 784/1000\n",
            "250/250 [==============================] - 0s 90us/sample - loss: 3.1857 - mae: 1.1507 - mse: 3.1857 - val_loss: 8.7478 - val_mae: 2.1359 - val_mse: 8.7478\n",
            "Epoch 785/1000\n",
            "250/250 [==============================] - 0s 80us/sample - loss: 3.1474 - mae: 1.1404 - mse: 3.1474 - val_loss: 5.8994 - val_mae: 1.7627 - val_mse: 5.8994\n",
            "Epoch 786/1000\n",
            "250/250 [==============================] - 0s 97us/sample - loss: 3.1472 - mae: 1.0897 - mse: 3.1472 - val_loss: 6.4201 - val_mae: 1.7741 - val_mse: 6.4201\n",
            "Epoch 787/1000\n",
            "250/250 [==============================] - 0s 105us/sample - loss: 3.2693 - mae: 1.1696 - mse: 3.2693 - val_loss: 5.9877 - val_mae: 1.7648 - val_mse: 5.9877\n",
            "Epoch 788/1000\n",
            "250/250 [==============================] - 0s 104us/sample - loss: 3.2614 - mae: 1.1490 - mse: 3.2614 - val_loss: 7.3790 - val_mae: 1.8764 - val_mse: 7.3790\n",
            "Epoch 789/1000\n",
            "250/250 [==============================] - 0s 89us/sample - loss: 3.0115 - mae: 1.0975 - mse: 3.0115 - val_loss: 6.3538 - val_mae: 1.7581 - val_mse: 6.3538\n",
            "Epoch 790/1000\n",
            "250/250 [==============================] - 0s 127us/sample - loss: 3.3499 - mae: 1.1711 - mse: 3.3499 - val_loss: 5.8599 - val_mae: 1.7667 - val_mse: 5.8599\n",
            "Epoch 791/1000\n",
            "250/250 [==============================] - 0s 99us/sample - loss: 3.0755 - mae: 1.0759 - mse: 3.0755 - val_loss: 5.8202 - val_mae: 1.7130 - val_mse: 5.8202\n",
            "Epoch 792/1000\n",
            "250/250 [==============================] - 0s 92us/sample - loss: 3.2333 - mae: 1.1393 - mse: 3.2333 - val_loss: 6.0483 - val_mae: 1.7561 - val_mse: 6.0483\n",
            "Epoch 793/1000\n",
            "250/250 [==============================] - 0s 90us/sample - loss: 3.1412 - mae: 1.1495 - mse: 3.1412 - val_loss: 6.6824 - val_mae: 1.7882 - val_mse: 6.6824\n",
            "Epoch 794/1000\n",
            "250/250 [==============================] - 0s 90us/sample - loss: 3.1185 - mae: 1.1367 - mse: 3.1185 - val_loss: 6.3615 - val_mae: 1.8218 - val_mse: 6.3615\n",
            "Epoch 795/1000\n",
            "250/250 [==============================] - 0s 97us/sample - loss: 3.1249 - mae: 1.1225 - mse: 3.1249 - val_loss: 5.9276 - val_mae: 1.7847 - val_mse: 5.9276\n",
            "Epoch 796/1000\n",
            "250/250 [==============================] - 0s 103us/sample - loss: 3.2478 - mae: 1.1123 - mse: 3.2478 - val_loss: 6.0408 - val_mae: 1.7549 - val_mse: 6.0408\n",
            "Epoch 797/1000\n",
            "250/250 [==============================] - 0s 80us/sample - loss: 3.1319 - mae: 1.1047 - mse: 3.1319 - val_loss: 6.4238 - val_mae: 1.8074 - val_mse: 6.4238\n",
            "Epoch 798/1000\n",
            "250/250 [==============================] - 0s 106us/sample - loss: 3.1462 - mae: 1.0984 - mse: 3.1462 - val_loss: 7.4693 - val_mae: 1.8887 - val_mse: 7.4693\n",
            "Epoch 799/1000\n",
            "250/250 [==============================] - 0s 84us/sample - loss: 3.1916 - mae: 1.0944 - mse: 3.1916 - val_loss: 6.2635 - val_mae: 1.7521 - val_mse: 6.2635\n",
            "Epoch 800/1000\n",
            "250/250 [==============================] - 0s 103us/sample - loss: 3.2104 - mae: 1.1394 - mse: 3.2104 - val_loss: 6.8612 - val_mae: 1.8104 - val_mse: 6.8612\n",
            "Epoch 801/1000\n",
            "250/250 [==============================] - 0s 91us/sample - loss: 3.0891 - mae: 1.0750 - mse: 3.0891 - val_loss: 6.6113 - val_mae: 1.8440 - val_mse: 6.6113\n",
            "Epoch 802/1000\n",
            "250/250 [==============================] - 0s 100us/sample - loss: 3.0245 - mae: 1.0790 - mse: 3.0245 - val_loss: 5.5512 - val_mae: 1.7427 - val_mse: 5.5512\n",
            "Epoch 803/1000\n",
            "250/250 [==============================] - 0s 91us/sample - loss: 3.0836 - mae: 1.0953 - mse: 3.0836 - val_loss: 6.0656 - val_mae: 1.7999 - val_mse: 6.0656\n",
            "Epoch 804/1000\n",
            "250/250 [==============================] - 0s 123us/sample - loss: 3.2081 - mae: 1.1435 - mse: 3.2081 - val_loss: 7.2114 - val_mae: 1.8585 - val_mse: 7.2114\n",
            "Epoch 805/1000\n",
            "250/250 [==============================] - 0s 120us/sample - loss: 2.9576 - mae: 1.0723 - mse: 2.9576 - val_loss: 5.6938 - val_mae: 1.7236 - val_mse: 5.6938\n",
            "Epoch 806/1000\n",
            "250/250 [==============================] - 0s 83us/sample - loss: 3.1944 - mae: 1.1635 - mse: 3.1944 - val_loss: 5.9040 - val_mae: 1.7430 - val_mse: 5.9040\n",
            "Epoch 807/1000\n",
            "250/250 [==============================] - 0s 123us/sample - loss: 3.0713 - mae: 1.0928 - mse: 3.0713 - val_loss: 6.8012 - val_mae: 1.8792 - val_mse: 6.8012\n",
            "Epoch 808/1000\n",
            "250/250 [==============================] - 0s 103us/sample - loss: 3.0690 - mae: 1.0846 - mse: 3.0690 - val_loss: 6.3458 - val_mae: 1.7952 - val_mse: 6.3458\n",
            "Epoch 809/1000\n",
            "250/250 [==============================] - 0s 98us/sample - loss: 3.0858 - mae: 1.0911 - mse: 3.0858 - val_loss: 5.8026 - val_mae: 1.7722 - val_mse: 5.8026\n",
            "Epoch 810/1000\n",
            "250/250 [==============================] - 0s 107us/sample - loss: 3.3811 - mae: 1.1629 - mse: 3.3811 - val_loss: 6.5442 - val_mae: 1.8106 - val_mse: 6.5442\n",
            "Epoch 811/1000\n",
            "250/250 [==============================] - 0s 96us/sample - loss: 2.9838 - mae: 1.0795 - mse: 2.9838 - val_loss: 7.3995 - val_mae: 1.8815 - val_mse: 7.3995\n",
            "Epoch 812/1000\n",
            "250/250 [==============================] - 0s 114us/sample - loss: 3.0714 - mae: 1.0694 - mse: 3.0714 - val_loss: 5.7748 - val_mae: 1.7637 - val_mse: 5.7748\n",
            "Epoch 813/1000\n",
            "250/250 [==============================] - 0s 89us/sample - loss: 3.1839 - mae: 1.1282 - mse: 3.1839 - val_loss: 7.9394 - val_mae: 1.9889 - val_mse: 7.9394\n",
            "Epoch 814/1000\n",
            "250/250 [==============================] - 0s 97us/sample - loss: 3.0435 - mae: 1.0854 - mse: 3.0435 - val_loss: 6.3902 - val_mae: 1.8277 - val_mse: 6.3902\n",
            "Epoch 815/1000\n",
            "250/250 [==============================] - 0s 84us/sample - loss: 3.0542 - mae: 1.1074 - mse: 3.0542 - val_loss: 6.1683 - val_mae: 1.7947 - val_mse: 6.1683\n",
            "Epoch 816/1000\n",
            "250/250 [==============================] - 0s 76us/sample - loss: 2.9894 - mae: 1.0945 - mse: 2.9894 - val_loss: 6.5287 - val_mae: 1.8057 - val_mse: 6.5287\n",
            "Epoch 817/1000\n",
            "250/250 [==============================] - 0s 88us/sample - loss: 3.1401 - mae: 1.1259 - mse: 3.1401 - val_loss: 8.6571 - val_mae: 2.1095 - val_mse: 8.6571\n",
            "Epoch 818/1000\n",
            "250/250 [==============================] - 0s 100us/sample - loss: 3.2825 - mae: 1.1572 - mse: 3.2825 - val_loss: 6.3521 - val_mae: 1.7938 - val_mse: 6.3521\n",
            "Epoch 819/1000\n",
            "250/250 [==============================] - 0s 84us/sample - loss: 2.9471 - mae: 1.0778 - mse: 2.9471 - val_loss: 8.0284 - val_mae: 2.0044 - val_mse: 8.0284\n",
            "Epoch 820/1000\n",
            "250/250 [==============================] - 0s 92us/sample - loss: 3.0914 - mae: 1.0964 - mse: 3.0914 - val_loss: 6.8854 - val_mae: 1.8542 - val_mse: 6.8854\n",
            "Epoch 821/1000\n",
            "250/250 [==============================] - 0s 87us/sample - loss: 2.8739 - mae: 1.0314 - mse: 2.8739 - val_loss: 5.9272 - val_mae: 1.7670 - val_mse: 5.9272\n",
            "Epoch 822/1000\n",
            "250/250 [==============================] - 0s 101us/sample - loss: 3.0033 - mae: 1.0897 - mse: 3.0033 - val_loss: 6.5426 - val_mae: 1.8541 - val_mse: 6.5426\n",
            "Epoch 823/1000\n",
            "250/250 [==============================] - 0s 91us/sample - loss: 3.1429 - mae: 1.0990 - mse: 3.1429 - val_loss: 7.3061 - val_mae: 1.9174 - val_mse: 7.3061\n",
            "Epoch 824/1000\n",
            "250/250 [==============================] - 0s 93us/sample - loss: 3.0498 - mae: 1.0986 - mse: 3.0498 - val_loss: 6.1187 - val_mae: 1.7880 - val_mse: 6.1187\n",
            "Epoch 825/1000\n",
            "250/250 [==============================] - 0s 86us/sample - loss: 3.0765 - mae: 1.0999 - mse: 3.0765 - val_loss: 7.0866 - val_mae: 1.8509 - val_mse: 7.0866\n",
            "Epoch 826/1000\n",
            "250/250 [==============================] - 0s 83us/sample - loss: 2.9348 - mae: 1.0747 - mse: 2.9348 - val_loss: 5.7569 - val_mae: 1.7591 - val_mse: 5.7569\n",
            "Epoch 827/1000\n",
            "250/250 [==============================] - 0s 84us/sample - loss: 3.1345 - mae: 1.1325 - mse: 3.1345 - val_loss: 6.1311 - val_mae: 1.7846 - val_mse: 6.1311\n",
            "Epoch 828/1000\n",
            "250/250 [==============================] - 0s 92us/sample - loss: 2.9261 - mae: 1.0420 - mse: 2.9261 - val_loss: 7.8366 - val_mae: 1.9842 - val_mse: 7.8366\n",
            "Epoch 829/1000\n",
            "250/250 [==============================] - 0s 78us/sample - loss: 3.1326 - mae: 1.1205 - mse: 3.1326 - val_loss: 8.5343 - val_mae: 2.0917 - val_mse: 8.5343\n",
            "Epoch 830/1000\n",
            "250/250 [==============================] - 0s 85us/sample - loss: 3.1661 - mae: 1.0995 - mse: 3.1661 - val_loss: 7.1332 - val_mae: 1.8754 - val_mse: 7.1332\n",
            "Epoch 831/1000\n",
            "250/250 [==============================] - 0s 86us/sample - loss: 3.1062 - mae: 1.1218 - mse: 3.1062 - val_loss: 6.2686 - val_mae: 1.7645 - val_mse: 6.2686\n",
            "Epoch 832/1000\n",
            "250/250 [==============================] - 0s 88us/sample - loss: 2.9916 - mae: 1.0726 - mse: 2.9916 - val_loss: 6.2612 - val_mae: 1.7921 - val_mse: 6.2612\n",
            "Epoch 833/1000\n",
            "250/250 [==============================] - 0s 82us/sample - loss: 3.0009 - mae: 1.1014 - mse: 3.0009 - val_loss: 6.5959 - val_mae: 1.8331 - val_mse: 6.5959\n",
            "Epoch 834/1000\n",
            "250/250 [==============================] - 0s 86us/sample - loss: 3.1664 - mae: 1.1355 - mse: 3.1664 - val_loss: 6.2847 - val_mae: 1.7698 - val_mse: 6.2847\n",
            "Epoch 835/1000\n",
            "250/250 [==============================] - 0s 106us/sample - loss: 3.0300 - mae: 1.0945 - mse: 3.0300 - val_loss: 6.0993 - val_mae: 1.7486 - val_mse: 6.0993\n",
            "Epoch 836/1000\n",
            "250/250 [==============================] - 0s 109us/sample - loss: 2.8830 - mae: 1.0368 - mse: 2.8830 - val_loss: 5.7886 - val_mae: 1.7642 - val_mse: 5.7886\n",
            "Epoch 837/1000\n",
            "250/250 [==============================] - 0s 85us/sample - loss: 2.8969 - mae: 1.0888 - mse: 2.8969 - val_loss: 6.7417 - val_mae: 1.8651 - val_mse: 6.7417\n",
            "Epoch 838/1000\n",
            "250/250 [==============================] - 0s 79us/sample - loss: 3.3894 - mae: 1.1548 - mse: 3.3894 - val_loss: 7.0386 - val_mae: 1.8619 - val_mse: 7.0386\n",
            "Epoch 839/1000\n",
            "250/250 [==============================] - 0s 77us/sample - loss: 3.1513 - mae: 1.1170 - mse: 3.1513 - val_loss: 7.0881 - val_mae: 1.8881 - val_mse: 7.0881\n",
            "Epoch 840/1000\n",
            "250/250 [==============================] - 0s 102us/sample - loss: 3.0343 - mae: 1.0694 - mse: 3.0343 - val_loss: 6.3533 - val_mae: 1.7782 - val_mse: 6.3533\n",
            "Epoch 841/1000\n",
            "250/250 [==============================] - 0s 81us/sample - loss: 2.9744 - mae: 1.0482 - mse: 2.9744 - val_loss: 7.4023 - val_mae: 1.9265 - val_mse: 7.4023\n",
            "Epoch 842/1000\n",
            "250/250 [==============================] - 0s 91us/sample - loss: 3.0683 - mae: 1.1064 - mse: 3.0683 - val_loss: 7.2005 - val_mae: 1.8754 - val_mse: 7.2005\n",
            "Epoch 843/1000\n",
            "250/250 [==============================] - 0s 83us/sample - loss: 3.2192 - mae: 1.1187 - mse: 3.2192 - val_loss: 6.2539 - val_mae: 1.8109 - val_mse: 6.2539\n",
            "Epoch 844/1000\n",
            "250/250 [==============================] - 0s 91us/sample - loss: 2.9798 - mae: 1.0470 - mse: 2.9798 - val_loss: 6.0182 - val_mae: 1.8204 - val_mse: 6.0182\n",
            "Epoch 845/1000\n",
            "250/250 [==============================] - 0s 94us/sample - loss: 2.9682 - mae: 1.0876 - mse: 2.9682 - val_loss: 7.2249 - val_mae: 1.8827 - val_mse: 7.2249\n",
            "Epoch 846/1000\n",
            "250/250 [==============================] - 0s 104us/sample - loss: 2.9339 - mae: 1.0682 - mse: 2.9339 - val_loss: 8.2589 - val_mae: 2.0818 - val_mse: 8.2589\n",
            "Epoch 847/1000\n",
            "250/250 [==============================] - 0s 84us/sample - loss: 3.2541 - mae: 1.1320 - mse: 3.2541 - val_loss: 6.4908 - val_mae: 1.8065 - val_mse: 6.4908\n",
            "Epoch 848/1000\n",
            "250/250 [==============================] - 0s 104us/sample - loss: 2.7828 - mae: 1.0194 - mse: 2.7828 - val_loss: 5.7891 - val_mae: 1.8440 - val_mse: 5.7891\n",
            "Epoch 849/1000\n",
            "250/250 [==============================] - 0s 93us/sample - loss: 3.1155 - mae: 1.1379 - mse: 3.1155 - val_loss: 7.2525 - val_mae: 1.8925 - val_mse: 7.2525\n",
            "Epoch 850/1000\n",
            "250/250 [==============================] - 0s 118us/sample - loss: 3.0073 - mae: 1.0610 - mse: 3.0073 - val_loss: 6.1651 - val_mae: 1.7903 - val_mse: 6.1651\n",
            "Epoch 851/1000\n",
            "250/250 [==============================] - 0s 100us/sample - loss: 2.9296 - mae: 1.0835 - mse: 2.9296 - val_loss: 7.1638 - val_mae: 1.8923 - val_mse: 7.1638\n",
            "Epoch 852/1000\n",
            "250/250 [==============================] - 0s 102us/sample - loss: 3.0752 - mae: 1.0950 - mse: 3.0752 - val_loss: 6.3998 - val_mae: 1.8437 - val_mse: 6.3998\n",
            "Epoch 853/1000\n",
            "250/250 [==============================] - 0s 96us/sample - loss: 2.9269 - mae: 1.0680 - mse: 2.9269 - val_loss: 6.8595 - val_mae: 1.8753 - val_mse: 6.8595\n",
            "Epoch 854/1000\n",
            "250/250 [==============================] - 0s 96us/sample - loss: 2.9802 - mae: 1.0786 - mse: 2.9802 - val_loss: 5.9502 - val_mae: 1.7460 - val_mse: 5.9502\n",
            "Epoch 855/1000\n",
            "250/250 [==============================] - 0s 86us/sample - loss: 2.9439 - mae: 1.0934 - mse: 2.9439 - val_loss: 6.3622 - val_mae: 1.7796 - val_mse: 6.3622\n",
            "Epoch 856/1000\n",
            "250/250 [==============================] - 0s 86us/sample - loss: 3.0011 - mae: 1.0709 - mse: 3.0011 - val_loss: 6.7435 - val_mae: 1.8521 - val_mse: 6.7435\n",
            "Epoch 857/1000\n",
            "250/250 [==============================] - 0s 86us/sample - loss: 2.9546 - mae: 1.0754 - mse: 2.9546 - val_loss: 8.4152 - val_mae: 2.1090 - val_mse: 8.4152\n",
            "Epoch 858/1000\n",
            "250/250 [==============================] - 0s 94us/sample - loss: 2.9823 - mae: 1.0637 - mse: 2.9823 - val_loss: 6.6481 - val_mae: 1.8992 - val_mse: 6.6481\n",
            "Epoch 859/1000\n",
            "250/250 [==============================] - 0s 92us/sample - loss: 2.9662 - mae: 1.0799 - mse: 2.9662 - val_loss: 6.1524 - val_mae: 1.7868 - val_mse: 6.1524\n",
            "Epoch 860/1000\n",
            "250/250 [==============================] - 0s 141us/sample - loss: 3.0277 - mae: 1.0765 - mse: 3.0277 - val_loss: 7.5170 - val_mae: 1.9281 - val_mse: 7.5170\n",
            "Epoch 861/1000\n",
            "250/250 [==============================] - 0s 79us/sample - loss: 2.9141 - mae: 1.0512 - mse: 2.9141 - val_loss: 6.7158 - val_mae: 1.8685 - val_mse: 6.7158\n",
            "Epoch 862/1000\n",
            "250/250 [==============================] - 0s 93us/sample - loss: 2.7815 - mae: 1.0430 - mse: 2.7815 - val_loss: 6.6010 - val_mae: 1.8448 - val_mse: 6.6010\n",
            "Epoch 863/1000\n",
            "250/250 [==============================] - 0s 109us/sample - loss: 3.1250 - mae: 1.1257 - mse: 3.1250 - val_loss: 6.2469 - val_mae: 1.7739 - val_mse: 6.2469\n",
            "Epoch 864/1000\n",
            "250/250 [==============================] - 0s 90us/sample - loss: 2.9824 - mae: 1.0404 - mse: 2.9824 - val_loss: 5.7730 - val_mae: 1.7421 - val_mse: 5.7730\n",
            "Epoch 865/1000\n",
            "250/250 [==============================] - 0s 82us/sample - loss: 3.1195 - mae: 1.1257 - mse: 3.1195 - val_loss: 5.9700 - val_mae: 1.7907 - val_mse: 5.9700\n",
            "Epoch 866/1000\n",
            "250/250 [==============================] - 0s 107us/sample - loss: 2.8886 - mae: 1.0166 - mse: 2.8886 - val_loss: 6.0511 - val_mae: 1.8323 - val_mse: 6.0511\n",
            "Epoch 867/1000\n",
            "250/250 [==============================] - 0s 122us/sample - loss: 3.0563 - mae: 1.0734 - mse: 3.0563 - val_loss: 7.4526 - val_mae: 1.9523 - val_mse: 7.4526\n",
            "Epoch 868/1000\n",
            "250/250 [==============================] - 0s 81us/sample - loss: 2.9597 - mae: 1.0869 - mse: 2.9597 - val_loss: 6.7477 - val_mae: 1.8455 - val_mse: 6.7477\n",
            "Epoch 869/1000\n",
            "250/250 [==============================] - 0s 97us/sample - loss: 2.8851 - mae: 1.0639 - mse: 2.8851 - val_loss: 6.8008 - val_mae: 1.8753 - val_mse: 6.8008\n",
            "Epoch 870/1000\n",
            "250/250 [==============================] - 0s 92us/sample - loss: 2.9513 - mae: 1.0567 - mse: 2.9513 - val_loss: 6.7945 - val_mae: 1.8567 - val_mse: 6.7945\n",
            "Epoch 871/1000\n",
            "250/250 [==============================] - 0s 81us/sample - loss: 2.9059 - mae: 1.0638 - mse: 2.9059 - val_loss: 6.6994 - val_mae: 1.8488 - val_mse: 6.6994\n",
            "Epoch 872/1000\n",
            "250/250 [==============================] - 0s 92us/sample - loss: 2.7534 - mae: 0.9899 - mse: 2.7534 - val_loss: 6.6124 - val_mae: 1.8832 - val_mse: 6.6124\n",
            "Epoch 873/1000\n",
            "250/250 [==============================] - 0s 95us/sample - loss: 2.9669 - mae: 1.0682 - mse: 2.9669 - val_loss: 8.2459 - val_mae: 2.0780 - val_mse: 8.2459\n",
            "Epoch 874/1000\n",
            "250/250 [==============================] - 0s 89us/sample - loss: 2.9818 - mae: 1.0649 - mse: 2.9818 - val_loss: 5.8509 - val_mae: 1.8270 - val_mse: 5.8509\n",
            "Epoch 875/1000\n",
            "250/250 [==============================] - 0s 85us/sample - loss: 2.9058 - mae: 1.0385 - mse: 2.9058 - val_loss: 6.1218 - val_mae: 1.7740 - val_mse: 6.1218\n",
            "Epoch 876/1000\n",
            "250/250 [==============================] - 0s 90us/sample - loss: 2.9486 - mae: 1.0621 - mse: 2.9486 - val_loss: 6.3130 - val_mae: 1.9156 - val_mse: 6.3130\n",
            "Epoch 877/1000\n",
            "250/250 [==============================] - 0s 92us/sample - loss: 2.9106 - mae: 1.0685 - mse: 2.9106 - val_loss: 6.2481 - val_mae: 1.8113 - val_mse: 6.2481\n",
            "Epoch 878/1000\n",
            "250/250 [==============================] - 0s 85us/sample - loss: 2.9675 - mae: 1.0791 - mse: 2.9675 - val_loss: 6.4215 - val_mae: 1.7964 - val_mse: 6.4215\n",
            "Epoch 879/1000\n",
            "250/250 [==============================] - 0s 86us/sample - loss: 2.9988 - mae: 1.0448 - mse: 2.9988 - val_loss: 6.5190 - val_mae: 1.8044 - val_mse: 6.5190\n",
            "Epoch 880/1000\n",
            "250/250 [==============================] - 0s 93us/sample - loss: 2.8836 - mae: 1.0332 - mse: 2.8836 - val_loss: 6.6531 - val_mae: 1.8273 - val_mse: 6.6531\n",
            "Epoch 881/1000\n",
            "250/250 [==============================] - 0s 86us/sample - loss: 3.1583 - mae: 1.1172 - mse: 3.1583 - val_loss: 6.2324 - val_mae: 1.8057 - val_mse: 6.2324\n",
            "Epoch 882/1000\n",
            "250/250 [==============================] - 0s 96us/sample - loss: 2.9664 - mae: 1.0541 - mse: 2.9664 - val_loss: 7.6994 - val_mae: 2.0059 - val_mse: 7.6994\n",
            "Epoch 883/1000\n",
            "250/250 [==============================] - 0s 89us/sample - loss: 2.8857 - mae: 1.0529 - mse: 2.8857 - val_loss: 6.3057 - val_mae: 1.7840 - val_mse: 6.3057\n",
            "Epoch 884/1000\n",
            "250/250 [==============================] - 0s 88us/sample - loss: 2.8682 - mae: 1.0519 - mse: 2.8682 - val_loss: 5.9497 - val_mae: 1.7688 - val_mse: 5.9497\n",
            "Epoch 885/1000\n",
            "250/250 [==============================] - 0s 94us/sample - loss: 2.8332 - mae: 1.0288 - mse: 2.8332 - val_loss: 7.8564 - val_mae: 1.9863 - val_mse: 7.8564\n",
            "Epoch 886/1000\n",
            "250/250 [==============================] - 0s 77us/sample - loss: 3.0226 - mae: 1.1101 - mse: 3.0226 - val_loss: 7.5482 - val_mae: 1.9746 - val_mse: 7.5482\n",
            "Epoch 887/1000\n",
            "250/250 [==============================] - 0s 92us/sample - loss: 3.0637 - mae: 1.0767 - mse: 3.0637 - val_loss: 7.0744 - val_mae: 1.9058 - val_mse: 7.0744\n",
            "Epoch 888/1000\n",
            "250/250 [==============================] - 0s 82us/sample - loss: 3.0798 - mae: 1.0768 - mse: 3.0798 - val_loss: 6.8686 - val_mae: 1.8488 - val_mse: 6.8686\n",
            "Epoch 889/1000\n",
            "250/250 [==============================] - 0s 103us/sample - loss: 2.7879 - mae: 1.0199 - mse: 2.7879 - val_loss: 5.8834 - val_mae: 1.7733 - val_mse: 5.8834\n",
            "Epoch 890/1000\n",
            "250/250 [==============================] - 0s 80us/sample - loss: 3.0250 - mae: 1.0663 - mse: 3.0250 - val_loss: 6.1068 - val_mae: 1.7664 - val_mse: 6.1068\n",
            "Epoch 891/1000\n",
            "250/250 [==============================] - 0s 110us/sample - loss: 2.8884 - mae: 1.0314 - mse: 2.8884 - val_loss: 6.0309 - val_mae: 1.7599 - val_mse: 6.0309\n",
            "Epoch 892/1000\n",
            "250/250 [==============================] - 0s 78us/sample - loss: 2.8536 - mae: 1.0292 - mse: 2.8536 - val_loss: 7.4708 - val_mae: 1.9303 - val_mse: 7.4708\n",
            "Epoch 893/1000\n",
            "250/250 [==============================] - 0s 92us/sample - loss: 2.9394 - mae: 1.0461 - mse: 2.9394 - val_loss: 6.6463 - val_mae: 1.8500 - val_mse: 6.6463\n",
            "Epoch 894/1000\n",
            "250/250 [==============================] - 0s 84us/sample - loss: 2.7965 - mae: 1.0265 - mse: 2.7965 - val_loss: 6.3129 - val_mae: 1.7907 - val_mse: 6.3129\n",
            "Epoch 895/1000\n",
            "250/250 [==============================] - 0s 107us/sample - loss: 2.7957 - mae: 1.0513 - mse: 2.7957 - val_loss: 6.2188 - val_mae: 1.8364 - val_mse: 6.2188\n",
            "Epoch 896/1000\n",
            "250/250 [==============================] - 0s 82us/sample - loss: 2.9544 - mae: 1.1164 - mse: 2.9544 - val_loss: 6.4347 - val_mae: 1.8137 - val_mse: 6.4347\n",
            "Epoch 897/1000\n",
            "250/250 [==============================] - 0s 91us/sample - loss: 2.7736 - mae: 1.0080 - mse: 2.7736 - val_loss: 6.3786 - val_mae: 1.8011 - val_mse: 6.3786\n",
            "Epoch 898/1000\n",
            "250/250 [==============================] - 0s 80us/sample - loss: 3.1578 - mae: 1.1277 - mse: 3.1578 - val_loss: 7.9091 - val_mae: 2.0225 - val_mse: 7.9091\n",
            "Epoch 899/1000\n",
            "250/250 [==============================] - 0s 80us/sample - loss: 2.9087 - mae: 1.0158 - mse: 2.9087 - val_loss: 7.0656 - val_mae: 1.8876 - val_mse: 7.0656\n",
            "Epoch 900/1000\n",
            "250/250 [==============================] - 0s 82us/sample - loss: 2.8168 - mae: 1.0388 - mse: 2.8168 - val_loss: 6.2193 - val_mae: 1.8226 - val_mse: 6.2193\n",
            "Epoch 901/1000\n",
            "250/250 [==============================] - 0s 87us/sample - loss: 2.8258 - mae: 1.0765 - mse: 2.8258 - val_loss: 6.1009 - val_mae: 1.8224 - val_mse: 6.1009\n",
            "Epoch 902/1000\n",
            "250/250 [==============================] - 0s 99us/sample - loss: 2.8766 - mae: 1.0650 - mse: 2.8766 - val_loss: 6.3691 - val_mae: 1.8389 - val_mse: 6.3691\n",
            "Epoch 903/1000\n",
            "250/250 [==============================] - 0s 109us/sample - loss: 2.8981 - mae: 1.0543 - mse: 2.8981 - val_loss: 5.9010 - val_mae: 1.8066 - val_mse: 5.9010\n",
            "Epoch 904/1000\n",
            "250/250 [==============================] - 0s 95us/sample - loss: 3.0756 - mae: 1.1106 - mse: 3.0756 - val_loss: 6.5356 - val_mae: 1.8363 - val_mse: 6.5356\n",
            "Epoch 905/1000\n",
            "250/250 [==============================] - 0s 82us/sample - loss: 2.7497 - mae: 0.9782 - mse: 2.7497 - val_loss: 6.2858 - val_mae: 1.8238 - val_mse: 6.2858\n",
            "Epoch 906/1000\n",
            "250/250 [==============================] - 0s 92us/sample - loss: 2.7847 - mae: 1.0540 - mse: 2.7847 - val_loss: 5.8037 - val_mae: 1.8078 - val_mse: 5.8037\n",
            "Epoch 907/1000\n",
            "250/250 [==============================] - 0s 103us/sample - loss: 2.6987 - mae: 1.0290 - mse: 2.6987 - val_loss: 6.7997 - val_mae: 1.8590 - val_mse: 6.7997\n",
            "Epoch 908/1000\n",
            "250/250 [==============================] - 0s 123us/sample - loss: 2.9134 - mae: 1.0801 - mse: 2.9134 - val_loss: 5.9952 - val_mae: 1.8683 - val_mse: 5.9952\n",
            "Epoch 909/1000\n",
            "250/250 [==============================] - 0s 100us/sample - loss: 2.9494 - mae: 1.0715 - mse: 2.9494 - val_loss: 6.4594 - val_mae: 1.8090 - val_mse: 6.4594\n",
            "Epoch 910/1000\n",
            "250/250 [==============================] - 0s 90us/sample - loss: 2.7301 - mae: 1.0211 - mse: 2.7301 - val_loss: 6.7042 - val_mae: 1.9009 - val_mse: 6.7042\n",
            "Epoch 911/1000\n",
            "250/250 [==============================] - 0s 106us/sample - loss: 2.8585 - mae: 1.0432 - mse: 2.8585 - val_loss: 5.8099 - val_mae: 1.8244 - val_mse: 5.8099\n",
            "Epoch 912/1000\n",
            "250/250 [==============================] - 0s 81us/sample - loss: 3.0190 - mae: 1.0803 - mse: 3.0190 - val_loss: 6.2985 - val_mae: 1.8018 - val_mse: 6.2985\n",
            "Epoch 913/1000\n",
            "250/250 [==============================] - 0s 87us/sample - loss: 2.7808 - mae: 1.0390 - mse: 2.7808 - val_loss: 7.1884 - val_mae: 1.9137 - val_mse: 7.1884\n",
            "Epoch 914/1000\n",
            "250/250 [==============================] - 0s 110us/sample - loss: 2.7004 - mae: 0.9985 - mse: 2.7004 - val_loss: 6.6892 - val_mae: 1.8887 - val_mse: 6.6892\n",
            "Epoch 915/1000\n",
            "250/250 [==============================] - 0s 106us/sample - loss: 2.7878 - mae: 1.0084 - mse: 2.7878 - val_loss: 7.3809 - val_mae: 1.9357 - val_mse: 7.3809\n",
            "Epoch 916/1000\n",
            "250/250 [==============================] - 0s 91us/sample - loss: 2.8968 - mae: 1.0921 - mse: 2.8968 - val_loss: 6.6920 - val_mae: 1.8822 - val_mse: 6.6920\n",
            "Epoch 917/1000\n",
            "250/250 [==============================] - 0s 128us/sample - loss: 2.9272 - mae: 1.0794 - mse: 2.9272 - val_loss: 7.1330 - val_mae: 1.8968 - val_mse: 7.1330\n",
            "Epoch 918/1000\n",
            "250/250 [==============================] - 0s 117us/sample - loss: 2.9156 - mae: 1.0402 - mse: 2.9156 - val_loss: 7.4043 - val_mae: 1.9705 - val_mse: 7.4043\n",
            "Epoch 919/1000\n",
            "250/250 [==============================] - 0s 82us/sample - loss: 2.8195 - mae: 1.0512 - mse: 2.8195 - val_loss: 6.8206 - val_mae: 1.9073 - val_mse: 6.8206\n",
            "Epoch 920/1000\n",
            "250/250 [==============================] - 0s 83us/sample - loss: 2.9486 - mae: 1.0359 - mse: 2.9486 - val_loss: 6.5008 - val_mae: 1.8517 - val_mse: 6.5008\n",
            "Epoch 921/1000\n",
            "250/250 [==============================] - 0s 93us/sample - loss: 2.8670 - mae: 1.0529 - mse: 2.8670 - val_loss: 7.1233 - val_mae: 1.9616 - val_mse: 7.1233\n",
            "Epoch 922/1000\n",
            "250/250 [==============================] - 0s 97us/sample - loss: 2.7297 - mae: 1.0164 - mse: 2.7297 - val_loss: 6.2715 - val_mae: 1.8281 - val_mse: 6.2715\n",
            "Epoch 923/1000\n",
            "250/250 [==============================] - 0s 96us/sample - loss: 2.8328 - mae: 1.0174 - mse: 2.8328 - val_loss: 7.1499 - val_mae: 1.8897 - val_mse: 7.1499\n",
            "Epoch 924/1000\n",
            "250/250 [==============================] - 0s 108us/sample - loss: 2.7822 - mae: 1.0600 - mse: 2.7822 - val_loss: 6.6175 - val_mae: 1.8521 - val_mse: 6.6175\n",
            "Epoch 925/1000\n",
            "250/250 [==============================] - 0s 111us/sample - loss: 2.8729 - mae: 1.0511 - mse: 2.8729 - val_loss: 7.0923 - val_mae: 1.9042 - val_mse: 7.0923\n",
            "Epoch 926/1000\n",
            "250/250 [==============================] - 0s 101us/sample - loss: 2.7404 - mae: 1.0265 - mse: 2.7404 - val_loss: 6.4560 - val_mae: 1.8669 - val_mse: 6.4560\n",
            "Epoch 927/1000\n",
            "250/250 [==============================] - 0s 85us/sample - loss: 2.6988 - mae: 0.9996 - mse: 2.6988 - val_loss: 6.5011 - val_mae: 1.8469 - val_mse: 6.5011\n",
            "Epoch 928/1000\n",
            "250/250 [==============================] - 0s 81us/sample - loss: 2.8394 - mae: 1.0177 - mse: 2.8394 - val_loss: 6.6404 - val_mae: 1.9072 - val_mse: 6.6404\n",
            "Epoch 929/1000\n",
            "250/250 [==============================] - 0s 92us/sample - loss: 2.6897 - mae: 1.0082 - mse: 2.6897 - val_loss: 7.7379 - val_mae: 1.9762 - val_mse: 7.7379\n",
            "Epoch 930/1000\n",
            "250/250 [==============================] - 0s 87us/sample - loss: 2.7440 - mae: 1.0203 - mse: 2.7440 - val_loss: 6.4591 - val_mae: 1.8191 - val_mse: 6.4591\n",
            "Epoch 931/1000\n",
            "250/250 [==============================] - 0s 100us/sample - loss: 2.8537 - mae: 1.0450 - mse: 2.8537 - val_loss: 5.9993 - val_mae: 1.8514 - val_mse: 5.9993\n",
            "Epoch 932/1000\n",
            "250/250 [==============================] - 0s 115us/sample - loss: 2.8432 - mae: 1.0754 - mse: 2.8432 - val_loss: 7.1599 - val_mae: 1.9228 - val_mse: 7.1599\n",
            "Epoch 933/1000\n",
            "250/250 [==============================] - 0s 92us/sample - loss: 2.9244 - mae: 1.0602 - mse: 2.9244 - val_loss: 6.4824 - val_mae: 1.8406 - val_mse: 6.4824\n",
            "Epoch 934/1000\n",
            "250/250 [==============================] - 0s 88us/sample - loss: 2.7389 - mae: 0.9831 - mse: 2.7389 - val_loss: 7.4693 - val_mae: 1.9381 - val_mse: 7.4693\n",
            "Epoch 935/1000\n",
            "250/250 [==============================] - 0s 90us/sample - loss: 2.7353 - mae: 1.0064 - mse: 2.7353 - val_loss: 7.0535 - val_mae: 1.9013 - val_mse: 7.0535\n",
            "Epoch 936/1000\n",
            "250/250 [==============================] - 0s 100us/sample - loss: 2.8976 - mae: 1.0841 - mse: 2.8976 - val_loss: 5.7007 - val_mae: 1.8185 - val_mse: 5.7007\n",
            "Epoch 937/1000\n",
            "250/250 [==============================] - 0s 94us/sample - loss: 2.7414 - mae: 1.0397 - mse: 2.7414 - val_loss: 7.0518 - val_mae: 1.9398 - val_mse: 7.0518\n",
            "Epoch 938/1000\n",
            "250/250 [==============================] - 0s 81us/sample - loss: 2.7523 - mae: 1.0217 - mse: 2.7523 - val_loss: 7.1128 - val_mae: 1.9288 - val_mse: 7.1128\n",
            "Epoch 939/1000\n",
            "250/250 [==============================] - 0s 101us/sample - loss: 2.7180 - mae: 1.0148 - mse: 2.7180 - val_loss: 8.1293 - val_mae: 2.0349 - val_mse: 8.1293\n",
            "Epoch 940/1000\n",
            "250/250 [==============================] - 0s 91us/sample - loss: 2.8794 - mae: 1.0580 - mse: 2.8794 - val_loss: 7.4774 - val_mae: 1.9647 - val_mse: 7.4774\n",
            "Epoch 941/1000\n",
            "250/250 [==============================] - 0s 88us/sample - loss: 2.7856 - mae: 1.0267 - mse: 2.7856 - val_loss: 6.0556 - val_mae: 1.8328 - val_mse: 6.0556\n",
            "Epoch 942/1000\n",
            "250/250 [==============================] - 0s 98us/sample - loss: 2.6696 - mae: 0.9689 - mse: 2.6696 - val_loss: 6.2545 - val_mae: 1.8483 - val_mse: 6.2545\n",
            "Epoch 943/1000\n",
            "250/250 [==============================] - 0s 98us/sample - loss: 2.7902 - mae: 1.0668 - mse: 2.7902 - val_loss: 6.7787 - val_mae: 1.9211 - val_mse: 6.7787\n",
            "Epoch 944/1000\n",
            "250/250 [==============================] - 0s 111us/sample - loss: 2.7020 - mae: 1.0050 - mse: 2.7020 - val_loss: 6.5146 - val_mae: 1.8573 - val_mse: 6.5146\n",
            "Epoch 945/1000\n",
            "250/250 [==============================] - 0s 111us/sample - loss: 2.6982 - mae: 1.0247 - mse: 2.6982 - val_loss: 10.0392 - val_mae: 2.3373 - val_mse: 10.0392\n",
            "Epoch 946/1000\n",
            "250/250 [==============================] - 0s 96us/sample - loss: 2.9503 - mae: 1.0578 - mse: 2.9503 - val_loss: 7.2835 - val_mae: 1.9385 - val_mse: 7.2835\n",
            "Epoch 947/1000\n",
            "250/250 [==============================] - 0s 88us/sample - loss: 2.9279 - mae: 1.0433 - mse: 2.9279 - val_loss: 6.5458 - val_mae: 1.8829 - val_mse: 6.5458\n",
            "Epoch 948/1000\n",
            "250/250 [==============================] - 0s 100us/sample - loss: 2.5495 - mae: 0.9916 - mse: 2.5495 - val_loss: 8.6717 - val_mae: 2.1528 - val_mse: 8.6717\n",
            "Epoch 949/1000\n",
            "250/250 [==============================] - 0s 106us/sample - loss: 2.7583 - mae: 1.0308 - mse: 2.7583 - val_loss: 6.6358 - val_mae: 1.8702 - val_mse: 6.6358\n",
            "Epoch 950/1000\n",
            "250/250 [==============================] - 0s 82us/sample - loss: 2.7275 - mae: 1.0278 - mse: 2.7275 - val_loss: 6.7943 - val_mae: 1.8954 - val_mse: 6.7943\n",
            "Epoch 951/1000\n",
            "250/250 [==============================] - 0s 93us/sample - loss: 2.5504 - mae: 0.9440 - mse: 2.5504 - val_loss: 6.0765 - val_mae: 1.8399 - val_mse: 6.0765\n",
            "Epoch 952/1000\n",
            "250/250 [==============================] - 0s 96us/sample - loss: 2.9086 - mae: 1.0811 - mse: 2.9086 - val_loss: 6.8797 - val_mae: 1.9274 - val_mse: 6.8797\n",
            "Epoch 953/1000\n",
            "250/250 [==============================] - 0s 295us/sample - loss: 2.6031 - mae: 0.9734 - mse: 2.6031 - val_loss: 6.8345 - val_mae: 1.9148 - val_mse: 6.8345\n",
            "Epoch 954/1000\n",
            "250/250 [==============================] - 0s 82us/sample - loss: 2.7852 - mae: 1.0151 - mse: 2.7852 - val_loss: 7.0454 - val_mae: 1.9498 - val_mse: 7.0454\n",
            "Epoch 955/1000\n",
            "250/250 [==============================] - 0s 80us/sample - loss: 2.7581 - mae: 0.9935 - mse: 2.7581 - val_loss: 6.6867 - val_mae: 1.9024 - val_mse: 6.6867\n",
            "Epoch 956/1000\n",
            "250/250 [==============================] - 0s 93us/sample - loss: 2.7162 - mae: 0.9954 - mse: 2.7162 - val_loss: 6.0406 - val_mae: 1.8475 - val_mse: 6.0406\n",
            "Epoch 957/1000\n",
            "250/250 [==============================] - 0s 90us/sample - loss: 2.7632 - mae: 1.0108 - mse: 2.7632 - val_loss: 6.8401 - val_mae: 1.9047 - val_mse: 6.8401\n",
            "Epoch 958/1000\n",
            "250/250 [==============================] - 0s 92us/sample - loss: 2.7043 - mae: 1.0509 - mse: 2.7043 - val_loss: 6.3879 - val_mae: 1.8348 - val_mse: 6.3879\n",
            "Epoch 959/1000\n",
            "250/250 [==============================] - 0s 80us/sample - loss: 2.5314 - mae: 1.0056 - mse: 2.5314 - val_loss: 6.9637 - val_mae: 1.8953 - val_mse: 6.9637\n",
            "Epoch 960/1000\n",
            "250/250 [==============================] - 0s 107us/sample - loss: 2.8279 - mae: 0.9863 - mse: 2.8279 - val_loss: 7.1221 - val_mae: 1.9805 - val_mse: 7.1221\n",
            "Epoch 961/1000\n",
            "250/250 [==============================] - 0s 78us/sample - loss: 2.7883 - mae: 1.0105 - mse: 2.7883 - val_loss: 7.9932 - val_mae: 2.0394 - val_mse: 7.9932\n",
            "Epoch 962/1000\n",
            "250/250 [==============================] - 0s 91us/sample - loss: 2.6893 - mae: 0.9923 - mse: 2.6893 - val_loss: 6.5957 - val_mae: 1.8642 - val_mse: 6.5957\n",
            "Epoch 963/1000\n",
            "250/250 [==============================] - 0s 91us/sample - loss: 2.7171 - mae: 1.0035 - mse: 2.7171 - val_loss: 7.7356 - val_mae: 2.0095 - val_mse: 7.7356\n",
            "Epoch 964/1000\n",
            "250/250 [==============================] - 0s 104us/sample - loss: 2.6522 - mae: 1.0144 - mse: 2.6522 - val_loss: 7.0943 - val_mae: 1.9795 - val_mse: 7.0943\n",
            "Epoch 965/1000\n",
            "250/250 [==============================] - 0s 91us/sample - loss: 2.7310 - mae: 1.0225 - mse: 2.7310 - val_loss: 7.4949 - val_mae: 1.9761 - val_mse: 7.4949\n",
            "Epoch 966/1000\n",
            "250/250 [==============================] - 0s 98us/sample - loss: 2.7133 - mae: 1.0177 - mse: 2.7133 - val_loss: 6.1317 - val_mae: 1.8437 - val_mse: 6.1317\n",
            "Epoch 967/1000\n",
            "250/250 [==============================] - 0s 103us/sample - loss: 2.7249 - mae: 0.9938 - mse: 2.7249 - val_loss: 7.4570 - val_mae: 1.9939 - val_mse: 7.4570\n",
            "Epoch 968/1000\n",
            "250/250 [==============================] - 0s 108us/sample - loss: 2.7613 - mae: 1.0020 - mse: 2.7613 - val_loss: 6.6664 - val_mae: 1.8803 - val_mse: 6.6664\n",
            "Epoch 969/1000\n",
            "250/250 [==============================] - 0s 114us/sample - loss: 2.6033 - mae: 0.9762 - mse: 2.6033 - val_loss: 6.0397 - val_mae: 1.8143 - val_mse: 6.0397\n",
            "Epoch 970/1000\n",
            "250/250 [==============================] - 0s 108us/sample - loss: 2.6487 - mae: 1.0316 - mse: 2.6487 - val_loss: 6.5604 - val_mae: 1.8683 - val_mse: 6.5604\n",
            "Epoch 971/1000\n",
            "250/250 [==============================] - 0s 102us/sample - loss: 2.6477 - mae: 1.0320 - mse: 2.6477 - val_loss: 6.1407 - val_mae: 1.8338 - val_mse: 6.1407\n",
            "Epoch 972/1000\n",
            "250/250 [==============================] - 0s 95us/sample - loss: 2.7373 - mae: 0.9804 - mse: 2.7373 - val_loss: 6.4060 - val_mae: 1.8398 - val_mse: 6.4060\n",
            "Epoch 973/1000\n",
            "250/250 [==============================] - 0s 94us/sample - loss: 2.8064 - mae: 1.0064 - mse: 2.8064 - val_loss: 6.4200 - val_mae: 1.8511 - val_mse: 6.4200\n",
            "Epoch 974/1000\n",
            "250/250 [==============================] - 0s 94us/sample - loss: 2.6929 - mae: 0.9804 - mse: 2.6929 - val_loss: 6.2396 - val_mae: 1.8312 - val_mse: 6.2396\n",
            "Epoch 975/1000\n",
            "250/250 [==============================] - 0s 111us/sample - loss: 2.5611 - mae: 1.0188 - mse: 2.5611 - val_loss: 6.9721 - val_mae: 1.9594 - val_mse: 6.9721\n",
            "Epoch 976/1000\n",
            "250/250 [==============================] - 0s 102us/sample - loss: 2.9446 - mae: 1.0883 - mse: 2.9446 - val_loss: 6.2879 - val_mae: 1.8832 - val_mse: 6.2879\n",
            "Epoch 977/1000\n",
            "250/250 [==============================] - 0s 104us/sample - loss: 2.7953 - mae: 1.0574 - mse: 2.7953 - val_loss: 6.4558 - val_mae: 1.8493 - val_mse: 6.4558\n",
            "Epoch 978/1000\n",
            "250/250 [==============================] - 0s 104us/sample - loss: 2.6619 - mae: 0.9763 - mse: 2.6619 - val_loss: 7.6923 - val_mae: 1.9982 - val_mse: 7.6923\n",
            "Epoch 979/1000\n",
            "250/250 [==============================] - 0s 104us/sample - loss: 2.7379 - mae: 1.0173 - mse: 2.7379 - val_loss: 6.8377 - val_mae: 1.9561 - val_mse: 6.8377\n",
            "Epoch 980/1000\n",
            "250/250 [==============================] - 0s 87us/sample - loss: 2.6097 - mae: 0.9987 - mse: 2.6097 - val_loss: 7.6569 - val_mae: 1.9898 - val_mse: 7.6569\n",
            "Epoch 981/1000\n",
            "250/250 [==============================] - 0s 107us/sample - loss: 2.7604 - mae: 1.0068 - mse: 2.7604 - val_loss: 6.2150 - val_mae: 1.8691 - val_mse: 6.2150\n",
            "Epoch 982/1000\n",
            "250/250 [==============================] - 0s 89us/sample - loss: 2.5044 - mae: 0.9615 - mse: 2.5044 - val_loss: 6.1648 - val_mae: 1.8336 - val_mse: 6.1648\n",
            "Epoch 983/1000\n",
            "250/250 [==============================] - 0s 101us/sample - loss: 2.8638 - mae: 1.0558 - mse: 2.8638 - val_loss: 7.1513 - val_mae: 1.9287 - val_mse: 7.1513\n",
            "Epoch 984/1000\n",
            "250/250 [==============================] - 0s 87us/sample - loss: 2.7521 - mae: 0.9894 - mse: 2.7521 - val_loss: 6.5617 - val_mae: 1.8812 - val_mse: 6.5617\n",
            "Epoch 985/1000\n",
            "250/250 [==============================] - 0s 104us/sample - loss: 2.7623 - mae: 1.0059 - mse: 2.7623 - val_loss: 6.9166 - val_mae: 1.9483 - val_mse: 6.9166\n",
            "Epoch 986/1000\n",
            "250/250 [==============================] - 0s 127us/sample - loss: 2.5704 - mae: 0.9871 - mse: 2.5704 - val_loss: 7.0887 - val_mae: 1.9675 - val_mse: 7.0887\n",
            "Epoch 987/1000\n",
            "250/250 [==============================] - 0s 110us/sample - loss: 2.6351 - mae: 0.9697 - mse: 2.6351 - val_loss: 6.9410 - val_mae: 1.9024 - val_mse: 6.9410\n",
            "Epoch 988/1000\n",
            "250/250 [==============================] - 0s 94us/sample - loss: 2.6198 - mae: 0.9968 - mse: 2.6198 - val_loss: 8.2263 - val_mae: 2.1026 - val_mse: 8.2263\n",
            "Epoch 989/1000\n",
            "250/250 [==============================] - 0s 92us/sample - loss: 2.7829 - mae: 1.0033 - mse: 2.7829 - val_loss: 7.0173 - val_mae: 1.9534 - val_mse: 7.0173\n",
            "Epoch 990/1000\n",
            "250/250 [==============================] - 0s 86us/sample - loss: 2.6719 - mae: 0.9928 - mse: 2.6719 - val_loss: 7.0770 - val_mae: 1.9936 - val_mse: 7.0770\n",
            "Epoch 991/1000\n",
            "250/250 [==============================] - 0s 107us/sample - loss: 2.5779 - mae: 0.9804 - mse: 2.5779 - val_loss: 6.1214 - val_mae: 1.8269 - val_mse: 6.1214\n",
            "Epoch 992/1000\n",
            "250/250 [==============================] - 0s 92us/sample - loss: 2.5986 - mae: 0.9987 - mse: 2.5986 - val_loss: 6.4147 - val_mae: 1.8586 - val_mse: 6.4147\n",
            "Epoch 993/1000\n",
            "250/250 [==============================] - 0s 87us/sample - loss: 2.6087 - mae: 1.0009 - mse: 2.6087 - val_loss: 7.2896 - val_mae: 2.0137 - val_mse: 7.2896\n",
            "Epoch 994/1000\n",
            "250/250 [==============================] - 0s 108us/sample - loss: 2.5740 - mae: 0.9620 - mse: 2.5740 - val_loss: 6.1241 - val_mae: 1.9089 - val_mse: 6.1241\n",
            "Epoch 995/1000\n",
            "250/250 [==============================] - 0s 113us/sample - loss: 2.7348 - mae: 1.0054 - mse: 2.7348 - val_loss: 6.7769 - val_mae: 1.9051 - val_mse: 6.7769\n",
            "Epoch 996/1000\n",
            "250/250 [==============================] - 0s 137us/sample - loss: 2.6015 - mae: 1.0142 - mse: 2.6015 - val_loss: 6.3302 - val_mae: 1.8743 - val_mse: 6.3302\n",
            "Epoch 997/1000\n",
            "250/250 [==============================] - 0s 94us/sample - loss: 2.4715 - mae: 0.9572 - mse: 2.4715 - val_loss: 6.5769 - val_mae: 1.8619 - val_mse: 6.5769\n",
            "Epoch 998/1000\n",
            "250/250 [==============================] - 0s 104us/sample - loss: 2.6577 - mae: 1.0240 - mse: 2.6577 - val_loss: 7.6027 - val_mae: 2.0089 - val_mse: 7.6027\n",
            "Epoch 999/1000\n",
            "250/250 [==============================] - 0s 96us/sample - loss: 2.9658 - mae: 1.0369 - mse: 2.9658 - val_loss: 6.6967 - val_mae: 1.9283 - val_mse: 6.6967\n",
            "Epoch 1000/1000\n",
            "250/250 [==============================] - 0s 91us/sample - loss: 2.6161 - mae: 1.0009 - mse: 2.6161 - val_loss: 7.6490 - val_mae: 2.0410 - val_mse: 7.6490\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Llh5LqyxrVwk",
        "colab_type": "code",
        "outputId": "34ae21ed-7fc2-4625-bc62-a648af4c92df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "### START CODING HERE ### \n",
        "# Evaluate nn_reg1 using .evaluate() method on X_test, y_test and verbose=2\n",
        "loss1, mae1, mse1 = nn_reg1.evaluate(X_test, y_test, verbose = 2)\n",
        "### END CODING HERE ###\n",
        "\n",
        "print(\"Testing set Mean Abs Error: {:5.2f} MPG\".format(mae1))"
      ],
      "execution_count": 295,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
            "79/79 - 0s - loss: 6.2805 - mae: 1.8779 - mse: 6.2805\n",
            "Testing set Mean Abs Error:  1.88 MPG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "im1kpCUsrVwm",
        "colab_type": "code",
        "outputId": "1815a90b-3f25-4acd-d265-166e16948a44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# Your numbers might be a little different due to the randomness!\n",
        "hist1 = pd.DataFrame(nn_reg1_history.history)\n",
        "hist1['epoch'] = nn_reg1_history.epoch\n",
        "hist1.tail()"
      ],
      "execution_count": 296,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss</th>\n",
              "      <th>mae</th>\n",
              "      <th>mse</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>val_mae</th>\n",
              "      <th>val_mse</th>\n",
              "      <th>epoch</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>2.601517</td>\n",
              "      <td>1.014175</td>\n",
              "      <td>2.601516</td>\n",
              "      <td>6.330175</td>\n",
              "      <td>1.874348</td>\n",
              "      <td>6.330175</td>\n",
              "      <td>995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>2.471494</td>\n",
              "      <td>0.957227</td>\n",
              "      <td>2.471494</td>\n",
              "      <td>6.576865</td>\n",
              "      <td>1.861933</td>\n",
              "      <td>6.576865</td>\n",
              "      <td>996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>2.657730</td>\n",
              "      <td>1.023978</td>\n",
              "      <td>2.657730</td>\n",
              "      <td>7.602674</td>\n",
              "      <td>2.008889</td>\n",
              "      <td>7.602674</td>\n",
              "      <td>997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>2.965806</td>\n",
              "      <td>1.036911</td>\n",
              "      <td>2.965806</td>\n",
              "      <td>6.696673</td>\n",
              "      <td>1.928331</td>\n",
              "      <td>6.696673</td>\n",
              "      <td>998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>2.616071</td>\n",
              "      <td>1.000939</td>\n",
              "      <td>2.616071</td>\n",
              "      <td>7.648971</td>\n",
              "      <td>2.040980</td>\n",
              "      <td>7.648971</td>\n",
              "      <td>999</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         loss       mae       mse  val_loss   val_mae   val_mse  epoch\n",
              "995  2.601517  1.014175  2.601516  6.330175  1.874348  6.330175    995\n",
              "996  2.471494  0.957227  2.471494  6.576865  1.861933  6.576865    996\n",
              "997  2.657730  1.023978  2.657730  7.602674  2.008889  7.602674    997\n",
              "998  2.965806  1.036911  2.965806  6.696673  1.928331  6.696673    998\n",
              "999  2.616071  1.000939  2.616071  7.648971  2.040980  7.648971    999"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 296
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPSqMOEnrVwo",
        "colab_type": "code",
        "outputId": "d53158bd-0d56-4114-84fe-75a1839bf947",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 549
        }
      },
      "source": [
        "# Your plots might be slightly different, i.e. they should look very similar to the plots below!\n",
        "def plot_history(history):\n",
        "  hist = pd.DataFrame(history.history)\n",
        "  hist['epoch'] = history.epoch\n",
        "\n",
        "  plt.figure()\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Mean Abs Error [MPG]')\n",
        "  plt.plot(hist['epoch'], hist['mae'],\n",
        "           label='Train Error')\n",
        "  plt.plot(hist['epoch'], hist['val_mae'],\n",
        "           label = 'Val Error')\n",
        "  plt.ylim([0,5])\n",
        "  plt.legend()\n",
        "\n",
        "  plt.figure()\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Mean Square Error [$MPG^2$]')\n",
        "  plt.plot(hist['epoch'], hist['mse'],\n",
        "           label='Train Error')\n",
        "  plt.plot(hist['epoch'], hist['val_mse'],\n",
        "           label = 'Val Error')\n",
        "  plt.ylim([0,20])\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "plot_history(nn_reg1_history)"
      ],
      "execution_count": 297,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEKCAYAAAAYd05sAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deXhTVfrA8e+bdKWFln2VRURZBBEZ\nERER3FBEx2VUxN0ZR2dxmxnX8ec2KuqM4zruuwjuG6goiyKKIEtBVtmhULZCWwpdk/P74yRN2qZN\n2iZNSd7P8+RJcnNz77lJ++bc9yxXjDEopZSKPY5oF0AppVRkaIBXSqkYpQFeKaVilAZ4pZSKURrg\nlVIqRmmAV0qpGJUQyY2LyEZgH+ACyo0xgyO5P6WUUj4RDfAeI40xuxthP0oppfxoikYppWKURHIk\nq4hsAPYCBnjBGPNigHWuBa4FSEtLO6Z3794N2uf+vF2kHcimrHVvEpNTG7QtpZRq6hYuXLjbGNM2\n0GuRDvCdjTFbRaQd8A3wV2PM7JrWHzx4sFmwYEGD9vnTpy9w3OJbybl0Nh0PO6pB21JKqaZORBbW\n1L4Z0RSNMWar534n8DFwbCT3B4DD6dm3O+K7UkqppixiAV5E0kSkufcxcBqwLFL7q9ivQwAwLlek\nd6WUUk1aJHvRtAc+FhHvft4xxnwVwf0BIGIPybjLI70rpZRq0iIW4I0x64FGT4IbcXj339i7Viqu\nlZWVkZ2dTXFxcbSLEpNSUlLo0qULiYmJIb+nMfrBNy5PgEdr8Eo1quzsbJo3b0737t3xnLmrMDHG\nkJubS3Z2Nj169Aj5fTHXD14cnhSNNrIq1aiKi4tp3bq1BvcIEBFat25d57Oj2Avwnj8u49ZGVqUa\nmwb3yKnPZxtzAR5vDV570Sil4lwMBnh7SG5N0SgVV3Jzcxk4cCADBw6kQ4cOdO7cueJ5aWlpSNu4\n6qqrWL16dcj7fPnll2nbtm3FfgYOHFin90dazDWyitiBTmiKRqm40rp1a7KysgC49957SU9P5+9/\n/3uldYwxGGNwOALXbV977bU673f8+PE88cQTNb5eXl5OQoIv1AYrgz+Xy4XT6axzmbxirgYvTk8N\n3q01eKUUrF27lr59+zJ+/Hj69etHTk4O1157LYMHD6Zfv37cf//9FeuecMIJZGVlUV5eTmZmJrff\nfjtHHXUUQ4cOZefOnSHvc/r06Zx00kmcddZZ9O/fP2AZ3n77bfr378+RRx7JnXfeCVCx35tuuokB\nAwYwf/78Bh17zNbgdaCTUtFz3+fLWbGtIKzb7NupBfeM7Vev965atYo333yTwYPtlC0TJkygVatW\nlJeXM3LkSC644AL69u1b6T35+fmMGDGCCRMmcMstt/Dqq69y++23V9v2xIkT+fbbbyuee4PyggUL\nWLFiBV27dmXt2rWVypCdnc0///lPFixYQEZGBqeccgpTpkxh9OjR5Ofnc+KJJ9Z6VhCqmKvBO5ze\nAK8pGqWU1bNnz4rgDjBp0iQGDRrEoEGDWLlyJStWrKj2ntTUVM444wwAjjnmGDZu3Bhw2+PHjycr\nK6vilpSUBMDQoUPp2rVrwDLMmzePUaNG0aZNGxITE7nkkkuYPdvOw5iUlMS5554bluOOvRq8J8C7\ntBeNUlFT35p2pKSlpVU8XrNmDU8++STz588nMzOTSy+9NGD/cm+gBnA6nZSX1y0r4L/PQM9rkpqa\nGrbuprFXg9dGVqVULQoKCmjevDktWrQgJyeHadOmNXoZhgwZwqxZs8jNzaW8vJzJkyczYsSIsO8n\n5mrw3hSNNrIqpQIZNGgQffv2pXfv3nTr1o1hw4Y1aHtVc/AvvPBC0Pd06dKFBx54gJNOOgljDGPH\njmXMmDF1PksIJqIX/KircFzwY2XWXPp8Mpplw57iyFOvCFPJlFLBrFy5kj59+kS7GDEt0GcctQt+\nRENFI6vm4JVScS7mArx3sjEdyaqUincxF+C9NXhtZFVKxbuYC/BO7QevlFJADAZ4h3c2SQ3wSqk4\nF4MB3nPJPg3wSqk4F3sB3ulpZNV+8ErFlZEjR1YbtPTEE09w/fXX1/q+9PT0gMudTmelaYAnTJgQ\ntrI2lpgb6OTURlal4tK4ceOYPHkyp59+esWyyZMn8+ijj9Zre6mpqRXTD9ek6nS+VacGrkmo6zVU\nDNbgPY2sRgO8UvHkggsuYOrUqRUX99i4cSPbtm1j+PDhFBYWcvLJJzNo0CD69+/Pp59+Wu/9dO/e\nndtuu41Bgwbx/vvvc9JJJ3HTTTcxePBgnnzySTZu3MioUaMYMGAAJ598Mps3bwbgyiuv5LrrrmPI\nkCHceuutYTnmYGKuBu9tZNUavFJR9OXtsP2X8G6zQ384o+Y0SatWrTj22GP58ssvOeecc5g8eTIX\nXnghIkJKSgoff/wxLVq0YPfu3Rx33HGcffbZtU7qVVRUxMCBAyue33HHHVx00UWAvbjIokWLAHj+\n+ecpLS3FOwp/7NixXHHFFVxxxRW8+uqr3HDDDXzyyScAZGdn8+OPPzboIh51EXMBPkG7SSoVt7xp\nGm+Af+WVVwB7FaU777yT2bNn43A42Lp1Kzt27KBDhw41bqu2FI030Ad6PnfuXD766CMALrvsskq1\n9d/97neNFtwhBgO8VAR4bWRVKmpqqWlH0jnnnMPNN9/MokWLOHDgAMcccwxgJwTbtWsXCxcuJDEx\nke7duwecIjhU9Z0KONT1wiXmcvAVv46ag1cq7qSnpzNy5Eiuvvpqxo0bV7E8Pz+fdu3akZiYyKxZ\ns9i0aVPEynD88cczefJkwP6wDB8+PGL7CibmavDelmmtwSsVn8aNG8e5555bEWTBXnVp7Nix9O/f\nn8GDB9O7d++g26magx89enRIXSWffvpprrrqKh577DHatm1brwt5h0vMBfiKkaxag1cqLv32t7+l\n6jTobdq0Ye7cuQHXLywsDLi8pqvCVb10n/9c8ADdunVj5syZ1d73+uuvBy5wBMVuikZr8EqpOBdz\nAd47VYHm4JVS8S7mArw4HLiMaD94paKgKV0hLtbU57ONuQAP4MKhAV6pRpaSkkJubq4G+QgwxpCb\nm0tKSkqd3hdzjawALpyaolGqkXXp0oXs7Gx27doV7aLEpJSUFLp06VKn98RkgHfjQDTAK9WoEhMT\n6dGjR7SLofzEZopGNEWjlFIRD/Ai4hSRxSIyJdL78tIUjVJKNU4N/kZgZSPsp4KmaJRSKsIBXkS6\nAGOAlyO5n6rcOBBN0Sil4lyka/BPALcCNQ4rFZFrRWSBiCwIV+u7TdHoSFalVHyLWIAXkbOAncaY\nhbWtZ4x50Rgz2BgzuG3btmHZt1sciCkPy7aUUupgFcka/DDgbBHZCEwGRonI2xHcXwWDA9EavFIq\nzkUswBtj7jDGdDHGdAcuBmYaYy6N1P78uUR70SilVEz2g3fjwKGNrEqpONcoI1mNMd8C3zbGvgAM\nTu0mqZSKezFZg3eJE6m5445SSsWFmAzwRhyIW3vRKKXiW0wGeDcOrcErpeJebAZ4ceLQHLxSKs7F\nZIA3oo2sSikVmwEeBw4d6KSUinO1dpMUkVYhbMNtjMkLU3nCwi1OnO7SaBdDKaWiKlg/+G2em9Sy\njhPoGrYShYERJw40RaOUim/BAvxKY8zRta0gIovDWJ6wMOLQRlalVNwLloMfGsI2QlmnUbklAacG\neKVUnKu1Bm+MKfZ/LiKdsSkZgG3GmPKq6zQFbkcCDnSgk1IqvgVrZL0DSDTG3O9ZNBfIA5KAN4CH\nI1u8+jGORBJMWbSLoZRSURUsRfM74D9+z3ONMQOAfthL8TVJbkeSpmiUUnEvaD94Y8x+v6dPepa5\ngNRIFaqhjCOBRE3RKKXiXLAAny4iid4nxpjXAUQkGWgRwXI1iHEkkoCmaJRS8S1YgP8AeEFEmnkX\niEga8LzntSbJ5uA1RaOUim/BAvzdwE5gs4gsFJFFwEZgh+e1Jsk4kzRFo5SKe8G6SbqA20XkPuAw\nz+K1xpiiiJesIRwJJEk5GANS2yBcpZSKXbXW4EWkl4h8CvwM3AnsafLBHVuDBzB60Q+lVBwLlqJ5\nFZgCnA8sAp6OeInCwWnbhctKS6JcEKWUip5gAb65MeYlY8xqY8xjQPdGKFODiQZ4pZQKOtlYiogc\njW82yVT/58aYRZEsXL15UjSuMp0yWCkVv4IF+O3A4zU8N8CoSBSqoRyeGnx5WZObJkcppRpNsF40\nJzVSOcLLG+DLdbCTUip+BZts7LzaXjfGfBTe4oSHOO1hlZdpLxqlVPwKlqL5AMjy3KDylZ0M0CQD\nvDPB1uBd5ZqDV0rFr2AB/jzgYmAA8CkwyRizNuKlaiBxeFM0WoNXSsWvWrtJGmM+McZcDIwA1gH/\nEZE5IjKiUUpXT44E+7vl0hy8UiqOBZ0u2KMYyAcKgHQgJWIlCgOnNwevAV4pFceCNbKOwqZojgWm\nA08aYxY0RsEawttN0u3SAK+Uil/BcvDTgaXAHCAZuFxELve+aIy5IYJlqzeHU1M0SikVLMBfje0t\nc1BxJnp70Wgjq1IqfgUb6PR6I5UjrJyaolFKqaDTBd8bbAOhrNPYvL1o3FqDV0rFsWApmt+LSEEt\nrwu2Efbeai+IpACzsbn7BOADY8w99SxnnSR4BzppDV4pFceCBfiXgOYhrBNICTDKGFPouXD3HBH5\n0hjzU10LWVfekazGpTV4pVT8CpaDv6++GzbGGKDQ8zTRc2uUBtuKqQo0wCul4lioA53qRUScIpKF\nvXD3N8aYeQHWuVZEFojIgl27doVlvwkVNXhN0Sil4ldEA7wxxmWMGQh0AY4VkSMDrPOiMWawMWZw\n27Ztw7LfxIpukhrglVLxK2iA99TCb27ITowxecAsYHRDthOqlORkAMrKNMArpeJX0ABvjHEB4+q6\nYRFpKyKZnsepwKnAqjqXsB6SEr1XdNIAr5SKX8F60Xj9ICLPAO8C+70Lg1yTtSPwhog4sT8k7xlj\nptS7pHUgekUnpZQKOcAP9Nzf77es1muyGmOWAkfXs1wN4/AOdNILfiil4ldIAd4YMzLSBQmrxFQA\njF50WykVx0LqRSMiGSLyuLc7o4j8R0QyIl24ekuwAV7Ki6JcEKWUip5Qu0m+CuwDLvTcCoDXIlWo\nBnM4KCEJKdMAr5SKX6Hm4HsaY873e36fZwBTk1XqSMbp0gCvlIpfodbgi0TkBO8TERkGNOnoWSYp\nOF2ag1dKxa9Qa/DXAW/65d33AldEpkjhUeZIIUEDvFIqjgUN8CLiAI4wxhwlIi0AjDG1TSHcJLic\nKSRqI6tSKo6FMpLVDdzqeVxwMAR3gHJnKsluDfBKqfgVag5+uoj8XUQOEZFW3ltES9ZAZYnppJkD\n2FmLlVIq/oSag7/Ic/9nv2UGODS8xQkfV1Jz0llHSbmblERntIujlFKNLtQc/KXGmB8aoTzhk9yC\n5nKAgqIyDfBKqbgUag7+mUYoS1g5UjNpzgHyDuh8NEqp+BRqDn6GiJwvIhLR0oRRQrNMksRFwb59\n0S6KUkpFRagB/o/A+0CJiBSIyD4RadK9aZLSMgEozM+NckmUUio6Qp1NsnmkCxJuyek2wBcV7o1y\nSZRSKjpqrcGLyKV+j4dVee0vkSpUOKQ2bwlAyb68KJdEKaWiI1iK5ha/x09Xee3qMJclrFKb2276\nZQe0Bq+Uik/BArzU8DjQ8yZFUuy0OeUHtAavlIpPwQK8qeFxoOdNS1IzAMqL9wdZUSmlYlOwRtbe\nIrIUW1vv6XmM53mTHcUKQGIaAO4SDfBKqfgULMD3aZRSRIKnBu8u1QCvlIpPtQZ4Y8ymxipI2Hmu\ny4oGeKVUnAp1oNPBx+Gg1JGClOmMkkqpCCnKgx+egiYaY2I3wAMuZyqJrmLyDpRFuyhKqVj0xT/g\nm7th/bfRLklAdQ7wItJSRAZEojDhZpLSSZMituw9EO2iKBUd2QuhOD/apYhd3s+2vCS65ahBSAFe\nRL4VkRaei3wsAl4SkccjW7QwSGtLG/LJ3qtXdlJxyO2Cl0fBxN9FuySxq2L+xYM7RZPhuVTfecCb\nxpghwCmRK1Z4JGZ2pJ3ksWWP1uBVHHKX2/vsBdEtRzw4yHPwCSLSEbgQmBLB8oRVYosOtHPka4pG\nxSe3K9oliANNekB/yAH+fmAasM4Y87OIHAqsiVyxwqR5BzIpZPsezUGqOGQ0wEdcE0/RhDpd8PvY\n+eC9z9cD50eqUGGT3h6Awt05US6IUlGgNfhG4AnwB3OKRkQOFZHPRWSXiOwUkU89tfimzRPgS/Zu\n5bMl26JcGKUamXF7H0S1GHFh0RvRLkFAoaZo3gHeAzoCnbC1+UmRKlTYZHQGoKPkMn+DXtlJxZmm\nXoPfuhBeG9NkuxiGxJuiWfM1FDS9SmSoAb6ZMeYtY0y55/Y2kBLJgoVFxiEAHJa8F0c0Lye7dZH9\nY46mojydtiHeNPUc/Oc3wqY5sGtV7esV7oRpd4GrPLTtusph+r2wv5Erde4Qy2cMrJvZKGmdYFd0\nauXp+/6liNwuIt1FpJuI3Ap8EfHSNVRqJiRncIt5izY/P0Zpudv32pafIXdd45TjpZHw0qjG2VdN\nHukGTw+ObhlU4/LW4CMZSIyBj66FzT/V470hrjf1bzD3GVg3wz7/+p/w4R8qr7NvO6yZbh+v+Rrm\n/Be+vLVyOXOW1L2MwfhXHEP9nJdMgrfOhax3wl+eKoLV4BcCC7DdI/8IzAK+Ba4HLopoycIlsysA\nNyR8wvdrdvmWv3IKPD0oSoWKkn1N7xQyprndtiIRLnmbYd+O0NdvjBp8cT4sfRcmXhi5fXhTON42\nhR+fhl/eq7zOK6fBxPNtkPUed5nfAMeFr8MLJ/p+BCIh1Bp83hZ7v2d95MriUWuAN8b0MMYc6rmv\ndAOOqO29InKIiMwSkRUislxEbgxryUOVeUjFw2XT3w79NC8U2xZDeWn4tqdiy7znbEVi3azwbO+J\n/vCfw0NfP9SA0xDeswRHQ6a1CkP6NM8z8a273G97fjXq7b/Y+3AFVWNg5eeV2zlcnliQuw5+fKbm\n93o/K+8P0bwX4YNrwlOuqruqy8pinSwirwDZQVYvB/5mjOkLHAf8WUT61rOc9de8Q8XDG3PvZ/47\n97Fnv19Q3r02tO24XfDe5XZuD4C9G+HFkyqfBirlb8cKe5+3OTr7d7uDr9NQ5cX2XpwN2EiIqQ0T\nwvG4SkG8AdRvfe9jhwNcZVBcUPl9Mx+E964IrRxgJxd791JY7Zep9p5pvHIafH0X7K1htnXvZ5W9\nALbMtz1w6pPiCkGo3SSPE5GngE3Ap8BsoHdt7zHG5BhjFnke7wNWAp0bVtx6aNam0tOiX79l0ANf\n+xYsnQwFObD6y+rv3bPeNu643baGsOJT+OAq+9qBPfY+2o2nqunyViSDNfDv2w6b5oZ//8FSNOHI\nzXsDvKMBAb623j4F23zBb/IlIZSnxPd5+x+fN8CLA96/EiYcUrkNbvajsOKT0MscaAI3l2fW2gO7\n7f2TAyDL09kwbws80gNWfeH7rDZ+D6+cCjuWQUGw+nL9BGtkfUhE1gAPAkuBo4Fdxpg3jDF7Q92J\niHT3vHdegNeuFZEFIrJg165dVV9uuLS2lZ6OcC5liPi12hsDb58Pky6unLMD+4cw9xnYvdr35TkT\nqx5A5ef3ZsBHfwxP2euicCcsfT/4eqrxhBo/nx8Or42u/37yswN3NawInAbmvwSvn+X3mhvuy7S9\nTRrCu9/aavDZC+CrOwL8oHie15ZKem4YlPgF02A/Sq4yv8blADV4ccAqz2wrTw/yVdRqsnWRzfnv\n320/5ycHwp4NNew7QLr2k+vs/Zz/QtEemDwO1n9X+z7DKFgN/vfADuA54C1jTC51HDUhIunAh8BN\nngnLKjHGvGiMGWyMGdy2bdvqG2govxSNV2fx/ZAUFJVi8j2NHt5+rLt+tbX6smLfm8o889k4qgT4\nQJZODrx82l1Qsi+UUtfdOxfCR78P/gcbr1xlNsi5msC1AVZOqdwouX9n/bflKoP/9oOPA1Qq/Gvw\nX/zd1hi9ivPs/dxn679vCK0G//LJ8NP/qn/2JoQAX1Tl77nc738yULB3ldibXcFW2l4dbdvLwJe+\n8aqti6bbbXvAff1PeKwnLH4b9m6AeS/Aph+qr79qCsz+d/XlWxfBgld8z729gRpBsADfEfgXMBZY\nJyJvAakiEtIUByKSiA3uE40xHzWopPXVpXrXwDTx/ZG89dNG9hp7ge6KRtNnfwP/7UvFb1neZigp\ntI+dnkP3Bnzq0E1q7jMw+7G6H0Mo8j2neMEa1ooLYHkdTkVjxfyXbJD7+WX7PJo/hO+OhzXTqqcm\nqnYAKM6HjXOqv/+R7r7HKz+39ys+q75eoNSHNy/vPX7vpS0DvTeUbsQVOfgQsr2uGgY01eVHd1tW\n5fcZYwNwRXlKfR0f1k63t81zbRoEqreHzH8RXhwZuCyf31B5XW9aZt5z9n1V/fQ/mPlA9eU5WdWX\nNZJgvWhcxpivjDFXAD2BT4AfgK0iUmsnThER4BVgpTEmenPHt+hU7Y/vlK6+oDzCsZT9xZ5/rA+v\nYd9bnjyfcftO6965ELYtso+dSbD8Y3h9TPV9+f9x1NRoEulRe64yeKgzLPQbOu3/w/PZX+D9K2DX\n6vDsr6zIXrIsnL2TvPbvhl8+qL68YJsdJBNKD6bNP9lg5s2LFhfYhvJHe8AyvzrHrIcb9dQZqP63\n4F87BZsifH0MFFXJhvo/97YJBcq3B1pW7klDHvAMAkqsYbzirAdtCqOmdETVMoeSg6/p+3LXIcD7\np7LKi+1n8ePTvmXZP0Npoe/5nCcqv79qBWv5x77/bfCdYX/7CCx+q/K6P/0v9HL6C+V/4/xXgq9T\nDyH3ojHGlBhjPjTGXAD0Ar4K8pZhwGXAKBHJ8tzObEBZ6++MRys9HZHzWsXjIx0bOcThS9k03/SN\nb8Vcvx423/yfvc9Zav/xvMr8piIu98vhv3q67Xe7fVnlsoTSE6AhivPsH/hXt/uW+dfkvN3EysI0\nhfL3j9tLltWUlmqIdy+FD6+p3vd76t9tv+Zgp7put/0e3jzbrzuf01ej8r/M2ncT7HpV5a6z7So7\nlte+r7Li6gG7oree5wf2y9tsesarakCffi/sXOl77v3beTnApRdK9vnOKmsSqBfNhtn23pv6cCb7\nXivK8/14eNfbH6RdLFgO3v9Mqerxes39H5QesGeh92bYH/X8bJu3r3XfxfDWbysv++Q6e6bmta+O\nEw2WFNgyf/tQ3d5Xm7waetP463Va+Pbnp16dV40xBcaYN4OsM8cYI8aYAcaYgZ5bdEa/HvsHuCcP\nfvtcw7dVtbax+1d7ipif7Wsx98pZAtOq/JH616bXfGNTB4veDG2Uael++48QiHe7RZ7cqsMvi+Zf\nZm9/4Ib0PV79pf1H/PwmX21pY4CcpNfyT3yphGAO7LGNjlmT7Kk12MarkkJYNdU+99ZMvcf8ywc2\nL1pVmWdqhu2/+FJX4vAFN28DeW01LG+DXKBRhzlLfTXchzraBjh/i9+2995T/XnP2/RMRfmqNOr/\n/BL87zjflBLeBv3cAF15H+4CDwfplBYoXTfpYnvv3Yf3M3C77GjnR7rbNijvZ7v7V9vLpybeikLu\nGs8gI1M5tfPYYb7Hxfm+/e7bDjs9P5rrZtgzBm/65dsJtl0hWI05Z2nw0akFW2t/varCnfbsLpzm\nBugTP3pC5edJaeHdp0dMX3S7EhE4alxktv3j0/YP8qvbqr+WX/UPzC/AT7zA1jY++6v9B/nydthc\nraORz0OdbCB5aZTth7/0verreBvP/E+ZA+U43S4bTGf/2waabVl2yHnV+Wr2bLDB3FuzApuSAVj4\nmu+fcMk7sLuGSwS8f4WtjYdi1RTYvtTX+wBsmuXzG203ue3L4FfvyaPns/zwGjsewe321VqL8ytf\nqs57VaM5/4WZ/7KPV3xmR5pu8EvN+H9W81/ynbkF6iHxwnB4yhPUjbv2kcKBvoPyYlgS4MznoU72\nOPzfE0qXxi9vs8fsrTXX1E1yyWT7mYEN0B9cA59c73v92d/4frg//TP85wh4I8DZDVTuT75zBcy4\n36Z2vH3A/cvw3FB7bK5yu01/B3J9nRxyQ7zUxMQIzFj+yqmVn18TwsjXvr+tvuzYa2tev+tQ3w/P\nkefD2Ccb1s20FiE1lsYMEbh5hacBFduFMtgpaENV6zlQS4pm3nP2dm+QC5RsXQhPHmUfdx0K71zk\nyzF7/0kq1eAD1OTK9sOnf4HVUys3DKW2suufej8kNaucBvnwGkhIrrnvf+FOaNOr9rIHsmU+dDra\n1lgDNQx+9Hvf4+n3+B7nLK3clnB/S+g5Ci772KYYNvv1Ld/iaRMp8QtIRXvsSFN/D7SBYTfZz9X/\nVN8b4MtLbENtn7E1H0/uuoqpqisE6uVSVhR4ubec/r1rQrlw9rzn7f2jPWxONzUz8Hr++9y/C5YF\naOeo2rtkw3fw/lUw6p/27+O9K+wP2qDLfes8d7zv8ZMD4LyXA+//nQDXiF0yyd4i5c8/2x+uukrN\nhIsn2e6Nf/ze/qh7OZOg37lw7gu28XbbYmjeERa8Cqc/BIePhrfPs+u27QN/mGkrc8NusGcwPz4N\nJ/4D2vUJzzEGEHKAF5Hjge7+7wmWpmmSMjrDlV/YL6HsQOWRaIC7w0Ac28PY6l21FrV7jaeWXEuK\nxFVmA7QIbPgeln8EZ/038Lr7cnynuuALSv4/XIFqj2/UEKDmedJYP78EV31ZOUcLwWviW+bDm+fA\njUshvUq3V2MqjxtY8ZmtxX70Bzj5/2D434L3AlrrV6P6bkL119fNtPcN6SXzwxP25i9nqf0HXvON\nTSdMu9P32s9+DWQPdYHSfdDz5MrvX/Zh9f18/5+ay1B1crra1g1k2l1QWEtqpT6Wf2QbJPdu9C3z\nb+Csyv+H2Z/3O2osKRnQNoQpHtof6ett45XWxlZavJWuq76y81vlZEH34ZDSwi5v2c3eALoOsfc9\nR8HJ90Dfc6B1T7vsAs/fSsej4O5cX6+8CAm1u+Nb2F40WYDf6AkOvgAP0H2Yve3ZYAOhOG0Nbdsi\nHIMuhWkrwFXKA2l3cvd+X3eD8NAAABviSURBVGPLI2UXc13CZ0xz/Yb2spcRzqXB91U1uG783jbS\nFtYyadQDbaDbCXDVVHjDMzjltAcDr7tqSuDlYKdhaHNY3Xop+HvtDDijDt065zzuC8DzX7S1uw+u\n9r3+88u2PSTrHduo9N5lvtfmv2xrzeFohF47w9fvOVy2LbJTUxwWoMFz6i2+x6WeXhih9HVeXkvP\n4T1Vuij++FTw7fkLFNybtfb1nqkv/+DemHqMqJxKq+r6udCio/17q/gBEcBAUnrldS96u3pF5YJX\nofdYSEiyz42xn1Vqy8rrdRtq7zNCGJQvAsNvqfn1CAd3ADEh5PZEZCXQ14SycgMMHjzYLFgQpSvA\n791oc5GXvGcbuvbvZPfNW1nz9QsM6NWDbR1PZXt+Eat3FLKrsIRlazYwcU+Ecvpet2+GCXY2TM59\nET6uJa8XyMWT7A9Y1kTYUktuvzEdfVn17meqZpe8Z7vpNlRaO7jue9tTJ5KpkLr4ywJ4Jkjngk6D\n4JgrYOCltjND4Q5bqek5ytcWc8sqG9zBpigf7wOtD4ML37J5/xZd4JblNo1Sut+mROa/ZGvmU26B\nI86A02uoQB0ERGShMSbgBxlqgH8fuMEYE9GLm0Y1wPvLXWcb+vqdW+tqbrfBcX8NeU7gnfKRXJLQ\ngJkEvbk/FVx6h5pTEuc8axsL6yM5o/JQ+bo65Dhf/r8+/rkL/lUl1ZXcwvYG8jaog/0hKNnnuThG\ngO6FR4yBce/YrqFvnlPz/s572Zda+c0fbKoOoHmnmhuRe58Fh55kGw7T2kHvM31tRKMfsak675nc\nGY9B7zG2fcdbO/7m/+CHJ2HwNTDoMht8syba12prj1o73X7vHY70LTPGptf6/86W+fO/wqAr4JBj\nA2+jatrwIBSOAD8LGAjMByo6+xpjamhar58mE+DrwHx5O5sTe9Bu+3ekrp1a6bVDi99mfUrlU8HX\nyk/nqoRplZa9Un4G1yTYyc5uKb2Ox5Oer32n4qh/KuPsp21DT21OfzhwkIiWs56AKTdVXz7iNvju\nEfv46q9tF8CqQ9vBdpG9r+Yf4goXv1N9Qqt7820PolD0Os1ebMLfrRuqd7tLb+9L0bXoYieacibb\nkZ5/Xwv/9nQtPPUB2yB3b4YN6iUFtvHuuD/ZC0asn2VHov7pR2jld4nknatg6wJIbGYHQp3+sE2X\nJafbtN0zx0DLHtChP6z8DMY+ZVOUgy63jejzX7J55sNP9/Ukum6Ofc+BXNug/9Z5cMhv4NCRMHC8\nL7Xh5SqzqZLDTrUzOHq7UAaaVjh7Ibw8Cv442+amwabzepxkU4yqVuEI8CMCLTfGhHXo38EY4P2V\n5W8nMSnF9mpxJsKVU3AX5rJhWw493xkGwAklTzIn2Tc1/gNl43ndNZpbEt6nJYXcV345LhyMarae\nF933hr+Qd+fa7mz+vQGunmZ7o3j7a9+TZ2uHr51p14XKPyrHXGW7SIbbGY/ZAJ3c3Aas9y63Xcj6\nXwgPtLbrtOhsa+QFW21geW6YrRGOusu+/us0O3px849w+We2VtvnLF+QHnGbPSVPSLGpOIATb7Vz\nsty1zdaAt2XZ4JWQAompsOgtGzCXf2x7s4yeYBvOyops982N3/vSBF/cCvNfsL1N0trCMVfa6Qbm\nv2h/XGc/Zrvr5iy1g4oGX22PpVkrmz7I6GL7/Cck+/rBb19mR2Q3a+X7rIrybEPfoSfV/pm63dWD\n6t6NkNE19DncY6CWG8saHOAby8Ee4GtVsI0rJ7zGmhZDeX14PomU8d4X03nJNYayGtq6r3V+zp2J\nvnxpoUmhLK0DWZmnM3JbgIE9Xsf+0dbW/HtedB1qG6pGemrmL5xog8Op9/vW8QZB/9Pikn02qGV0\nsV0Yi/Ntr4RnBtvam3cSpSun+qZv+Osi24964xzb02Danb6JrpKa+xoivfqcbWtuJ/6dGnnLdkMW\ntAoyEKWs2PYs6nyMb9ma6dC8va21en34e9td7cpaGqr9FeXZXiPH/6V645uX22UH33SOs6uFqagJ\nRw3+OOBpoA+QBDiB/caYFuEsaEwHeGDNjn20SU+mZZo9nd1fUk5JuZtWaUnMWLmDts2TcYhw1tOV\nJ5gaLKtoL3lMddsap+BmkKzBiZsznfPYYDrSXbYzyTWKNaYzq/81hqQEB/PmzaGbazMtuw/AtOxB\nSmqQ0XI/PW9TB6fcU/t6/jbPsz0KMrrYH5TMbtD/gurrPdgJ2veDcZPs6fuWn+y6oQbC/bm2N1CA\n2UGVimfhCPALgIuB94HBwOXA4caYsCZqYz3Ah+rxb37lqRl2NN81J/Rg4aa9ZG3JC/Iun+6tmzG0\nZxsmzbcz5yU4hHK3YdHdp7IkO4/+nTNwG8NdHy/jkfMH0CrNlz/dV1yGQ4S05DB34XKV29P8CI3Y\nUype1RbgQ/4vNsasFRGnMcYFvCYii4Em1BIXO37T3Z7+v3XNsQzv1ZYtew6weEseJWUuju6ayc59\nJfTt2IKB938T8P0bcw+wMdc3LWq52/6ID3qg+vrfrPiGn+86hX98sIQJ5w3guIdn0KFFCj/daQfq\n/LhuN4O7tSIpoXK+du66XMpcbk48PMQ5/Buhz69SqrJQa/CzgVOAl4HtQA5wpTHmqHAWRmvwPnv2\nl1aqWQfichv2l5azfGsBO/cVM/rIDrz54yYe/GJlre8LRa926dx9Vl8uf3U+44d05Z6x/Xjthw20\nTk/mnIGd6HWX7fWzcYLNu+cXlXGgtJySMjfd20Rm4iSlVHXhSNF0w17ZKQm4GcgA/meMCfGK1aHR\nAB8euYUlvPbDRi4f2o3mKYm4jaHfPZW7Zp43qDMfLarjTHseVw/rwas/2FkUvQG+7/99xYFSO8h5\nw8NnItrrQqlGEZZeNCKSCnQ1xoTpShHVaYCPnNzCEn7euJfr3l7IjL+NoGfbdHYUFDPkoYZdPqxP\nxxbcdWYfLn3FN1J20d2n0iotieIyFwVFZXy+NIcEh3DJkK4kOh2UudyUlrvDn+dXKg6FowY/Fvg3\nkGSM6SEiA4H7daDTwW/mqh0M6tqS3YUl5B0o499fr+an9Xs4tkcr5m8I/2XtLhnSlXfm2faBeXee\nTGazRJITbMPr/pJynpq5hrP6d6J/F9stcsueA/yyNZ8z+3cMe1mUigXhCPALgVHAt8aYoz3LfjHG\n9K/9nXWjAb5pyC8qIyM1katf/5mZq3byxQ3D6da6GU/NWMMLs9cz8fdDGP/yPE48vC2zfw3/dMsi\nsOZfZ5DgdDDogW/Ys7+Uj/50PKtybP/584/pXPGjcO9nyzm6aybnDOwcUruFUrEmHAH+J2PMcSKy\n2C/ALzXGDAhnQTXANy37istYvDmvoqeM220oKXeTmuTr6njla/MZ1rMNfzjxUKYuzaFdi2RSEpys\n21VIi9QErn49Mt/nlcd3Z+Gmvfyy1Q7K+ubmEzn1v7O5bkRPlm3N5/nLjiE9xBSQMYbFnm6oD01d\nydu/H0JKonbnVAeHcAT4V4AZwO3A+cANQKIx5rpa31hHGuBjz4ptBcxZu4uHvlhF/84Z3HRKL655\nI/zfcdWzid4dmvPmNcdSUuamc2YqWdl5/OP9JVx6XDeuGmbHFhx9SCYOh/DVsu1c97bvIibvXnsc\nq7bvY/yQriQ44+eiZ+rgFI4A3wy4CzgNO8nyNOABY0wNV9GtHw3wsevb1TsZ2rM1yQlOjDEYA2We\ny+slOR289P16Hvqi8lWEpt10Im/O3cjEeZsDbLH+jmjfnNU7bLpn/UNn8uoPG/jXVF/X0v6dM/hl\naz6dM1OZfetInI7APYI+X7KNds2TGXJo67CWT6m60Llo1EFhd2EJGam2W+eBEhct05Jwuw1fr9jB\n8m35PD3T9sq9+ZTDcRlD1pa8iLQB+OvTsQV79pcwul8Hbh3dm8temceizXlMv2UEpzxu59rzdhX1\nytqSx+Ht00lNdPKXSYu5cPAhjKgyIOy7X3exfFs+fzpJZ0tUDVPvAC8in9W2Ye1FoxpTcZmL575d\nx1XDupPZLInt+cVc/frPvHDZMTgdwta8Iqav3MFnWdvIya98cnnHGb15+MtVNWw5NE6H4HJX/3/p\n07EFB0rLue/sfvTvnMEx/5rO8F5t+N/4QfS/104f/M4fhtChRQqHtrVXF+p+u51aesE/T2HBxr2M\nPlLn2FH105AAvwvYAkwC5lHlQqI6XbBqivbsL6WgqIzubdJ49+fNHNujNT3apPHsrLVMmr+Z7L1F\nXDT4EN5dsCXs++6cmcrWvCIAzj26Mx8v9g0ma5OexLt/HErHjBT6/p8deNazbRrrdu1n5t9G4BAh\nJ7+YY7q1JCnBwabc/XTOTNV2AFWrhgR4J3AqMA4YAEwFJhljltf4pgbQAK8aw+rt+zisXTo79xXj\nFKGguLwi3QJ2lO/1I3qS6HQwc9VOOmWm8KeJiwhQeY+IUb3bUVruZs7a3bRslsic20bxxPRf2Zh7\ngOcvPYbFm/eyZ38pA7tmkpzgZEdBMe/M28zVw3rQISMFtzEVvYDmb9jDgdJyTjqiXeMUXjW6cI1k\nTcYG+seA+4wxz4SviJYGeBUtZS43OXnFOJ1C58zUgOvs2V/KLe9lsWxrPrsLSyu9NvsfI5m5agf3\nfr4i7GU7rF06a3cWAtCjTRobdu+vcd3eHZqzavs+Nk4Yw8JNezj/ubkADOnRikl/OA5HDQ3Gxhi2\n5hWR6HSwIqeAkfqDcNBoUID3BPYx2ODeHfgMeNUYU7+JTGqhAV4dLJ6asQa3MVw3oifFZS4ym9kB\nVjsLijn2oRkkOoUyl/3fapOezO7CkmrbiFSaCOClywfzhzer/y/dM7Yvp/ZtT5eWzQAb2Mtchonz\nNnGf34/TuofOrLH3kGpaGpKieRM4EvgCmGyMWRaZIloa4FUsKCp1YTCs37Wfdi2SyTtQxpe/bOev\now5jSXYeE+dt5saTe9G2eTKLNu2lT8cWfL50G//3qc18TvnrCRUXfXnvj0O59YMlHNYunV7tm/Pc\nt+vCUsYkp4Mz+ndgU+4BlmTnccJhbfh+ze6K1z/58zB6d2jOtOXbOb1fB7L3HqBLy2bVBoD9kp3P\ntvwiTu+njcTR0pAA7wa854P+Kwpg9IpOSoXPrR8s4b0F2cy/62SemrGGxZvzmHrD8ErrfLUsh+ve\nXsRNp/Tiielrat3e4e3T+XVHYdjKN2ZAR2459XCWbMljQJcMDmvXvKI30KK7T6V5SoJOIhcF2g9e\nqYNASbmLtTsL6dcpo9b1du0roW3zZErKXazK2UdhSTlLsvN49+ct7NpXUjFt89BDWzN3fS7tWySz\no6B6iqih/u+svtw/xZfW8eb/n71kEKlJDj5ctJX7z+5HUZmLTxZv5YtftvPFjcMxxuh00mGkAV6p\nOPLrjn28v2ALvxt8CH+euIhHLxjAuf/7kQnn9ad3xxbc//ly/jiiJ4O6tuQ3D07ntwM78UnWtoiU\n5ZBWqWzZU1TxfMyAjkxdmsOy+04nPTmBeetzuejFnzitb3tevNwXo7K25LFhdyGJTgdnDejE5twD\ntM9IrphkzquwpJzkBAeJcdyVVAO8UnGuqNRVaZI4r+IyF8kJDnrc8QUAy+47nR0FxTz21Wr6dGxB\n6/Qkhh3WhpH//jbsZeqYkVJpQNqY/h25c0wffly7m398sLRi+fe3jmT4o7O48vju3Ht2P3YXltAm\nPZmsLXn89tkfGNO/I8+OH8TKnAKKy1wc3bVl2MvalGmAV0rVatX2AnILSxl2WJuAr/+6Yx9vzd3E\nPWP78vqPG2mVlsTkn7dE5JoBNRnQJYOrh/Xgpnezqr12er/2TFu+A/BNHbFsaz5nPT2HMQM68uwl\ngyrW/X7NLlqnJZPZLJFOmamUudwc/s8v6depBW9cdSzJic6QZyJtCjTAK6XCLr+ojHs/W849Y/uy\neHMeQw5tRd6BMl6cvZ6zB3Zi7Y5Cbv3QVxOPZLdQf1cN687ZR3Xihe/W89Xy7QAVKSHwTRMB8NGf\njueHNbv5zze/Vizr1roZ3/1jJMYYvvhlOyf3aVet95DbbWocUxDItrwiWqQmRuSHQwO8Uipqdu0r\nIdEpZDZLorCknFU5BazfvZ9b/dIwAOOHdA37zKFeXVs149bRR/D9r7sr/cikJjo5vV/7am0Q/uMY\nzuzfgWcvGYQxsKuwhHs+Xc5Xy7fzwDn9GHJoazpkpJB/oIzhj85i6g0nBGwk7377VHq2TWPG304K\n+7FFJcCLyKvAWcBOY8yRobxHA7xS8cV73d4Hv1jJ/eccCQb+/sESvlmxg/MGdearZds5qksmnVum\n8sHC7KiW9cjOLRjeq23AsQiDu7Vkwaa9nNa3PUd0aM5NpxxeMVCstNymgAD+MvIwkhIc3HByr4r3\nNrRXUbQC/IlAIfCmBnilVH14g9/cdbmMe+knRhzelu9+3cXYozoxd10uuwtLSE5wUFLurnjPqgdG\ns2tfCcMfnRX28iQ5HZS63EHX+/D6oezaV8KaHYV8tmQba3ZWHo/gbScoLnNx0mPfMu7Yrtx4Sq9A\nmwoqaikaEekOTNEAr5RqiHKXm4e/XMU1J/QgwWHTPbn7S9iwez/H97QNw91vn8qgrpl89KdhgB1X\ncN7/fmT5toKK7Xx4/VBm/7qb5ikJlS7yUpuapppoiOtG9KRFagKPfrW6YlnV6wqEqkkHeBG5FrgW\noGvXrsds2rQpYuVRSsWuvAOlpCQ6KzWIFpfZwWNnPzMHt6kcRN1uw94DpbROT+bf01aTkZrIg1/Y\noL/o7lO57u2FbM8v5ptbTuTHtbncMHkx+4rLK96fluRkv2dQWThsePjMeqVqmnSA96c1eKVUJGzK\n3c/mPQcY3qttres9M3MNT81Yy68PnkGpJ+2TlGAHUXkvNfn+wi3sLiwlPTmBez5bzsPn9eeOj36p\n2MajFwxg4ca9XDviUE7+T2iXzFh096m0Skuq17FpgFdKqTBzuw2zVu9kVO92DH14JtsLiln1wOhK\nZxAut6lobC1zufnXlBWMGdCJlEQHzZISuOiFuXTISGHKX0+od0OrBnillIqgnPwiVuYUMKp3+0bf\nd20BPmITOIjIJGAucISIZIvINZHal1JKRVPHjNSoBPdgIjYe1xgzLlLbVkopFVz8TsGmlFIxTgO8\nUkrFKA3wSikVozTAK6VUjNIAr5RSMUoDvFJKxSgN8EopFaM0wCulVIzSAK+UUjFKA7xSSsUoDfBK\nKRWjNMArpVSM0gCvlFIxSgO8UkrFKA3wSikVozTAK6VUjNIAr5RSMUoDvFJKxSgN8EopFaM0wCul\nVIzSAK+UUjFKA7xSSsUoDfBKKRWjNMArpVSM0gCvlFIxSgO8UkrFKA3wSikVozTAK6VUjNIAr5RS\nMUoDvFJKxSgN8EopFaM0wCulVIzSAK+UUjFKA7xSSsUoDfBKKRWjIhrgRWS0iKwWkbUicnsk96WU\nUqqyiAV4EXECzwJnAH2BcSLSN1L7U0opVVkka/DHAmuNMeuNMaXAZOCcCO5PKaWUn4QIbrszsMXv\neTYwpOpKInItcK3naaGIrK7n/toAu+v53oOVHnN80GOOfQ053m41vRDJAB8SY8yLwIsN3Y6ILDDG\nDA5DkQ4aeszxQY859kXqeCOZotkKHOL3vItnmVJKqUYQyQD/M9BLRHqISBJwMfBZBPenlFLKT8RS\nNMaYchH5CzANcAKvGmOWR2p/hCHNcxDSY44PesyxLyLHK8aYSGxXKaVUlOlIVqWUilEa4JVSKkYd\n9AE+VqdDEJFDRGSWiKwQkeUicqNneSsR+UZE1njuW3qWi4g85fkclorIoOgeQf2JiFNEFovIFM/z\nHiIyz3Ns73oa7RGRZM/ztZ7Xu0ez3PUlIpki8oGIrBKRlSIyNNa/ZxG52fN3vUxEJolISqx9zyLy\nqojsFJFlfsvq/L2KyBWe9deIyBV1KcNBHeBjfDqEcuBvxpi+wHHAnz3HdjswwxjTC5jheQ72M+jl\nuV0LPNf4RQ6bG4GVfs8fAf5rjDkM2Atc41l+DbDXs/y/nvUORk8CXxljegNHYY89Zr9nEekM3AAM\nNsYcie2EcTGx9z2/DoyusqxO36uItALuwQ4SPRa4x/ujEBJjzEF7A4YC0/ye3wHcEe1yRehYPwVO\nBVYDHT3LOgKrPY9fAMb5rV+x3sF0w46XmAGMAqYAgh3hl1D1O8f20BrqeZzgWU+ifQx1PN4MYEPV\ncsfy94xvlHsrz/c2BTg9Fr9noDuwrL7fKzAOeMFveaX1gt0O6ho8gadD6BylskSM55T0aGAe0N4Y\nk+N5aTvQ3vM4Vj6LJ4BbAbfneWsgzxhT7nnuf1wVx+x5Pd+z/sGkB7ALeM2TlnpZRNKI4e/ZGLMV\n+DewGcjBfm8Lie3v2auu32uDvu+DPcDHPBFJBz4EbjLGFPi/ZuxPesz0cxWRs4CdxpiF0S5LI0oA\nBgHPGWOOBvbjO20HYvJ7bomdeLAH0AlIo3oqI+Y1xvd6sAf4mJ4OQUQSscF9ojHmI8/iHSLS0fN6\nR2CnZ3ksfBbDgLNFZCN29tFR2Px0poh4B+X5H1fFMXtezwByG7PAYZANZBtj5nmef4AN+LH8PZ8C\nbDDG7DLGlAEfYb/7WP6ever6vTbo+z7YA3zMTocgIgK8Aqw0xjzu99JngLcl/Qpsbt67/HJPa/xx\nQL7fqeBBwRhzhzGmizGmO/a7nGmMGQ/MAi7wrFb1mL2fxQWe9Q+qmq4xZjuwRUSO8Cw6GVhBDH/P\n2NTMcSLSzPN37j3mmP2e/dT1e50GnCYiLT1nPqd5loUm2o0QYWjEOBP4FVgH3BXt8oTxuE7Anr4t\nBbI8tzOxuccZwBpgOtDKs75gexStA37B9lCI+nE04PhPAqZ4Hh8KzAfWAu8DyZ7lKZ7naz2vHxrt\nctfzWAcCCzzf9SdAy1j/noH7gFXAMuAtIDnWvmdgEraNoQx7pnZNfb5X4GrPsa8FrqpLGXSqAqWU\nilEHe4pGKaVUDTTAK6VUjNIAr5RSMUoDvFJKxSgN8EopFaM0wKu4IiIuEcnyu4VtBlIR6e4/c6BS\n0RaxS/Yp1UQVGWMGRrsQSjUGrcErBYjIRhF5VER+EZH5InKYZ3l3EZnpmaN7hoh09SxvLyIfi8gS\nz+14z6acIvKSZ67zr0UkNWoHpeKeBngVb1KrpGgu8nst3xjTH3gGO6slwNPAG8aYAcBE4CnP8qeA\n74wxR2HnjvFeUL4X8Kwxph+QB5wf4eNRqkY6klXFFREpNMakB1i+ERhljFnvmeRtuzGmtYjsxs7f\nXeZZnmOMaSMiu4AuxpgSv210B74x9mIOiMhtQKIx5l+RPzKlqtMavFI+pobHdVHi99iFtnOpKNIA\nr5TPRX73cz2Pf8TObAkwHvje83gGcD1UXEM2o7EKqVSotHah4k2qiGT5Pf/KGOPtKtlSRJZia+Hj\nPMv+ir3a0j+wV166yrP8RuBFEbkGW1O/HjtzoFJNhubglaIiBz/YGLM72mVRKlw0RaOUUjFKa/BK\nKRWjtAavlFIxSgO8UkrFKA3wSikVozTAK6VUjNIAr5RSMer/ARn7LbOKOtAWAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEKCAYAAAAFJbKyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+H/0PJh6hxmiRgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tm772qfyrVwq",
        "colab_type": "text"
      },
      "source": [
        "> Based on the above plots, it looks like that there is not too much improvement after around 100 epochs. Later when we study Chapter 11, you will learn that there is a technique called `EarlyStopping` that can be used here which stops training if there is not much improvement after a fixed number of epochs. Moreover for now, you should fine-tune the hyperparameters of the network to see if you can see any improvements. Report the results of your hyperparameter tuning in the following cell. This is what's called <b>Grid Search</b> in hyperparameter tuning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BH5GfZytrVwr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        },
        "outputId": "33926b06-3ec2-4f63-d422-9a4430bf034f"
      },
      "source": [
        "### START CODING HERE ### \n",
        "# Build another regression neural network with the same architecture,\n",
        "# but you later compile it with different hyperparameters\n",
        "nn_reg2 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation = 'relu', input_shape = [len(X_train.keys())]),\n",
        "    tf.keras.layers.Dense(64, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "# Fine-tune the optimizer as follows:\n",
        "# First, try a different optimizer - SGD with the learning_rate = 0.001\n",
        "# Then, switch optimizer to Adam,\n",
        "# and use three different values of learning_rate in the order of 10 or 10^(-1) like 0.01, 0.1 and 1.\n",
        "# You may try other values for learning_rate.\n",
        "# Report the results of your hyperparameter tuning in the following cell.\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate = .001)\n",
        "\n",
        "# Compile nn_reg2 with loss='mae', optimizer=optimizer and metrics=['mae', 'mse']\n",
        "nn_reg2.compile(loss = 'mae', optimizer = optimizer, metrics = ['mae','mse'])\n",
        "\n",
        "# Fit nn_reg2 on X_train, y_train, epochs=EPOCHS, validation_split=0.2, verbose=0\n",
        "nn_reg2_history = nn_reg2.fit(X_train, y_train, epochs = EPOCHS, validation_split = .2, verbose = 0)\n",
        "### END CODING HERE ###\n",
        "\n",
        "plot_history(nn_reg2_history)"
      ],
      "execution_count": 298,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEKCAYAAAAYd05sAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deZwdVZnw8d9z99t7utNJZ99YQiAh\nhIaEbQgBFFlV1giKyLyoHx3EZRSY8RV1dHh1ZECdUVARGZXM4LAoiigQwAUDCQQIBEhCts7a3Ul6\n777b8/5xbnc6Iem+3bm3K139fD+f++lbdetWPXUreerUqVPniKpijDHGfwJeB2CMMaYwLMEbY4xP\nWYI3xhifsgRvjDE+ZQneGGN8yhK8Mcb4VKiQKxeRDUALkAZSqlpbyO0ZY4zZq6AJPussVW0Ygu0Y\nY4zpxapojDHGp6SQT7KKyHpgN6DA3ap6zwGWuQG4AaC4uPjEmTNnFiyeQkmkMmzdsYOpsh1GHwWR\nYq9DMsaMECtWrGhQ1eoDfVboBD9BVbeIyBjgj8A/qOpzB1u+trZWly9fXrB4CmVnSyef/ub3+Z/o\n1+HDj8CMs7wOyRgzQojIioPd3yxoFY2qbsn+3Qk8DJxcyO15JRoK0kbMTSTbvQ3GGGOyCpbgRaRY\nREq73wPvAVYVanteioUDdBB1EwlL8MaYw0MhW9GMBR4Wke7t/FJVf1/A7XkmEgzQRtxNJFq8DcYY\nY7IKluBV9R3g+EKt/3AiIqTDJW6iyxK8GZmSySR1dXV0dnZ6HYovxWIxJk6cSDgczvk7Q9EOfmQI\nF5FOBwl2NnsdiTGeqKuro7S0lKlTp5K9cjd5oqo0NjZSV1fHtGnTcv6etYPPk1gkRFegyErwZsTq\n7OykqqrKknsBiAhVVVUDvjqyBJ8n8XCQDkvwZoSz5F44g/ltLcHnSTwSpF2KoMuqaIwxhwdL8HkS\nCwdpswRvjGcaGxuZO3cuc+fOpaamhgkTJvRMJxKJnNZx3XXX8dZbb+W8zR//+MdUV1f3bGfu3LkD\n+n6h2U3WPImHg7RiVTTGeKWqqoqVK1cCcNttt1FSUsIXvvCFfZZRVVSVQODAZduf/vSnA97u1Vdf\nzZ133nnQz1OpFKHQ3lTbXwy9pdNpgsHggGPqZiX4PImFA7Rq3BK8MYeZtWvXMmvWLK6++mqOPfZY\ntm3bxg033EBtbS3HHnssX/va13qWPf3001m5ciWpVIqKigpuvvlmjj/+eE455RR27tyZ8zaffPJJ\nFi5cyIUXXsjs2bMPGMPPf/5zZs+ezXHHHcett94K0LPdm266iTlz5vDCCy8c0r5bCT5P4uEgzRoD\nayZpDF/9zeu8sTW//xdmjS/jKxcdO6jvvvnmm9x///3U1rouW26//XYqKytJpVKcddZZXHbZZcya\nNWuf7zQ1NXHmmWdy++2387nPfY57772Xm2+++V3r/sUvfsEzzzzTM92dlJcvX84bb7zB5MmTWbt2\n7T4x1NXV8c///M8sX76c8vJyzjnnHB577DHOO+88mpqa+Lu/+7s+rwpyZSX4PIlHgjRlYlaCN+Yw\nNGPGjJ7kDvDAAw8wb9485s2bx+rVq3njjTfe9Z14PM773vc+AE488UQ2bNhwwHVfffXVrFy5sucV\niUQAOOWUU5g8efIBY1i2bBmLFi1i9OjRhMNhPvShD/Hcc64fxkgkwgc+8IG87LeV4PMkFg6yJxOH\nTAekkxDM/WkzY/xmsCXtQiku3tuF95o1a7jrrrt44YUXqKio4Jprrjlg+/LuRA0QDAZJpVKD3uaB\npg8mHo/nrbmpleDzJB4Osied7VHSSvHGHLaam5spLS2lrKyMbdu28cQTTwx5DPPnz2fp0qU0NjaS\nSqVYsmQJZ555Zt63YyX4PImHg+zMxCCIaypZVOl1SMaYA5g3bx6zZs1i5syZTJkyhdNOO+2Q1rd/\nHfzdd9/d73cmTpzI17/+dRYuXIiqctFFF3HBBRcM+CqhPwUd8GOghuuAHwA//tM7vPj4z7g7cid8\n4s9QM9vrkIwZUqtXr+aYY47xOgxfO9Bv7NmAHyNJSTREC0VuwqpojDGHAUvweVISC9Gi2QTfscfb\nYIwxBkvweVMSDbGbbJ/wHbu9DcYYY7AEnzelsRB7tDvB7/I2GGOMwRJ83pTGwrQSJyMhaLcEb4zx\nniX4PCmJhgAhES6zErwx5rBgCT5PSmLukYKOULmV4I3xwFlnnfWuh5buvPNOPvnJT/b5vZKSkgPO\nDwaD+3QDfPvtt+ct1qFiDzrlSXHE/ZTtoTJG2U1WY4bc4sWLWbJkCe9973t75i1ZsoRvfetbg1pf\nPB7v6X74YPbvznf/roEPJtflDpWV4PMkGBCKI0FaA2XWisYYD1x22WX89re/7RncY8OGDWzdupUz\nzjiD1tZWzj77bObNm8fs2bN59NFHB72dqVOn8qUvfYl58+bx4IMPsnDhQm666SZqa2u566672LBh\nA4sWLWLOnDmcffbZbNq0CYCPfvSjfOITn2D+/Pl88YtfzMs+98dK8HlUEgvRTCm0r/U6FGO89fjN\nsP21/K6zZja87+DVJJWVlZx88sk8/vjjXHLJJSxZsoQrrrgCESEWi/Hwww9TVlZGQ0MDCxYs4OKL\nL+6zU6+Ojg7mzp3bM33LLbdw5ZVXAm5wkZdeegmAH/7whyQSCbqfwr/ooou49tprufbaa7n33nu5\n8cYbeeSRRwCoq6vjr3/96yEN4jEQluDzqDQWZreUQ1sDqIINQGzMkOqupulO8D/5yU8AN4rSrbfe\nynPPPUcgEGDLli3s2LGDmpqag66rryqa7kR/oOnnn3+ehx56CIAPf/jD+5TWL7/88iFL7mAJPq9K\noiHqk+WQSULnHoiP8jokY7zRR0m7kC655BI++9nP8tJLL9He3s6JJ54IuA7B6uvrWbFiBeFwmKlT\npx6wi+BcDbYr4FyXyxerg8+j0liI+kypm2hr8DYYY0agkpISzjrrLD72sY+xePHinvlNTU2MGTOG\ncDjM0qVL2bhxY8FiOPXUU1myZAngTixnnHFGwbbVHyvB51FJNMS2VLbJVVs9jD7S24CMGYEWL17M\nBz7wgZ4kC27UpYsuuojZs2dTW1vLzJkz+13P/nXw5513Xk5NJb/3ve9x3XXX8e1vf5vq6upBDeSd\nL5bg86gkGmJLsrsEX+9tMMaMUO9///vZvxv00aNH8/zzzx9w+dbW1gPOT6fTB5y//9B9vfuCB5gy\nZQpPP/30u7533333HTjgArIqmjwqjYWpS2RL8K25j8BujDGFYAk+j0piIeoScRSxOnhjjOcswedR\naTRESoMQr7QqGjMiHU4jxPnNYH5bS/B51N0fTSpeZQnejDixWIzGxkZL8gWgqjQ2NhKLxQb0PbvJ\nmkeuR0lIxqoIW4I3I8zEiROpq6ujvt7+7RdCLBZj4sSJA/qOJfg8Ks2W4DujVRQ1v+lxNMYMrXA4\nzLRp07wOw/RiVTR51J3g28OjrIrGGOO5gid4EQmKyMsi8liht+W18ngEgJbgKOhsglSXxxEZY0ay\noSjBfwZYPQTb8VxFURjAdTgG1lTSGOOpgiZ4EZkIXAD8uJDbOVxUxF2Cb9TuBG/VNMYY7xS6BH8n\n8EUgc7AFROQGEVkuIsuH+933UDBAaTTEjrR1OGaM8V7BEryIXAjsVNUVfS2nqveoaq2q1lZXVxcq\nnCFTXhRme0+Ct+4KjDHeKWQzydOAi0XkfCAGlInIz1X1mgJu03OjiiJsTmQ79LcqGmOMhwpWglfV\nW1R1oqpOBa4CnvZ7cgd3o3VHRxBCcUvwxhhPWTv4PKsoirCnIwnF1VYHb4zx1JAkeFV9RlUvHIpt\neW10SYSdLV1oSbV1GWyM8ZSV4PNsXHmM9kSaVMw6HDPGeMsSfJ7VlMeB7u4KrIrGGOMdS/B5Nq7c\ndefZFMj2R2NdpxpjPGIJPs9qylyCr5dKyCStHt4Y4xlL8Hk2NpvgN+sYN2P3Bu+CMcaMaJbg8ywS\nCjC6JMraVPapXEvwxhiP9Pkkq4hU5rCOjKruyVM8vjCuPMabHTFALMEbYzzTX1cFW7Mv6WOZIDA5\nbxH5QE15jE2N7VA6zhK8McYz/SX41ap6Ql8LiMjLeYzHF8aVx1j2TiNMnmoJ3hjjmf7q4E/JYR25\nLDOi1JTHaO5MkSyfbAneGOOZPkvwqtrZe1pEJuCqZAC2qmpq/2UMjM8+7NQSn0hlyzZIdkI45nFU\nxpiRps8SvIjcIiL/t9es54HHgD8A/1jIwIazmuzDTg2hGkChabO3ARljRqT+qmguB77Ta7pRVecA\nx+KG4jMH0P006xYZ62ZYNY0xxgP9toNX1bZek3dl56WBeKGCGu66H3Zab23hjTEe6i/Bl4hIuHtC\nVe8DEJEoUFbAuIa1WDhIZXGEdR3FbuCPXe94HZIxZgTqL8H/CrhbRIq6Z4hIMfDD7GfmIGrKYmxt\n6oSqGdC4zutwjDEjUH8J/svATmCTiKwQkZeADcCO7GfmIKaNLmZ9Q1s2wa/1OhxjzAjUZ4JX1bSq\n3gxMAj4KXAtMVtWbVTU1BPENWzOqi9m0q53UqBmuDj6d9DokY8wI018zySNF5FHgReBWYJeqdgxJ\nZMPc9OoSMgoNkUmgadi90euQjDEjTH9VNPfi2r1fCrwEfK/gEfnEjOoSADZQ42ZYNY0xZoj1l+BL\nVfVHqvqWqn4bmDoEMfnC9OpiAFZ1ZdvCN67xMBpjzEjUX2djMRE5gb29ScZ7T6vqS4UMbjgrjoYY\nVx5j1a4AxCutBG+MGXL9JfjtwB0HmVZgUSGC8otjxpWxelsLVB1hTSWNMUOuv87GFg5RHL40a1wZ\nz75dT3rKDILrn/E6HGPMCNPfiE4f7OtzVX0ov+H4y6zxZaQzSn1kEjUt26CrFaIlXodljBkh+qui\n+RWwMvuCfUd2UsASfB9mjXO9OazL1Li2NLvWwbjjPY3JGDNy9JfgPwhcBcwBHgUeUFW7W5ijyZVF\nFEeCrGwfzWkADWsswRtjhkx/T7I+oqpXAWcC64DviMifReTMIYlumAsEhGPGlfGnXeUQCMGO170O\nyRgzgvTbXXBWJ9AENAMlgA1PlKM5Eyt4eWsHmdFHwfbXvA7HGDOC9NdVwSIRuQdYAZwF3KWqc1X1\niSGJzgfmT6+kK5VhV+nRsGOV1+EYY0aQ/krwTwInA38GosBHROS73a+CR+cDJ0+tBODNzBRo2QZt\nDR5HZIwZKfq7yfoxXGsZM0ijiiPMrCnlTy01nA6ummbGWV6HZYwZAfp70Om+IYrD1xZMr+J3L47i\nliCw8S+W4I0xQ6K/Ovjb+ltBLsuMdPOnVbI5WUI6VAx/+4HX4RhjRoj+qmj+XkSa+/hccO3kb3vX\nByIx4Dlc3X0I+JWqfmWQcQ5rJ0+rBITXat7P3LpfQPM2KBvndVjGGJ/r7ybrj4DSPl4l2WUOpAtY\npKrHA3OB80RkQT6CHm6qSqIcOaaEXydPcjO2vuxtQMaYEaG/OvivDnbFqqpAa3YynH2N2Bu2C6ZX\n8ehLe/hyMIhsWQEzz/c6JGOMz+X6oNOgiEhQRFbiBu7+o6ouO8AyN4jIchFZXl9fX8hwPDV/eiWN\niRAdlTOh7kWvwzHGjAAFTfDZQbvnAhOBk0XkuAMsc4+q1qpqbXV1dSHD8ZSrh4f1sVmwZQVk0h5H\nZIzxu34TfLYU/tlD2Yiq7gGWAucdynqGszGlMaZXF/NM10xItMK6p70OyRjjc/0meFVNA4sHumIR\nqRaRiuz7OHAu8OaAI/SRU6ZX8aOdM9FYBaz6X6/DMcb4XK5VNH8Rke+LyBkiMq/71c93xgFLReRV\n4EVcHfxjhxTtMHfOrLHsSQg7x5wG65aCjth7zsaYIdBfO/huc7N/v9ZrXp9jsqrqq8AJg4zLl06d\nUUVJNMSf07O4tPW3rn/46qO8DssY41M5JXhVtWfr8yAaCrLw6GruWzeNSwHe/r0leGNMweRURSMi\n5SJyR3dzRhH5joiUFzo4P3rvsTW81lZB+6hjYM0fvA7HGONjudbB3wu0AFdkX83ATwsVlJ8tPLqa\ncFB4NToPNi+DRLvXIRljfCrXBD9DVb+iqu9kX18FphcyML8qjYVZML2Kh5uOgHQCNj3vdUjGGJ/K\nNcF3iMjp3RMichrQUZiQ/O89s8by691TyATC1h7eGFMwuSb4TwD/ISIbRGQD8H3g4wWLyucuPn4C\n6VAR64pPhDcfs+aSxpiCyOVJ1gBwdLZXyDnAHFU9IdsM0gxCeVGY84+r4f6WE2D3BnvoyRhTELk8\nyZoBvph936yqffUPb3J0xUmT+GXnqbQWT4EV93kdjjHGh3KtonlSRL4gIpNEpLL7VdDIfG7BtCom\nVpXyLPNg8wuQ6vI6JGOMz+Sa4K8EPoUboWlF9rW8UEGNBIGAcEXtJB7ZPQ3SXa6HSWOMyaNc6+Cv\nUdVp+72smeQhuuzEibyoM0lJGF5/2OtwjDE+k2sd/PeHIJYRZ2xZjNqZ01lKLfr6w9ZHvDEmr3Kt\nonlKRC4VESloNCPQlSdN5uGuk5C2etj4V6/DMcb4SK4J/uPAg0CXiDSLSIuIWGuaPDjr6GpeK5pP\nl0ThtQe9DscY4yM5JXhVLVXVgKpGVLUsO11W6OBGglAwwIW1R/Dr1AL01f+Bjt1eh2SM8Yk+E7yI\nXNPr/Wn7ffbpQgU10lxRO4n7Uu9FUh3wyhKvwzHG+ER/JfjP9Xr/vf0++1ieYxmxpo0upurIk3iL\nqaRf/7XX4RhjfKK/BC8HeX+gaXMIblx0BL9PnYBs/hu0bPc6HGOMD/SX4PUg7w80bQ5B7dRKNky8\nmAAZ9HsnQjrldUjGmGGuvwQ/U0ReFZHXer3vnj56COIbURa/dyFL08cjiVbYscrrcIwxw1x/Y7Ie\nMyRRGABOnlbJp8Z9nrN2foTU47cQuv5xr0MyxgxjfSZ4Vd04VIEY54aLzoSfQGjzX9Fd65HKaV6H\nZIwZpnJ90MkMkeMnVfDw7B8AsO43/+ZxNMaY4cwS/GHo4kuuoF2KOGL9z9n+qg3pZ4wZnAEneBEZ\nJSJzChGMcYKhEC3X/I4EIVb97gck0xmvQzLGDEM5JXgReUZEyrKDfLwE/EhE7ihsaCPb2BknsGvc\nmZzT+Qe+/wdrUWOMGbhcS/Dl2aH6Pgjcr6rzgXMKF5YBqDnxQgCK/nI7D79c53E0xpjhJtcEHxKR\nccAVwGMFjMf0Nu9aMtMX8fHQb1n4yAKWLfuL1xEZY4aRXBP814AngHWq+qKITAfWFC4sA0AgSODi\n7wIwSlqY8rtr2NDQ5nFQxpjhItfugh9U1Tmq+sns9DuqemlhQzMAVEyCmzcBUCO7+MJPn2B7U6fH\nQRljhoNcb7JOF5HfiEi9iOwUkUezpXgzFGLl8PdPAfDN1v/L4h/9ja17OjwOyhhzuMu1iuaXwP8A\n44DxuNGdHihUUOYAJpwI0XKOks0c1/IXPviff+XlTTY4iDHm4HJN8EWq+l+qmsq+fg7EChmY2Y8I\nXPc7AL5TtgRNJ7n6x8t49u16jwMzxhyu+hvRqTLb9v1xEblZRKaKyBQR+SLwu6EJ0fSoOQ4uuINI\nyyaWpa/ksrI3uP6+F7nzybfpTKa9js4Yc5gR1YN36y4i63H9vh9ocA9V1bzWw9fW1ury5cvzuUr/\nUYUlH4K33Pn14TGf4t82HQUVk/nRR2qZNd6GyjVmJBGRFapae8DP+krw/aw0rKrJPj6fBNwPjMWd\nJO5R1bv6Wqcl+BylumDNH+C/e4bM5Zb0x/lN4Gz+7fLjOe+4Gg+DM8YMpb4S/ID6ohHnbBH5CdDf\no5Up4POqOgtYAHxKRGYNZHvmIEJROOYiuHVrz6x/Dd7N5cUvcf8vf8Y/PvgK7QkbEcqYkS7XZpIL\nROS7wEbgUeA5YGZf31HVbar6UvZ9C7AamHBo4Zp9RIrhtiaYfTkAX2m/nV9GvsmXVl3IjXfcx7N/\n/hODvUIzxgx//dXBfxO4HNiEaxb5MLBcVQc0CoWITMWdFI7L9mnT+7MbgBsAJk+efOLGjTbGyICp\nwor7YNPz8Op/7/PRN6r+lflnX8rCo6sJBa13aGP8ZtB18CKyE3gbuBP4jap2icg7A7m5KiIlwLPA\nN1T1ob6WtTr4PHju2/Dst9FACEm2sUMr+HLyOsZVVTD/3Cs48+gxFEf7G6nRGDNcHEqCDwLnAouB\ns4GluF4kJ6lqv5W8IhLGdU72hKr2272wJfg8UnU3Yn95Rc+sZ9Nz+GzmJj44qZW2MSfw2XOPYkyp\nPc5gzHCWl1Y0IhIFLsQl+zOAp1T1Q30sL8DPgF2qelMu27AEXwCN6+Ch/wNbVuwz+83MJJ7OnMBR\nRa0cVdzBmjO/z0mj2iijDTr2QLQU1j8LxWNg/g3uS6qw6n+hbDxMPAmCYTc/lXDvu/8ticAbj0Aw\nAkedB+88A5NOdusE1wqorR7KJ+4bayYDgQNUI9WtgEAQikdD6Tj3PleJdlj/HBx9Xu7fMWYYyXsz\nSREpA96vqvf3sczpwJ+A14DuIYluVdWDPiBlCb6AMhl44lZY+yQ0DrAj0BmLYMfrLlkmWvpfPlIC\nidZ3zz/7K1B9tGvHDy75xyth5xuQScGOVVA+Gc74LKx8wCXyc78GPzl33/VMqIULvuP2pa0ekh2w\n6MtuvXUvwOdWQyYNT94Gq37lvnPBd+Ckv4ftr0HVERCOQ1cLpJPwzlJoqoNpZ8L4uQfep9WPQbId\n5mSviN54FKacDsVVOf2EPV7+OaQTUPuxvfNU3UnRmEEoSDv4QrAEP0QSba5EH6uA+rfYU/cGDds2\nMmb7s7wgs3mrvZTzA8tYkl7EeGng2tAfAcgQIMAwHj5w9NHQ8JZ7f9yl7mrkQKLlMOkkaNnuknoo\n5k5CAMdd5qZX/txNf2mju1IJhCDd5a5Skh1uGRHY/AJ0NcMR58DT/+LukQCceiOcdhP89nOw9SW4\n/o9Qmn1+IZ10V0SdTbDtFZj2d25+ZxM0roXx86Bzjzt+m19wV0cDOUE8eRuUT4KTrh/Qz2cOT5bg\nzYB0JNK8taOF7U0dvLm9hVdef4Pojpf5Q6aWDAHCQWFCRZyJZWFmti9n3sQiwrMu5oSKVsp3vUb4\n2ItdYgxGXCl56Tchk3TT40+AdUtBAjBqKlROcyXYmee7BJhJw+71LgG9/QS07XRBzVgEZ37JJbyO\n3a4kPGEepFPw1m/dMqXjXclc09C8zX1nzRMusdev9uz3BKBiMuzZ1PcyR5zjqr7+cpe7ytj+qpt/\n3u3QsAaW/+Tg360+BmZeAEeeCy//F0yaD+Ei9zuu+YO7SplzJUw+Be4+w33nww+7Y5BOud+0+0TR\nscdNx8rdOpq3wLqnYe6H3NVHWwNoBoqq4E93wMIvQWezO1brlsKJH3XVaaqw6x2onD6wE1D7Liiq\nzH353lp2uBPq6CPddMeebKyDXN8wYAneHLLOZJp19a28vqWZtfWtvFPfyt/e2UVr17732iOhAEeN\nLaFudwcTKuKcfcxYRpdEOKK6hFHFESaMilMcCREQkFz+02/6G9TMgUjRwZfJpF3iCcf3zkslIBTZ\nO92yA1q3w6hprjooFHXVMgiUjXPrCEZgy3L3t7PJlcQnngSv/NLFsP1VV2res8ndh1j1kKsSGjRx\nJf6u5n1nh+KQGqLuoIMR99vl25yr4NUl+86rmAyX/Ce0N8LKX7orl/kfd1dK4SJ3NbL5b27ZcLEr\nDIya4k4Sx18Fo49yx2n7a/DELe7E/d5vuHWd+FHX4+q/Zh+1ueAOmHkh/OcCV134hbfdOtf+ERre\nhpIaGHc8/O0/4D3fgHiFOyFl0u5klGhzMQWC8JvPwDEXw5G9Rilt3+X+PUxf6O4phaJufsduiJRC\nyzYom+DuKa24z93Lmnn+vr9Hw1roanInylFTB/1T5+sm66nAVKCnjV1fdfCDYQl+eFFVmjqSNHek\n+Nv6RtbsaGF9Qzv1LZ28UtfU7/dHl0RpaO3ipKmjqCmP09yRJBQQJo6KM3dyBZNGFVEeDxMOBhhf\nEScSCqCquZ0YvNCx2yXnRBvsWufuFaQ6XKJYcZ+7Wpl4squ+CQRcqTda6hJK81aQoOtj6JiLYO1T\nLjH95U6XAC+4w5XqJ50My++FV5a4BFh9tDthlYx1CfSl+2HNH+HML7rnIjYvcyX0Mz7nEuGW5VA0\nGsbNcaXymtnuRnyyPbd9HDcXtq0s6M/omViFq/rqb5mZF8DKX+w7/4hz3D2h/Y2Ztbd6D6DqSHdS\nCEbcyaXbOV+F03Nqi/Iuh5zgReS/gBnASqC720JV1RsHFdFBWIL3n6b2JC1dSd7e0cLO5i52tnTR\n3JGkK5Xh5c27WV/fRiAghALC7vaDdm1EMCDUlMVo7kgypixKOBhgSlURL6zfxe72JF+9+FjK4iEi\nwSDhoHDEmBIS6Qw7mruYVlVMOCSMK48fdP0jRu+WSt3vM2lXZdZW70rRMy9wJ53eraI6m9xyRZX7\n3hRuawTU3YMIRd1V1O4N0FrvqkY6dkN7g6sqCoTcVVP3dzf82c2fcqq7uqo53pXqZ18Or/6Pu0rq\nbILXH3L3S+Z/wlUXVc90J8a2Btj4l70txOZd6+I75mL49Y2w4zU49gOQ7ISmzS7RasadSDXtqvQm\nneRumIM7QUrQVRF64Za6vS3NBiAfCX41MEsLXJ9jCX5kS6YzNHUkWd/QRltXiowqb25vIZHKsKst\nQWtnijU7WwkItCXSNHck2dnSlfP6R5dEGF0SpbI4Qt3uDoIBIZnO8METJtCeSFNZEmFGdQkV8TCh\nYIBgQFBVjhxbSnEkiCoEAofp1YPJTVeLS6K9q/Ay6QM3vW3Z7q7Gyia4q6lgxF3pvJm953P8Ve5z\nzUB8FATC7sQhgb0nkkDAbautHvZsdFU3o4926+o+GY491i1fPHpQu5SPBP8gcKOqbhtUBDmyBG8G\nKpNR0qpsb+pk654O2hNpOpNpmjuTdCYz/GlNA61dSXa3Jakpj7GrLcHu9gR1uwdWx10UCdKeSDOh\nIs7RNaW0daVo7kwxuiTCqKKIuzeZSHPaEaNRVZJpZWx5jIp4mPZEmvEVMZo6klSXRtm2p5NjJ5QR\nDQUpj4cL9MuYkSIfCX4pMNxOQhgAAA1tSURBVBd4AegpMqnqxfkKEizBm6GXziitnSl2tyfIqFLf\n0kUyraQyGToSadbsbCWdUXY0d9LYlqAzmaaxNUEgAMmU0tqVojOZJpHO0NI5sB48AwJjSmNsb947\niHokFOCESRXZ+w4xOpIZ4uEARZEQVcURmjqSTKsupiIeIa1KUISa8ii725LEwkFqymMEA0IgW8MS\nCQUIBYVYOEhZbO/J5LC+l2EGpK8En2unJLflLxxjDh/BgFBeFKa8yCW/6dUl+3z+vhzXo6o0tiXI\nZJRkRgkFhMbWBNuaOkhllM5kmnRGWd/QxtY9ncwYU8zaHa0k0hkyqryyuYktezpIZ5SG1i4yCsvW\nNyIiZDJKQIREevDPIAQEMgpjy6JEQ0E27WqnNBaidsooSmJhVJW3d7SwaVc7588ex7jyGIlUhoqi\nCKm0EgsHSKYzVJdGmVBRRFE02NNTqSpUFkcIBwNUFIVJpZWKInflYv0eeSunX19Vny10IMYMZyLC\n6JLoPvPGlsXyMsJWdzUUQH1LF1v3dJBMK0WRIKu2NvWU5jfvaicaChAKCKFggJ3Nnaza2kxRJEhX\nKsOO5k7Glcd4p74NgFg4yM6WLt7e4a5SEukMnckMj7+2nY48DQEZEKgqiRIPBwkFhUgwQGNbgsqi\nCIGA0NaVYlJlnKJIiDe2NjN1dBGVxVGKI0GKo6Gev7vaE2xsaKcoEiStyrzJoyiOhshklClVRZTG\nwsQjQTKq7G5LcNyEcpo6ktS3dFHf2sWZR1bTlki5Jroj6D5KrlU0C4DvAccAESAItKlqXseHsyoa\nY7yjqrQl0oSDQmciQ0cyTTKdIRiQnjF/d7a4llCl0RDBbKJMpjNs2tVOS2cKVQiHXOJu6UwRDgZo\n6kjS2NpFW1eaZCZDOOiuBtq70uxqTxAPB+lIpmnJ3jcpjYaIR4K0daVoS+TnRBMLB+hMuiug6dXF\njCqKsLGxjaJIiGgowPYmd1+kqiRKKCBsb+pkfEWczmSa1q4UAZHszfl2MgqTK4uYNrqYiaPiREPu\nBm0qkyGVVoqjQaLhoDsxZ5TxFXHKYmFEYHxFnC27OwiHhKriKK3ZxgT7Fw4GIh9VNN8HrgIeBGqB\njwBHDToiY8xhR0QoyVapRENBynn3DeD9q7DyLZFyJ5Tuk0cmo3Qk04SCQktnivqWLsLBAOsb2uhI\nptm8q50JFXHCwQAdyTQ7mjtpT6SIhYI91UOrt7mrmKff2kk4GKC6JMqe9iRjSmNUlURo60oRDMTZ\n0+5aZXUm0jS0Jli2ftdB41yxcXde93tGdTFPfX5hXtcJuSd4VHWtiARVNQ38VEReBm7Je0TGmBEr\nEtq3N9FAQHoSdbQk2FPSPWLMwE80Xx3g8qqu2iqSHSintStFMq20daUIBNzJcFOje0AsrW7+pl3t\nFEdDlEZDtHalCAeFhtYEqXSGVEZ7mvaWx8OUxcOURENs2dNB8hDur/Ql1wTfLiIRYKWIfAvYxgDH\nczXGmOFERHqqXwBKs62QKov3doExe2L5Pt85bWhCy1muSfrD2WU/DbQBk4BLCxWUMcaYQ5drK5qN\nIhIHxqnqQK90jDHGeCCnEryIXITrh+b32em5IvLrQgZmjDHm0ORaRXMbcDKwB0BVVwLTChSTMcaY\nPMg1wSdVdf/+Xw+fjuSNMca8S66taF4XkQ8BQRE5ErgR+GvhwjLGGHOoci3B/wNwLK6jsQeAZmBw\nvdMbY4wZErm2omkH/in7MsYYMwz0meD7aymT7+6CjTHG5E9/JfhTgM24apllwMjphs0YY4a5/hJ8\nDXAusBj4EPBb4AFVfb3QgRljjDk0fd5kVdW0qv5eVa8FFgBrgWdE5NNDEp0xxphB6/cmq4hEgQtw\npfipwHeBhwsbljHGmEPV303W+4HjgN8BX1XVVUMSlTHGmEPWXwn+GlzvkZ8Bbuw1SK8Amu8RnYwx\nxuRPnwleVa3Pd2OMGaYsgRtjjE9ZgjfGGJ+yBG+MMT5lCd4YY3zKErwxxvhUwRK8iNwrIjtFxNrO\nG2OMBwpZgr8POK+A6zfGGNOHgiV4VX0O2FWo9RtjjOmb53XwInKDiCwXkeX19fVeh2OMMb7heYJX\n1XtUtVZVa6urq70OxxhjfMPzBG+MMaYwLMEbY4xPFbKZ5APA88DRIlInItcXalvGGGPerd8BPwZL\nVRcXat3GGGP6Z1U0xhjjU5bgjTHGpyzBG2OMT1mCN8YYn7IEb4wxPmUJ3hhjfMoSvDHG+JQleGOM\n8SlL8MYY41OW4I0xxqcswRtjjE9ZgjfGGJ+yBG+MMT5lCd4YY3zKErwxxviUJXhjjPEpS/DGGONT\nluCNMcanLMEbY4xPWYI3xhifsgRvjDE+ZQneGGN8yhK8Mcb4lCV4Y4zxKUvwxhjjU5bgjTHGpyzB\nG2OMT1mCN8YYn7IEb4wxPmUJ3hhjfMoSvDHG+JQleGOM8SlL8MYY41OW4I0xxqcswRtjjE8VNMGL\nyHki8paIrBWRmwu5LWOMMfsqWIIXkSDwH8D7gFnAYhGZVajtGWOM2VchS/AnA2tV9R1VTQBLgEsK\nuD1jjDG9hAq47gnA5l7TdcD8/RcSkRuAG7KTrSLy1iC3NxpoGOR3hyvb55HB9tn/DmV/pxzsg0Im\n+Jyo6j3APYe6HhFZrqq1eQhp2LB9Hhlsn/2vUPtbyCqaLcCkXtMTs/OMMcYMgUIm+BeBI0VkmohE\ngKuAXxdwe8YYY3opWBWNqqZE5NPAE0AQuFdVXy/U9shDNc8wZPs8Mtg++19B9ldUtRDrNcYY4zF7\nktUYY3zKErwxxvjUsE/wfu0OQUQmichSEXlDRF4Xkc9k51eKyB9FZE3276jsfBGR72Z/h1dFZJ63\nezB4IhIUkZdF5LHs9DQRWZbdt//O3rRHRKLZ6bXZz6d6GfdgiUiFiPxKRN4UkdUicorfj7OIfDb7\n73qViDwgIjG/HWcRuVdEdorIql7zBnxcReTa7PJrROTagcQwrBO8z7tDSAGfV9VZwALgU9l9uxl4\nSlWPBJ7KToP7DY7Mvm4AfjD0IefNZ4DVvab/H/DvqnoEsBu4Pjv/emB3dv6/Z5cbju4Cfq+qM4Hj\ncfvu2+MsIhOAG4FaVT0O1wjjKvx3nO8Dzttv3oCOq4hUAl/BPSR6MvCV7pNCTlR12L6AU4Anek3f\nAtzidVwF2tdHgXOBt4Bx2XnjgLey7+8GFvdavme54fTCPS/xFLAIeAwQ3BN+of2POa6F1inZ96Hs\ncuL1Pgxwf8uB9fvH7efjzN6n3Cuzx+0x4L1+PM7AVGDVYI8rsBi4u9f8fZbr7zWsS/AcuDuECR7F\nUjDZS9ITgGXAWFXdlv1oOzA2+94vv8WdwBeBTHa6CtijqqnsdO/96tnn7OdN2eWHk2lAPfDTbLXU\nj0WkGB8fZ1XdAvwbsAnYhjtuK/D3ce420ON6SMd7uCd43xOREuB/gZtUtbn3Z+pO6b5p5yoiFwI7\nVXWF17EMoRAwD/iBqp4AtLH3sh3w5XEehet4cBowHijm3VUZvjcUx3W4J3hfd4cgImFccv+Fqj6U\nnb1DRMZlPx8H7MzO98NvcRpwsYhswPU+ughXP10hIt0P5fXer559zn5eDjQOZcB5UAfUqeqy7PSv\ncAnfz8f5HGC9qtarahJ4CHfs/Xycuw30uB7S8R7uCd633SGIiAA/AVar6h29Pvo10H0n/Vpc3Xz3\n/I9k78YvAJp6XQoOC6p6i6pOVNWpuGP5tKpeDSwFLssutv8+d/8Wl2WXH1YlXVXdDmwWkaOzs84G\n3sDHxxlXNbNARIqy/86799m3x7mXgR7XJ4D3iMio7JXPe7LzcuP1TYg83MQ4H3gbWAf8k9fx5HG/\nTsddvr0KrMy+zsfVPT4FrAGeBCqzywuuRdE64DVcCwXP9+MQ9n8h8Fj2/XTgBWAt8CAQzc6PZafX\nZj+f7nXcg9zXucDy7LF+BBjl9+MMfBV4E1gF/BcQ9dtxBh7A3WNI4q7Urh/McQU+lt33tcB1A4nB\nuiowxhifGu5VNMYYYw7CErwxxviUJXhjjPEpS/DGGONTluCNMcanLMGbEUVE0iKystcrbz2QisjU\n3j0HGuO1gg3ZZ8xhqkNV53odhDFDwUrwxgAiskFEviUir4nICyJyRHb+VBF5OttH91MiMjk7f6yI\nPCwir2Rfp2ZXFRSRH2X7Ov+DiMQ92ykz4lmCNyNNfL8qmit7fdakqrOB7+N6tQT4HvAzVZ0D/AL4\nbnb+d4FnVfV4XN8x3QPKHwn8h6oeC+wBLi3w/hhzUPYkqxlRRKRVVUsOMH8DsEhV38l28rZdVatE\npAHXf3cyO3+bqo4WkXpgoqp29VrHVOCP6gZzQES+BIRV9V8Kv2fGvJuV4I3ZSw/yfiC6er1PY/e5\njIcswRuz15W9/j6fff9XXM+WAFcDf8q+fwr4JPSMIVs+VEEakysrXZiRJi4iK3tN/15Vu5tKjhKR\nV3Gl8MXZef+AG23pH3EjL12Xnf8Z4B4RuR5XUv8krudAYw4bVgdvDD118LWq2uB1LMbki1XRGGOM\nT1kJ3hhjfMpK8MYY41OW4I0xxqcswRtjjE9ZgjfGGJ+yBG+MMT71/wHLkAd4FxgJrAAAAABJRU5E\nrkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEKCAYAAAAFJbKyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_C_Kfr-lrVwt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "56713a47-b9df-4e24-c319-7ec225d47f7c"
      },
      "source": [
        "### START CODING HERE ### \n",
        "# Evaluate nn_reg2 using .evaluate() method on X_test, y_test and verbose=2\n",
        "loss2, mae2, mse2 = nn_reg2.evaluate(X_test, y_test, verbose = 2)\n",
        "### END CODING HERE ###\n",
        "\n",
        "print(\"Testing set Mean Abs Error: {:5.2f} MPG\".format(mae2))"
      ],
      "execution_count": 299,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
            "79/79 - 0s - loss: 1.6647 - mae: 1.6647 - mse: 5.9435\n",
            "Testing set Mean Abs Error:  1.66 MPG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlWd9VDCrVwx",
        "colab_type": "text"
      },
      "source": [
        "Report the results of your hyperparameter tuning HERE:\n",
        "\n",
        "SGD - Testing Set mae: `1.66` MPG\n",
        "<br>\n",
        "Adam learning_rate1 - Testing Set mae: `2.15` MPG    //using .01\n",
        "<br>\n",
        "Adam learning_rate2 - Testing Set mae: `2.23` MPG     //using .10\n",
        "<br>\n",
        "Adam learning_rate3 - Testing Set mae: `5.90` MPG    //using 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMJMhC6HrVwz",
        "colab_type": "text"
      },
      "source": [
        "> You can check the quality of the model predictions by the following plot. Use this plot to answer Part II Q1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OzLMdgUrVw1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "outputId": "fddefd28-1cc0-4f53-9aa6-e47cded95d5f"
      },
      "source": [
        "# Use the best hyperparameters you found for compiling nn_reg2, then run this cell\n",
        "nn_reg2_preds = nn_reg2.predict(X_test).flatten()\n",
        "\n",
        "a = plt.axes(aspect='equal')\n",
        "plt.scatter(y_test, nn_reg2_preds)\n",
        "plt.xlabel('True Values [MPG]')\n",
        "plt.ylabel('Predictions [MPG]')\n",
        "lims = [0, 50]\n",
        "plt.xlim(lims)\n",
        "plt.ylim(lims)\n",
        "_ = plt.plot(lims, lims)"
      ],
      "execution_count": 300,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQ8AAAEKCAYAAAAM4tCNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3de5RcZZnv8e+vOx3TIYSGJGSwAQkE\nkxWGSyQGNHiORAUVxQx6FLysjJOZqMcLjpxocDwahZEoM17XeAmgxhkFRGKIYTRgwDteOiQQEoiQ\nEA60hE6UJgl0Ql+e88fe1amu7KraVV27alfV81mrV9eu6xPo/vV+3/1eZGY451ypWmpdgHOuPnl4\nOOfK4uHhnCuLh4dzriweHs65snh4OOfKMibJN5e0E9gHDAIDZjZH0jHAzcBJwE7grWb2dJJ1OOcq\nrxpnHueb2VlmNic8XgqsN7NTgfXhsXOuztSi2fImYGV4eyWwoAY1OOdGSUmOMJX0KPA0YMA3zWyF\npF4z6wgfF/B05jjntYuBxQBHHHHE2TNnzkysTuea1cCgsWPPfvY98ac9ZjallNcm2ucBnGdm3ZKO\nBe6U9FD2g2ZmkiLTy8xWACsA5syZY11dXQmX6lxz6dl7gEuv+x3PP3OAB6963WOlvj7RZouZdYff\ne4AfAXOBpyQdBxB+70myBufc4TLBseuZA3zn3XPLeo/EwkPSEZKOzNwGLgAeANYAC8OnLQRuS6oG\n59zhcoNj7rRjynqfJJstU4EfBd0ajAG+b2Y/lfRH4AeSFgGPAW9NsAbnXJZKBQckGB5mtgM4M+L+\nvwCvSupznXPRKhkc4CNMnWsKlQ4O8PBwruElERzg4eFcQ0sqOMDDw7mGlWRwgIeHcw0p6eAADw/n\nGk41ggM8PJxrKNUKDvDwcK5hVDM4wMPDuYZQ7eAADw/n6l4tggM8PJyra7UKDvDwcK5u1TI4wMPD\nubpU6+AADw/n6k4aggM8PJyrK2kJDvDwcK5upCk4wMPDubqQtuAADw/nUi+NwQEeHs6lWlqDAzw8\nnEutNAcHeHg4l0ppDw7w8HAudeohOMDDw7lUqZfgAA8P51KjnoIDPDycS4V6Cw7w8HCu5uoxOMDD\nw7maqtfgAA8P52qmnoMDPDycq4l6Dw6AMbUuwLlmEyc4Vm/s5tp12/hzbx8v7GhnyYUzWDC7swbV\n5ufh4VwVxQ2OK1dtpq9/EIDu3j6uXLUZIFUB4s0W56okblPl2nXbhoMjo69/kGvXbatGmbF5eDhX\nBaX0cfy5t6+k+2vFw8O5hJXaOfrCjvaS7q8VDw/nElTOVZUlF86gva11xH3tba0suXBGUmWWJfHw\nkNQqaaOkteHxNEm/l/SIpJsljU26BudqodzLsQtmd3LNJafT2dGOgM6Odq655PRUdZZCda62XA48\nCEwMjz8HfNHMbpL0DWAR8PUq1OFc1Yx2HMeC2Z2pC4tciZ55SDoeuAi4PjwWMB/4YfiUlcCCJGtw\nrtoaYQBYHEk3W74EfBQYCo8nAb1mNhAePwFExqukxZK6JHXt3r074TKdq4xmCQ5IMDwkvQHoMbMN\n5bzezFaY2RwzmzNlypQKV+dc5TVTcECyfR7zgIslvR4YR9Dn8WWgQ9KY8OzjeKA7wRqcq4pmCw5I\n8MzDzK40s+PN7CTgUuAuM3sHcDfwlvBpC4HbkqrBuWpoxuCA2ozz+BjwEUmPEPSB3FCDGpyriGYN\nDqjSxDgz+znw8/D2DmBuNT7XuSQ1c3CAjzB1rizNHhzg4eFcyTw4Ar6eh3MlWPnbnVy1disDQ8bk\nCWNTN9O1mjw8nItp5W93smzNFiw83rP/+VQu0lMt3mxxLoaevQe4au3W4eDISOMiPdXi4eFcEZk+\njoGh3OgINGvTxcPDuQKyO0cnT4hePSJti/RUi4eHc3nkXlX5xEWz6mKRnmrxDlPXNErZzqDQ5di0\nb4lQLR4erimUsp1BoeCoh0V6qsXDwzWFQtsZZMJg9cZulv/kIXbtPYCAD8yf3rQDwOLwPg/XFIpt\nZ7B6YzdLb72fXXsPAGDA9b96lNUbfcWIfDw8XFMotp3B8p88xIGBoRGPNfMYjjg8PFxTKLSdQc/e\nA8NnHLmadQxHHN7n4ZpCpl8j90rJy0+ZxKXX/Q7BYaNHoXnHcMTh4eGaRu6VkuyrKh+YP51v/Hw7\n/VmjSNta1LRjOOLw8HBNKfdy7J97+0A5T8o9diMU7POQtLfI1z5Jf6pWsc5VQtQ4jmvXbaN/cGTD\npX/Q+PSPt9SoyvQr1mG63cwmFvg6Eni2GoU6Vwn5BoDl6xh9+rl+v1ybR7HweHOM94jzHOeqYvXG\nbuYtv4tpS29n3vK7RvziFxo5Wqhj1C/XRisYHuFixQXFeY5z1ZAZgt7d24dxaAj66o3dRZcOLNQx\n6pdroxXr81gkaUnWcXdWX8d7ky/PufjyDUFf/pOHiq45umB2Jx3tbZHv65droxVrtrwX+FbWcY+Z\nTQSmAJclVpVzZch3hrBr74FYixUvu/g0n3JfgmLhITP7S9bxLQBmdgDwOHapku8MQRBrlfMFszu5\n5pLT6exoR0BnRzvXXHK6z6LNo9g4j47sAzP7LICkFmByUkW55lbKuhvZllw4Y8S0+4xSZsf6lPv4\nioXHHZKuNrNP5Nz/GeCOhGpyTayUdTcyz88Omjef3cnPtvaMmFZ/xQWja3aUG2aNrlh4LAFuCPeV\nvS+870ygC/jHJAtzzSnOuhsZUUHzw64nmNjexvixrRXZkKnUMGsmBcPDzJ4FLpV0MnBaePdWM9ue\neGWuIZT6V7vYuhvZooLmwMAQB/cd5Ob3vKwiC/mUEmbNpmB4SDoW+DgwHdgMXGNme6tRmKt/5fzV\nfmFHO90RQRHVGZovaAwqtgJYKWHWbIpdbfkuwfDzrwITgK8kXpFrGIX+audTaN2NXPmurnRWcFxG\nsUWEmlmx8DjOzP7FzNaZ2QeBM6pRlGsM5fzVLuVy6ZILZzBuzMgf4UqPyyglzJpN0Sn5ko7m0OTk\n1uxjM/trgrW5OldKEyRb3MulLz9lEhPb2zi47yBGEDSVvhKSbxGhZu/vgOLhcRSwgZErG9wbfjfg\n5CSKco0hatxFpf5qZ+aq7D84ULHO0Xx87Ee0YldbTqpSHa4BJfVXu9gkN1cdxa62vKTQ42Z2b77H\nJI0Dfgm8IPycH5rZpyRNA24CJhGc1bzLzJ4vtXBXHyr9V9uDIz2KNVu6gAeAPeFxdvPFgPkFXnsQ\nmG9m+yW1Ab+W9BPgI8AXzewmSd8AFgFfL6t611Q8ONKlWHh8BHgL0EdwtvAjM9sf543NzIDMc9vC\nr0zgvD28fyWwDA8PlyVqYFlmlXMPjvQo1ufxJeBL4QjTS4H1kh4DPmtmm4q9uaRWgqbJdOA/gO1A\nr5kNhE95Aog8p5W0GFgMcOKJJ8b717jUiTPCNPs5R7W38ezzA8PriXb39rH01vuZ2N7G/oMDHhwp\nEmvTp3C1sNsIJsPNBV4c83WDZnYWcHz4uplxCzOzFWY2x8zmTJkyJe7LXIqs3tjNklvuG7Gy15Jb\n7huxNGDu6l+9ff2HLUR8YGCI3fsOenCkTLEO08wZx5uAxwmaLp81s5LG5ppZr6S7gZcBHZLGhGcf\nxwO+umyDWrZmy4h9UAD6h4xla7aMuBKTOwo1SiWHnGf4bNnRKdbn8QhwP8FZx17gROB9UtBvamZf\nyPdCSVOA/jA42oHXAJ8D7iboR7kJWBi+t2tAvX39ee+ftvT2vIPIouRbIrBcPlt29IqFx2c4tAvf\nhBLf+zhgZdjv0QL8wMzWStoK3CTpamAjcEOJ7+saQKYZE1f/4FDxJ5XAZ8uOXrEO02XlvrGZ3Q/M\njrh/B0H/h2twR49v4+nnos8+SvXs88WbNqXw2bKjV2z19MXF3iDOc1x1Fdq7pJo+9cbTaGtN556N\nPlt29Io1W5ZK2lPgcQGXAysqV5IbjTS15XOHp7dIDFrUXvTFVbrPI8l5N82iWHj8AnhjkefcWaFa\nXAWkrS2fPTw9N9iiTHhBK88dHCS7h6OtRSy7+LS8rym3LvDZsqNRrM/j3dUqxFVGNdvyUZc6If8v\nZOb7sjVbIq/EfDBcrLhal1B9tuzoyMo8jaymOXPmWFdXV63LqAvzlt8VeRWjs6Od3ywtNBWpNFFn\nEW0tAjFikFd7W2vkYj6rN3az/CcPVXSVc1c+SRvMbE4pr4k1wtTVj2qtfBXVPOofssNGh+ZbdvDl\np0xi/AtaGT+2lZvf8zIPjjpUdCUxV19G05YvpblQSjMo+7lRZxw+5Lw+xQoPSZcD3wb2AdcTjN9Y\nama+8VMKldOWL/UqTSmjQ1skpi29naPa29h/sJ+BsDfUgOt/9SinTJngfQ91KG6z5R/CLRcuAI4G\n3gUsT6wqV3WlrnQe1Txqa1HkuI5Bs+FJbwM5A0WLrabu0itueGR+Il4P/KeZbWHkwkCuzpV6lSZq\nlfNr/9eZvO2lJ9Cq0n40fFRnfYrb57FB0h3ANOBKSUcClZ1s4GqqnJXOc5tHqzd2c+uG7pIHgvmo\nzvoU98xjEbAUeKmZPQeMBXwMSAOpxFWauNPrcz/j/JlTUjGc3pUm1pmHmQ1JegqYJcmv0DSgSoy4\nLKX5IYIzjvNnTuHWDd2pGE7vShP3asvngLcBW4HMnxYjWB3dNYjRjriMewUme8DavOV3pWo4vYsv\n7lnEAmCGmR1MshhXX3LHhZw0qXh45DaFfGp8/YobHjsIVj/38GgSxQaMRY0LyfcL3yoxZBb5PuVu\nSelqL254PAdskrSerAAxsw8lUpWrqTgDxqI6R/NdYxky49HlF0U+5lPj61fc8FgTfrkmEGdafynN\niswI06gzD58aX7/iXm1ZKWksh7Zc2GZmlVlfzqVOnH6IUoanZ8Z9dPf2seSH9wEcFiAeFvUn1jgP\nSa8EHibYuOlrwJ8k/Y8E63I1FGeJvqhxIXH0Dxqf/vGWsmtz6RG32fLvwAVmtg1A0ouBG4GzkyrM\nla5Si+jku2py/sxDm28tmN3JM339XLV2KwNDxuQJY9mzP95+5ZVaFNnVVtzwaMsEB4CZ/SncvNql\nRKXWLl29sZvfbP9r5GO3bniCux/azZ97+5g6cRxDZowd08L3w53c8i1EFGXe8ru8b6POxR2e3iXp\nekmvDL+uA3xprxQpdVZsoffJp69/aHhbyF17D9Cz7yCLzps2vB5HKU2ZTLj5UPT6FTc83kcwuvRD\n4dfW8D6XEpUabFXq81fde+iXP3em7dHj2wr+gPl0/PoW92rLQeAL4ZdLoUoNtirlKgocHjZRM22v\nXbct73v6SNL6VWzTpx+E3zdLuj/3qzolujgqtXbpkgtnBAsZx1QsnBbM7uQ3S+fTWeAKTlo2qXKl\nKXbmcXn4/Q1JF+JGp1KDraK2RziqfQxmsPfAwIjnRoVTvis++UaSnj9zSmo2qXKlKbZvy5Phzf9t\nZh/Lfiycafuxw1/lamU0g61yf+mXXXwaC2Z30rP3AJde9zt2PXOAD86fzqp7u0ua7xI1rD379Wnb\npMrFF2vfFkn3mtlLcu6738zOSKyyLL5vS7Ly7cEy/gWtPNM3EHtflXL2jJm29PbIOTGCvPNhXOVV\nfN8WSe+TtBmYmdPf8SiweTTFuvTItwfLM31BMyWzynmxvohyrvj4htP1q9il2u8T7FV7W/g983W2\nmb0j4dpchRTrkIxzxSPOZdVygqBam1S5yisYHmb2jJntBL4M/NXMHjOzx4ABSedUo0A3OpkmSWZw\nV9TgrI7x8QYLFwuZcoIgahX2qO0pXfrEHZ7+dSC7z2N/xH0uhfJ1SF7xg0OzW+Mudh7nsmzmM0u5\n4uOzautT3PCQZfWshgsi+0LIdSDf2cKg2fCVkGcidqzPFbcp4UHQPOIOT98h6UOS2sKvywmWJnQp\nV+hsIdOPMXXiuMjHWyVvSri84obHe4GXA93AE8A5wOJCL5B0gqS7JW2VtCUMHCQdI+lOSQ+H348e\nzT/AFVZsslp3bx9DEe2WtlZx5Dg/uXT5xQoPM+sxs0vN7Fgzm2pmbzezniIvGwCuMLNZwLnA+yXN\nItg8ar2ZnQqsD49dQjIdkvm2gBzTIvYfHOCD86ePmNCGBXvL5utkda7gnxZJHzWzz0v6KhHr2xZa\nADkcnfpkeHufpAeBTuBNwCvDp60Efo6PVE1UprmROxAMYGDIeN9507jighnDg8DmLb/rsAV7fNSn\ny1XszOPB8HsXsCHiKxZJJwGzgd8DU7OGve8CpuZ5zWJJXZK6du/eHfejXAHj2qL/d3/1rkd4x3X3\nDB/n62Tt7u3ziWtuWKzh6aP6AGkC8AvgX81slaReM+vIevxpMyvY7+HD00fnE6s3873f/b+8WyNk\nvPPcE7l6welFVwRrb2v1DtQGk8Tw9B9LWpPvK0ZBbcCtwPfMbFV491OSjgsfPw4o1nfiRmH1xu5Y\nwQFw4+8fB4p3svoiPg6KN1v+jWDx40eBPuC68Gs/sL3QCyUJuAF40MyyFxFaAywMby8kGPruEnLt\num2xggMObZGQPeozH1/ExxWbkv8LAEn/nnNK82NJxdoR84B3AZslbQrv+ziwHPiBpEXAY8Bby6rc\nxVLKL3n2FZnMYK98TRifuObiXsg/QtLJZrYDQNI04IhCLzCzXxPMrI7yqvglulylbLEwdeI4du09\nEOt9LzvnhMPu8+0gXT5xw+OfgZ9L2kEQCC8C3pNYVS6vUrZY6Nl7IHIAmIDpxx7Bjt3PMWhGq8Rl\n55zA1QtOP+y5vh2kyyfuAsg/lXQqMDO866FwUWRXZXFX3sqsALb/4AAXzDqW9Q/uHhEUc150zHAg\n/M1R45jzomPyfqbPV3FRYoWHpPHAR4AXmdk/STpV0gwzW5tseS5XnAV3spcOXHTeNK7/1aPDnaGD\nZtz8h8e5+Y+P0z94aA9ZXzfUlSru3JZvA88DLwuPu4GrE6nIFXRUe/TaG0YwMnTlb3cOB8d33j2X\nVfd2R64SlgmODL/86koVt8/jFDN7m6TLAMzsufBSrKui1Ru7efb5gbyPd/f2sWzNFlpbRMf4Nt72\nzXtiX6YFv/zqShM3PJ6X1E44v0XSKYD3eVTZteu2HXbGkMsI5qvE3XQ6m19+daWIGx6fAn4KnCDp\newRjOP4+qaJctEqdGbS1CMSIIPLLr65URcMjbJ48BFxCMLVewOVmtifh2lyOUreCzKXwPTIh4Zdf\n3WgUDQ8zM0n/bWanA7dXoSaXR9SArVwiYu0EovdO8bBwoxH3asu9kl6aaCWuqAWzO3nz2Z0jhpG3\nCCZPGDu8XOA7zj3RtzJwVRG3z+Mc4J2SdgLPEv6Bq9aOcS6wemM3N//x8eExGwASfOKiWSPOIrIH\ngHmTxCUl7naTL4q6P9zDJXG+nkdg9mfuOGyFLwiWDdz4yQtqUJFrFOWs51FsGcJxBIsfTyfYXvIG\nM8s/0MAlKio4Ct3vXJKKNVtWAv3Ar4DXAbOAy5MuqpnlmzHbE3NmrHPVUiw8ZoVXWZB0A/CH5Etq\nHrlBcf7MKdy6oXvEjNkrbrmPD9+8qeD7dOQZsu5ckoqFx/D5sJkN+Ij0yomaWh+1XODgUOE+qbYW\nsezi0xKq0rn8ioXHmZL2hrcFtIfHmastExOtroFFTa0vZR5K9oAvv5LiaqHYMoT5V8F1ozLaoeaP\nLr+oQpU4V564g8RchXWML7+fIt/ub85Vk4dHjYxmu5xzT/btfV3teXjUyDN95Y/N2PrkvgpW4lx5\nPDxqJN+KYHH4oDCXBnHntrgKyB7X4Vy98/CoktxxHaPhg8JcGnizpUqixnXkk7maIg7fNcsHhbm0\n8DOPKimlqbL9mtcP3y5ldzjnqsnDo0riLiHYknOq4RsuubTyZkuVLLlwxmErfEV5+zknVqEa50bP\nw6NKFszuZOnrZjImPLWYPGEs8045Zrh/o1XineeeGLlfrHNp5M2WKunZe4CV9+xk7JgWvv/uucyd\nln9vWOfqgZ95VEH23rHf8eBwDcLPPCqk0ApgHhyuEXl4VEDUwj5XrtrMM339rLxnpweHa0geHhUQ\nNQCsr3+Qq9ZuZeyYFg8O15AS6/OQ9C1JPZIeyLrvGEl3Sno4/N4Qc8vzDQAbGDIPDtewkuww/Q7w\n2pz7lgLrzexUYH14XPfy7S4/ecJYDw7XsBILDzP7JfDXnLvfRLCdA+H3BUl9fjVFDQAb29rCJy6a\nVaOKnEtetS/VTjWzJ8Pbu4Cp+Z4oabGkLkldu3fvrk51ZYoaAPb5t5zhw8pdQ6tZh6mZmaS8i/GZ\n2QpgBQTbTVatsDL4ADDXjKp95vGUpOMAwu89Vf78ivNxHK5ZVTs81gALw9sLgduq/PkV5cHhmlmS\nl2pvBO4BZkh6QtIiYDnwGkkPA68Oj+uSB4drdon1eZjZZXkeelVSn1ktHhzO+cS4knlwOBfw8CiB\nB4dzh3h4xOTB4dxIHh4xeHA4dzgPjyI8OJyL5uFRgAeHc/l5eOThweFcYR4eETw4nCvOwyOHB4dz\n8Xh4ZPHgcC4+D4+QB4dzpfHwwIPDuXI0fXh4cDhXnqYODw8O58rXtOHhweHc6DRleHhwODd6TRce\nHhzOVUZThYcHh3OV0zTh4cHhXGU1RXh4cDhXeQ0fHh4cziWjocPDg8O55DRseHhwOJeshgwPDw7n\nktdw4eHB4Vx1NFR4eHA4Vz0NEx4eHM5VV0OEhweHc9VX9+HhweFcbdR1eHhwOFc7dRseHhzO1VZd\nhocHh3O1V3fh4cHhXDrUVXh4cDiXHnUTHh4czqVLTcJD0mslbZP0iKSlxZ4/MGgeHM6lTNXDQ1Ir\n8B/A64BZwGWSZhV6zY49+z04nEuZWpx5zAUeMbMdZvY8cBPwpkIv6B80Dw7nUmZMDT6zE3g86/gJ\n4JzcJ0laDCwODw+ec/KkB6pQWyVMBvbUuogS1FO99VQr1Fe9M0p9QS3CIxYzWwGsAJDUZWZzalxS\nLPVUK9RXvfVUK9RXvZK6Sn1NLZot3cAJWcfHh/c55+pILcLjj8CpkqZJGgtcCqypQR3OuVGoerPF\nzAYkfQBYB7QC3zKzLUVetiL5yiqmnmqF+qq3nmqF+qq35FplZkkU4pxrcHUzwtQ5ly4eHs65sqQ6\nPEodxl5tkr4lqUfSA1n3HSPpTkkPh9+PrmWNGZJOkHS3pK2Stki6PLw/rfWOk/QHSfeF9X46vH+a\npN+HPxM3h53uqSCpVdJGSWvD4zTXulPSZkmbMpdpS/1ZSG14lDOMvQa+A7w2576lwHozOxVYHx6n\nwQBwhZnNAs4F3h/+90xrvQeB+WZ2JnAW8FpJ5wKfA75oZtOBp4FFNawx1+XAg1nHaa4V4HwzOytr\nLEppPwtmlsov4GXAuqzjK4Era11XRJ0nAQ9kHW8DjgtvHwdsq3WNeeq+DXhNPdQLjAfuJRiJvAcY\nE/UzUuMajw9/4eYDawGltdawnp3A5Jz7SvpZSO2ZB9HD2DtrVEsppprZk+HtXcDUWhYTRdJJwGzg\n96S43rAZsAnoAe4EtgO9ZjYQPiVNPxNfAj4KDIXHk0hvrQAG3CFpQzgVBEr8WUjt8PRGYGYmKVXX\nwiVNAG4FPmxmeyUNP5a2es1sEDhLUgfwI2BmjUuKJOkNQI+ZbZD0ylrXE9N5ZtYt6VjgTkkPZT8Y\n52chzWce9TqM/SlJxwGE33tqXM8wSW0EwfE9M1sV3p3aejPMrBe4m+DUv0NS5o9eWn4m5gEXS9pJ\nMEt8PvBl0lkrAGbWHX7vIQjmuZT4s5Dm8KjXYexrgIXh7YUEfQs1p+AU4wbgQTP7QtZDaa13SnjG\ngaR2gv6ZBwlC5C3h01JRr5ldaWbHm9lJBD+nd5nZO0hhrQCSjpB0ZOY2cAHwAKX+LNS646ZIp87r\ngT8RtHX/pdb1RNR3I/Ak0E/Qpl1E0NZdDzwM/Aw4ptZ1hrWeR9DOvR/YFH69PsX1ngFsDOt9APhk\neP/JwB+AR4BbgBfUutacul8JrE1zrWFd94VfWzK/W6X+LPjwdOdcWdLcbHHOpZiHh3OuLB4ezrmy\neHg458ri4eGcK4uHh3OuLB4eKSZpUjhlepOkXZK6s44rMr1b0pGS/hIOW8++f62kNxd43aslra5E\nDXne/78kPSrpH8PjqyVZOC8n85z/E953Vnj8RDjN/H5JPw2HXmf+jd+UtF3SvZK6JP1D+NiM8L9n\nb1L/lkbl4ZFiZvYXC6ZMnwV8g2B691nh1/MQjByVVPb/RzPbRzAwaHjjrXAdh3OB20f3Lxi1fzaz\n67OONxOM4Mx4MyOnwAO8wszOIBhclplS/m3gKeBUM3sJweC4yQBmtg2oi+0R0sbDow5Jmh4u6vM9\nghGCJ2T/5ZR0qaTrw9tTJa0K/9r+IVwTI9eNHP5LebuZHZB0rqR7wkVufiPp1Ih6rpb04azjhyQd\nH95eGH7uJklfk9QiaYyk/wzPEh6Q9KGY//RVwN+F7/tiginvf83z3F8C0yXNAM4ElpnZEATzOczs\n8zE/0+Xh4VG/ZhKcicyi8ISrrwCft2DBl7cC10c857+Bc7JWjrqUIFAg+Mv+CjObDVwFXB23QEl/\nS/DL/vLw7GlM+N5nE6wlcbqZ/S3w3Zhv2QvskjQTuIxgElrU5wp4A8GZymnApkxwuMrxKfn1a7uZ\nxdnl69XAjKyp90dLajezvswdZnZQ0u3AJQqW0DuNYG4DQAfwXUmnlFHjq4GXAl3h57cTrNGyLqzp\nKwRNoztKeM+bCQLojcD/BN6X8/ivCNbU2ESwktersx+U9EngEmCSmZ2AK5uHR/16Nuv2EMHKVRnj\nsm4LmJvpIyngRmAJwS/4j+zQIjb/SrAC1tckTQd+GvHaAUaexWY+XwT78vzf3BdIOoNgicn3EzST\nFuc+J481BGdDvzWz/dnrkYReYcEU/sznbCFYE6TFzIbM7DPAZyTtj/l5Lg9vtjSA8JT8aUmnhp2n\nf5f18M8IfkEByFyZiLCe4IzjvRxqsgAcxaFm0d/nee1OgqYIkuZyaB2WnwFvlTQ5fGySpBMlTSHY\nM+gW4JPAS2L8MwEws/3Ax4BrYj5/G0Hz5dOZjmVJ4xgZtq4MHh6N42MEzYHfEiwPkPF+YF54+XIr\n8E9RL7Zg1a5VwETg11kPfU1O3/EAAACYSURBVA64VtK95P+FuwWYqmAV+cXAjvA9NwOfBn4m6X6C\n5slUgnD5pYIlBr8NfLyUf6iZfd/MNpXwkncDfwNsV7BS+J3AFaV8pjucT8l3qSTpv4AfmlliY0my\nPmsMsMfMOpL+rEbiZx4urXqBazKDxJISXsrtIhgH4krgZx7OubL4mYdzriweHs65snh4OOfK4uHh\nnCvL/wew0hBxRTTTnAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0XZ8Q0trVw5",
        "colab_type": "text"
      },
      "source": [
        "## Part II - Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hifLNlBWrVw6",
        "colab_type": "text"
      },
      "source": [
        "<b>ANSWER THE FOLLOWING QUESTIONS HERE:</b>\n",
        "\n",
        "Q1- How do you interprete the plot above in terms of the model performance (Predictions vs True Values)? GIVE COMPLETE ANSWER!\n",
        "<br>`Answer: I interpret the above graph as the following: as the MPG for the predictions increase, so do the true values for MPG. Look at the bottom left dot. It's approximately 10 for both predictions and true values. The same, or similar, could be said for those values at 20, 30, and even 40. Of course, with all line graphs/scatter plots, there are outliers. But for the most part the two numbers are generally the same.`\n",
        "\n",
        "Q2 - How do you interprete the first regression model's plots of mse and mae, i.e. `plot_history(nn_reg1_history)`? What is the impact of choosing `mse` vs `mae`? Why the axis scales are different for `mse` and `mae`? GIVE COMPLETE ANSWER!\n",
        "<br>`Answer: I interpret the first model's plots as MSE being less strict than MAE in terms of Val Error vs Train Error ratio. The impact of choosing MSE over MAE is that MAE appears to overfit the data, which is not what we want. MSE seemingly does a good enough job keeping close to the data, but enough to not be classified as underfitting. The y-axis scales are different because of the calculations. MAE involves absolute values, while MSE involves the use of squaring and roots. This can cause the numbers to see floating point decimals or be much larger than the values plotted against in the MAE graph.`\n",
        "\n",
        "Q3 - What is the role of `validation_split` hyperparameter in `fit` method? What does it change exactly and why is it used? GIVE COMPLETE ANSWER!\n",
        "<br>`Answer: The role is to find a floating point number between 0 and 1. It changes the amount of training data that is used. For example, if a float of .145 is selected, then 14.5% of the training data will be set apart. It is used because it will evaluate the loss and model metrics against this data at the end of each epoch.`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUsTMJ-xrVw7",
        "colab_type": "text"
      },
      "source": [
        "## Optional Part III - <font color=green>Extra Credit</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CAirB6crVw8",
        "colab_type": "text"
      },
      "source": [
        "<b>Notice:</b> This part is totally optional and for earning <b><font color=green>extra credit</font></b> in the \"Assignment\" section of your final grade. Attempt this part only if you have enough time and you're inclined to challenge yourself a bit!<br>\n",
        "\n",
        "[Download the video games dataset](https://raw.githubusercontent.com/fereydoonvafaei/UMBC-CMSC-478-Fall-2019/master/Assignment-4/video.csv)<br> \n",
        "\n",
        "You can read about the data [here](https://www.kaggle.com/rush4ratio/video-game-sales-with-ratings). <br>\n",
        "\n",
        "Build a neural network that can predict the \"<b>Rating</b>\" of each game based on other features. Alternatively, you may predict either the global sales or regional sales (in North America, Europe, etc) for each row/video game. Perform any necessary preprocessing steps needed on the dataset. <br>\n",
        "\n",
        "You should create a separate notebook for Extra Credit attempt and submit it via a separate link in Blackboard. If you can get good results based on the instructor's judgement of your work, you may earn up to 50 points of extra credit for A5 that can be used for the missing points of \"Assignment\" section of your final grade.<br>\n",
        "\n",
        "<b>Note:</b> Extra credits for A5 can only be used to compensate for \"Assignment\" section NOT for any other sections of the final grade such as quizzes or exams."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RWCcsstrVw-",
        "colab_type": "code",
        "outputId": "3622ebf4-06be-4ee3-fb1a-5b010ba06974",
        "colab": {}
      },
      "source": [
        "video_data = pd.read_csv('video.csv')\n",
        "print(video_data.shape)\n",
        "video_data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(16719, 16)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>Platform</th>\n",
              "      <th>Year_of_Release</th>\n",
              "      <th>Genre</th>\n",
              "      <th>Publisher</th>\n",
              "      <th>NA_Sales</th>\n",
              "      <th>EU_Sales</th>\n",
              "      <th>JP_Sales</th>\n",
              "      <th>Other_Sales</th>\n",
              "      <th>Global_Sales</th>\n",
              "      <th>Critic_Score</th>\n",
              "      <th>Critic_Count</th>\n",
              "      <th>User_Score</th>\n",
              "      <th>User_Count</th>\n",
              "      <th>Developer</th>\n",
              "      <th>Rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Wii Sports</td>\n",
              "      <td>Wii</td>\n",
              "      <td>2006.0</td>\n",
              "      <td>Sports</td>\n",
              "      <td>Nintendo</td>\n",
              "      <td>41.36</td>\n",
              "      <td>28.96</td>\n",
              "      <td>3.77</td>\n",
              "      <td>8.45</td>\n",
              "      <td>82.53</td>\n",
              "      <td>76.0</td>\n",
              "      <td>51.0</td>\n",
              "      <td>8</td>\n",
              "      <td>322.0</td>\n",
              "      <td>Nintendo</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Super Mario Bros.</td>\n",
              "      <td>NES</td>\n",
              "      <td>1985.0</td>\n",
              "      <td>Platform</td>\n",
              "      <td>Nintendo</td>\n",
              "      <td>29.08</td>\n",
              "      <td>3.58</td>\n",
              "      <td>6.81</td>\n",
              "      <td>0.77</td>\n",
              "      <td>40.24</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Mario Kart Wii</td>\n",
              "      <td>Wii</td>\n",
              "      <td>2008.0</td>\n",
              "      <td>Racing</td>\n",
              "      <td>Nintendo</td>\n",
              "      <td>15.68</td>\n",
              "      <td>12.76</td>\n",
              "      <td>3.79</td>\n",
              "      <td>3.29</td>\n",
              "      <td>35.52</td>\n",
              "      <td>82.0</td>\n",
              "      <td>73.0</td>\n",
              "      <td>8.3</td>\n",
              "      <td>709.0</td>\n",
              "      <td>Nintendo</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Wii Sports Resort</td>\n",
              "      <td>Wii</td>\n",
              "      <td>2009.0</td>\n",
              "      <td>Sports</td>\n",
              "      <td>Nintendo</td>\n",
              "      <td>15.61</td>\n",
              "      <td>10.93</td>\n",
              "      <td>3.28</td>\n",
              "      <td>2.95</td>\n",
              "      <td>32.77</td>\n",
              "      <td>80.0</td>\n",
              "      <td>73.0</td>\n",
              "      <td>8</td>\n",
              "      <td>192.0</td>\n",
              "      <td>Nintendo</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Pokemon Red/Pokemon Blue</td>\n",
              "      <td>GB</td>\n",
              "      <td>1996.0</td>\n",
              "      <td>Role-Playing</td>\n",
              "      <td>Nintendo</td>\n",
              "      <td>11.27</td>\n",
              "      <td>8.89</td>\n",
              "      <td>10.22</td>\n",
              "      <td>1.00</td>\n",
              "      <td>31.37</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       Name Platform  Year_of_Release         Genre Publisher  \\\n",
              "0                Wii Sports      Wii           2006.0        Sports  Nintendo   \n",
              "1         Super Mario Bros.      NES           1985.0      Platform  Nintendo   \n",
              "2            Mario Kart Wii      Wii           2008.0        Racing  Nintendo   \n",
              "3         Wii Sports Resort      Wii           2009.0        Sports  Nintendo   \n",
              "4  Pokemon Red/Pokemon Blue       GB           1996.0  Role-Playing  Nintendo   \n",
              "\n",
              "   NA_Sales  EU_Sales  JP_Sales  Other_Sales  Global_Sales  Critic_Score  \\\n",
              "0     41.36     28.96      3.77         8.45         82.53          76.0   \n",
              "1     29.08      3.58      6.81         0.77         40.24           NaN   \n",
              "2     15.68     12.76      3.79         3.29         35.52          82.0   \n",
              "3     15.61     10.93      3.28         2.95         32.77          80.0   \n",
              "4     11.27      8.89     10.22         1.00         31.37           NaN   \n",
              "\n",
              "   Critic_Count User_Score  User_Count Developer Rating  \n",
              "0          51.0          8       322.0  Nintendo      E  \n",
              "1           NaN        NaN         NaN       NaN    NaN  \n",
              "2          73.0        8.3       709.0  Nintendo      E  \n",
              "3          73.0          8       192.0  Nintendo      E  \n",
              "4           NaN        NaN         NaN       NaN    NaN  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAMSvCiPrVxB",
        "colab_type": "code",
        "outputId": "a92f58ef-e14d-40d1-b592-d1d1f714fbdb",
        "colab": {}
      },
      "source": [
        "video_data.dropna(inplace=True)\n",
        "print(video_data.shape)\n",
        "video_data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6825, 16)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>Platform</th>\n",
              "      <th>Year_of_Release</th>\n",
              "      <th>Genre</th>\n",
              "      <th>Publisher</th>\n",
              "      <th>NA_Sales</th>\n",
              "      <th>EU_Sales</th>\n",
              "      <th>JP_Sales</th>\n",
              "      <th>Other_Sales</th>\n",
              "      <th>Global_Sales</th>\n",
              "      <th>Critic_Score</th>\n",
              "      <th>Critic_Count</th>\n",
              "      <th>User_Score</th>\n",
              "      <th>User_Count</th>\n",
              "      <th>Developer</th>\n",
              "      <th>Rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Wii Sports</td>\n",
              "      <td>Wii</td>\n",
              "      <td>2006.0</td>\n",
              "      <td>Sports</td>\n",
              "      <td>Nintendo</td>\n",
              "      <td>41.36</td>\n",
              "      <td>28.96</td>\n",
              "      <td>3.77</td>\n",
              "      <td>8.45</td>\n",
              "      <td>82.53</td>\n",
              "      <td>76.0</td>\n",
              "      <td>51.0</td>\n",
              "      <td>8</td>\n",
              "      <td>322.0</td>\n",
              "      <td>Nintendo</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Mario Kart Wii</td>\n",
              "      <td>Wii</td>\n",
              "      <td>2008.0</td>\n",
              "      <td>Racing</td>\n",
              "      <td>Nintendo</td>\n",
              "      <td>15.68</td>\n",
              "      <td>12.76</td>\n",
              "      <td>3.79</td>\n",
              "      <td>3.29</td>\n",
              "      <td>35.52</td>\n",
              "      <td>82.0</td>\n",
              "      <td>73.0</td>\n",
              "      <td>8.3</td>\n",
              "      <td>709.0</td>\n",
              "      <td>Nintendo</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Wii Sports Resort</td>\n",
              "      <td>Wii</td>\n",
              "      <td>2009.0</td>\n",
              "      <td>Sports</td>\n",
              "      <td>Nintendo</td>\n",
              "      <td>15.61</td>\n",
              "      <td>10.93</td>\n",
              "      <td>3.28</td>\n",
              "      <td>2.95</td>\n",
              "      <td>32.77</td>\n",
              "      <td>80.0</td>\n",
              "      <td>73.0</td>\n",
              "      <td>8</td>\n",
              "      <td>192.0</td>\n",
              "      <td>Nintendo</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>New Super Mario Bros.</td>\n",
              "      <td>DS</td>\n",
              "      <td>2006.0</td>\n",
              "      <td>Platform</td>\n",
              "      <td>Nintendo</td>\n",
              "      <td>11.28</td>\n",
              "      <td>9.14</td>\n",
              "      <td>6.50</td>\n",
              "      <td>2.88</td>\n",
              "      <td>29.80</td>\n",
              "      <td>89.0</td>\n",
              "      <td>65.0</td>\n",
              "      <td>8.5</td>\n",
              "      <td>431.0</td>\n",
              "      <td>Nintendo</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Wii Play</td>\n",
              "      <td>Wii</td>\n",
              "      <td>2006.0</td>\n",
              "      <td>Misc</td>\n",
              "      <td>Nintendo</td>\n",
              "      <td>13.96</td>\n",
              "      <td>9.18</td>\n",
              "      <td>2.93</td>\n",
              "      <td>2.84</td>\n",
              "      <td>28.92</td>\n",
              "      <td>58.0</td>\n",
              "      <td>41.0</td>\n",
              "      <td>6.6</td>\n",
              "      <td>129.0</td>\n",
              "      <td>Nintendo</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    Name Platform  Year_of_Release     Genre Publisher  \\\n",
              "0             Wii Sports      Wii           2006.0    Sports  Nintendo   \n",
              "2         Mario Kart Wii      Wii           2008.0    Racing  Nintendo   \n",
              "3      Wii Sports Resort      Wii           2009.0    Sports  Nintendo   \n",
              "6  New Super Mario Bros.       DS           2006.0  Platform  Nintendo   \n",
              "7               Wii Play      Wii           2006.0      Misc  Nintendo   \n",
              "\n",
              "   NA_Sales  EU_Sales  JP_Sales  Other_Sales  Global_Sales  Critic_Score  \\\n",
              "0     41.36     28.96      3.77         8.45         82.53          76.0   \n",
              "2     15.68     12.76      3.79         3.29         35.52          82.0   \n",
              "3     15.61     10.93      3.28         2.95         32.77          80.0   \n",
              "6     11.28      9.14      6.50         2.88         29.80          89.0   \n",
              "7     13.96      9.18      2.93         2.84         28.92          58.0   \n",
              "\n",
              "   Critic_Count User_Score  User_Count Developer Rating  \n",
              "0          51.0          8       322.0  Nintendo      E  \n",
              "2          73.0        8.3       709.0  Nintendo      E  \n",
              "3          73.0          8       192.0  Nintendo      E  \n",
              "6          65.0        8.5       431.0  Nintendo      E  \n",
              "7          41.0        6.6       129.0  Nintendo      E  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvahNgA2rVxD",
        "colab_type": "text"
      },
      "source": [
        "## Grading\n",
        "\n",
        "For Assignment 5, your notebook will be run and graded with a maximum of 100 points. Make sure that you get the correct outputs for all cells that you implement and answer ALL questions COMPLETELY. Also, your notebook should be written with no grammatical and spelling errors and should be nicely-formatted and easy-to-read.\n",
        "\n",
        "The breakdown of the 100 points is as follows:\n",
        "\n",
        "Part I implementaion has 40 points:\n",
        "- 10 points: preprocessing steps.\n",
        "- 15 points: nn_clf implementation, and compile.\n",
        "- 15 points: correct ROC curve for nn_clf.\n",
        "\n",
        "Part I questions have 10 points (5 points each).\n",
        "\n",
        "Part II implementaion has 35 points:\n",
        "- 10 points: preprocessing steps.\n",
        "- 10 points: nn_reg1 implementation, and compile.\n",
        "- 15 points: nn_reg2 implementation, and compile including hyperparameter tuning.\n",
        "\n",
        "Part II questions have 15 points (5 points each).\n",
        "\n",
        "Part III is optional and for Extra Credit only - up to 50 extra points based on the quality of your work.\n",
        "\n",
        "Follow the instructions of each section carefully. Up to 10 points may be deducted if your submitted notebook is not easy to read and follow or if it has grammatical and spelling errors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6kubFWfrVxF",
        "colab_type": "text"
      },
      "source": [
        "## How to Submit and Due Date"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQiBpQQWrVxH",
        "colab_type": "text"
      },
      "source": [
        "Name your notebook ```Lastname-A5.ipynb```. Submit the file using the ```Assignment-5``` link on Blackboard.\n",
        "\n",
        "If you attempt the Extra Credit in Part III, create a separate notebook including all the necessary code, name it `Lastname-A5-EC.ipynb` and submit it using the ```A5-Extra-Credit``` link on Blackboard.\n",
        "\n",
        "Grading will be based on \n",
        "\n",
        "  * correct implementation, correct answer to the questions, and\n",
        "  * readability of the notebook.\n",
        "  \n",
        "<font color=red><b>Due Date: Monday December 2nd 11:59PM.</b></font>"
      ]
    }
  ]
}